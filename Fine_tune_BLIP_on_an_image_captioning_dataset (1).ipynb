{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets torch numpy transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ2wZ_SkHJ76",
        "outputId": "7b55f1cd-e518-4070-c2ff-d0467d8cd452"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.0.dev0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from datasets import load_dataset\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Custom dataset class\n",
        "class FootballImageCaptioningDataset(Dataset):\n",
        "    def __init__(self, dataset, processor):\n",
        "        self.dataset = dataset\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.dataset[idx]\n",
        "        encoding = self.processor(\n",
        "            images=item[\"image\"], padding=\"max_length\", return_tensors=\"pt\"\n",
        "        )\n",
        "        # Remove batch dimension\n",
        "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
        "        encoding[\"text\"] = item[\"text\"]\n",
        "        return encoding\n",
        "\n",
        "def collator(batch):\n",
        "    processed_batch = {}\n",
        "    for key in batch[0].keys():\n",
        "        if key != \"text\":\n",
        "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
        "        else:\n",
        "            text_inputs = processor.tokenizer(\n",
        "                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
        "            )\n",
        "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
        "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
        "    return processed_batch\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"tomytjandra/h-and-m-fashion-caption-12k\")\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(dataset['train']))\n",
        "val_size = int(0.1 * len(dataset['train']))\n",
        "test_size = len(dataset['train']) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset['train'], [train_size, val_size, test_size])\n",
        "\n",
        "# Initialize the processor and model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = FootballImageCaptioningDataset(train_dataset, processor)\n",
        "val_dataset = FootballImageCaptioningDataset(val_dataset, processor)\n",
        "test_dataset = FootballImageCaptioningDataset(test_dataset, processor)\n",
        "\n",
        "# Create DataLoader for each subset\n",
        "batch_size = 2\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collator)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size, collate_fn=collator)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size, collate_fn=collator)\n",
        "\n",
        "# Move model to the appropriate device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Training loop with early stopping\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3\n",
        "min_delta = 0.001\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(4):\n",
        "    print(f\"Epoch {epoch + 1}/50\")\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for idx, batch in enumerate(train_dataloader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
        "        train_loss = outputs.loss\n",
        "        running_train_loss += train_loss.item()\n",
        "\n",
        "        # Print the batch training loss\n",
        "        print(f\"Batch {idx + 1}/{len(train_dataloader)} - Training Loss: {train_loss.item():.4f}\")\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # Calculate average training loss for this epoch\n",
        "    avg_train_loss = running_train_loss / len(train_dataloader)\n",
        "    print(f\"Average Training Loss for Epoch {epoch + 1}: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
        "            val_loss = outputs.loss\n",
        "            running_val_loss += val_loss.item()\n",
        "\n",
        "    # Calculate average validation loss for this epoch\n",
        "    avg_val_loss = running_val_loss / len(val_dataloader)\n",
        "    print(f\"Validation Loss for Epoch {epoch + 1}: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping check\n",
        "    if avg_val_loss < best_val_loss - min_delta:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(\"Early stopping triggered. Training stopped to prevent overfitting.\")\n",
        "        break\n",
        "\n",
        "# After training, you can evaluate on the test set to see the model's performance on unseen data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIrQyGHOHH4q",
        "outputId": "1fd7cf45-42cf-4fc0-c9b1-ab1e93c083ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch 4956/4975 - Training Loss: 0.9339\n",
            "Batch 4957/4975 - Training Loss: 0.6665\n",
            "Batch 4958/4975 - Training Loss: 0.4894\n",
            "Batch 4959/4975 - Training Loss: 1.0001\n",
            "Batch 4960/4975 - Training Loss: 0.4361\n",
            "Batch 4961/4975 - Training Loss: 0.6849\n",
            "Batch 4962/4975 - Training Loss: 0.8695\n",
            "Batch 4963/4975 - Training Loss: 0.8750\n",
            "Batch 4964/4975 - Training Loss: 0.6406\n",
            "Batch 4965/4975 - Training Loss: 0.8629\n",
            "Batch 4966/4975 - Training Loss: 0.8260\n",
            "Batch 4967/4975 - Training Loss: 1.0050\n",
            "Batch 4968/4975 - Training Loss: 0.8100\n",
            "Batch 4969/4975 - Training Loss: 1.2503\n",
            "Batch 4970/4975 - Training Loss: 0.2608\n",
            "Batch 4971/4975 - Training Loss: 0.8673\n",
            "Batch 4972/4975 - Training Loss: 0.6038\n",
            "Batch 4973/4975 - Training Loss: 0.5367\n",
            "Batch 4974/4975 - Training Loss: 0.4726\n",
            "Batch 4975/4975 - Training Loss: 1.1184\n",
            "Average Training Loss for Epoch 3: 0.7127\n",
            "Validation Loss for Epoch 3: 0.8162\n",
            "Epoch 4/50\n",
            "Batch 1/4975 - Training Loss: 0.5633\n",
            "Batch 2/4975 - Training Loss: 0.7265\n",
            "Batch 3/4975 - Training Loss: 0.4885\n",
            "Batch 4/4975 - Training Loss: 0.4615\n",
            "Batch 5/4975 - Training Loss: 0.4140\n",
            "Batch 6/4975 - Training Loss: 0.4574\n",
            "Batch 7/4975 - Training Loss: 0.4253\n",
            "Batch 8/4975 - Training Loss: 0.4519\n",
            "Batch 9/4975 - Training Loss: 0.5721\n",
            "Batch 10/4975 - Training Loss: 0.6796\n",
            "Batch 11/4975 - Training Loss: 0.6493\n",
            "Batch 12/4975 - Training Loss: 0.4777\n",
            "Batch 13/4975 - Training Loss: 0.4176\n",
            "Batch 14/4975 - Training Loss: 0.5327\n",
            "Batch 15/4975 - Training Loss: 0.4155\n",
            "Batch 16/4975 - Training Loss: 0.3752\n",
            "Batch 17/4975 - Training Loss: 0.5665\n",
            "Batch 18/4975 - Training Loss: 0.9246\n",
            "Batch 19/4975 - Training Loss: 0.3955\n",
            "Batch 20/4975 - Training Loss: 0.8506\n",
            "Batch 21/4975 - Training Loss: 0.7786\n",
            "Batch 22/4975 - Training Loss: 0.5162\n",
            "Batch 23/4975 - Training Loss: 0.4451\n",
            "Batch 24/4975 - Training Loss: 0.5931\n",
            "Batch 25/4975 - Training Loss: 0.6912\n",
            "Batch 26/4975 - Training Loss: 0.4406\n",
            "Batch 27/4975 - Training Loss: 0.5887\n",
            "Batch 28/4975 - Training Loss: 0.6025\n",
            "Batch 29/4975 - Training Loss: 0.5066\n",
            "Batch 30/4975 - Training Loss: 0.4807\n",
            "Batch 31/4975 - Training Loss: 0.3664\n",
            "Batch 32/4975 - Training Loss: 0.7705\n",
            "Batch 33/4975 - Training Loss: 0.7185\n",
            "Batch 34/4975 - Training Loss: 0.3845\n",
            "Batch 35/4975 - Training Loss: 0.7190\n",
            "Batch 36/4975 - Training Loss: 0.6678\n",
            "Batch 37/4975 - Training Loss: 0.2829\n",
            "Batch 38/4975 - Training Loss: 0.5831\n",
            "Batch 39/4975 - Training Loss: 0.5119\n",
            "Batch 40/4975 - Training Loss: 0.4934\n",
            "Batch 41/4975 - Training Loss: 0.5367\n",
            "Batch 42/4975 - Training Loss: 0.4977\n",
            "Batch 43/4975 - Training Loss: 0.6526\n",
            "Batch 44/4975 - Training Loss: 0.3354\n",
            "Batch 45/4975 - Training Loss: 0.7885\n",
            "Batch 46/4975 - Training Loss: 0.7361\n",
            "Batch 47/4975 - Training Loss: 0.5955\n",
            "Batch 48/4975 - Training Loss: 0.8084\n",
            "Batch 49/4975 - Training Loss: 0.6055\n",
            "Batch 50/4975 - Training Loss: 0.5611\n",
            "Batch 51/4975 - Training Loss: 0.3563\n",
            "Batch 52/4975 - Training Loss: 0.7650\n",
            "Batch 53/4975 - Training Loss: 0.5226\n",
            "Batch 54/4975 - Training Loss: 0.3584\n",
            "Batch 55/4975 - Training Loss: 0.4439\n",
            "Batch 56/4975 - Training Loss: 0.7339\n",
            "Batch 57/4975 - Training Loss: 0.6315\n",
            "Batch 58/4975 - Training Loss: 0.6238\n",
            "Batch 59/4975 - Training Loss: 0.3302\n",
            "Batch 60/4975 - Training Loss: 0.6536\n",
            "Batch 61/4975 - Training Loss: 0.4691\n",
            "Batch 62/4975 - Training Loss: 0.7731\n",
            "Batch 63/4975 - Training Loss: 0.3630\n",
            "Batch 64/4975 - Training Loss: 0.5563\n",
            "Batch 65/4975 - Training Loss: 0.4988\n",
            "Batch 66/4975 - Training Loss: 0.5283\n",
            "Batch 67/4975 - Training Loss: 0.7301\n",
            "Batch 68/4975 - Training Loss: 0.4368\n",
            "Batch 69/4975 - Training Loss: 0.6502\n",
            "Batch 70/4975 - Training Loss: 0.7790\n",
            "Batch 71/4975 - Training Loss: 0.5398\n",
            "Batch 72/4975 - Training Loss: 0.5698\n",
            "Batch 73/4975 - Training Loss: 0.5120\n",
            "Batch 74/4975 - Training Loss: 0.9363\n",
            "Batch 75/4975 - Training Loss: 0.5794\n",
            "Batch 76/4975 - Training Loss: 0.7460\n",
            "Batch 77/4975 - Training Loss: 0.6919\n",
            "Batch 78/4975 - Training Loss: 0.7275\n",
            "Batch 79/4975 - Training Loss: 0.5525\n",
            "Batch 80/4975 - Training Loss: 0.6832\n",
            "Batch 81/4975 - Training Loss: 0.4138\n",
            "Batch 82/4975 - Training Loss: 0.6609\n",
            "Batch 83/4975 - Training Loss: 0.9449\n",
            "Batch 84/4975 - Training Loss: 0.5536\n",
            "Batch 85/4975 - Training Loss: 0.3802\n",
            "Batch 86/4975 - Training Loss: 0.6484\n",
            "Batch 87/4975 - Training Loss: 0.4112\n",
            "Batch 88/4975 - Training Loss: 0.4472\n",
            "Batch 89/4975 - Training Loss: 0.5463\n",
            "Batch 90/4975 - Training Loss: 0.2938\n",
            "Batch 91/4975 - Training Loss: 0.8525\n",
            "Batch 92/4975 - Training Loss: 0.5944\n",
            "Batch 93/4975 - Training Loss: 0.7025\n",
            "Batch 94/4975 - Training Loss: 1.1599\n",
            "Batch 95/4975 - Training Loss: 0.6584\n",
            "Batch 96/4975 - Training Loss: 0.3597\n",
            "Batch 97/4975 - Training Loss: 0.6534\n",
            "Batch 98/4975 - Training Loss: 0.3920\n",
            "Batch 99/4975 - Training Loss: 0.7460\n",
            "Batch 100/4975 - Training Loss: 0.4255\n",
            "Batch 101/4975 - Training Loss: 0.8268\n",
            "Batch 102/4975 - Training Loss: 0.4171\n",
            "Batch 103/4975 - Training Loss: 0.5823\n",
            "Batch 104/4975 - Training Loss: 0.5726\n",
            "Batch 105/4975 - Training Loss: 0.5447\n",
            "Batch 106/4975 - Training Loss: 0.6367\n",
            "Batch 107/4975 - Training Loss: 0.3982\n",
            "Batch 108/4975 - Training Loss: 0.3806\n",
            "Batch 109/4975 - Training Loss: 0.4680\n",
            "Batch 110/4975 - Training Loss: 0.6707\n",
            "Batch 111/4975 - Training Loss: 0.5542\n",
            "Batch 112/4975 - Training Loss: 0.9284\n",
            "Batch 113/4975 - Training Loss: 0.5504\n",
            "Batch 114/4975 - Training Loss: 0.3735\n",
            "Batch 115/4975 - Training Loss: 0.6385\n",
            "Batch 116/4975 - Training Loss: 0.5218\n",
            "Batch 117/4975 - Training Loss: 0.7717\n",
            "Batch 118/4975 - Training Loss: 0.5878\n",
            "Batch 119/4975 - Training Loss: 0.5300\n",
            "Batch 120/4975 - Training Loss: 0.5000\n",
            "Batch 121/4975 - Training Loss: 0.7333\n",
            "Batch 122/4975 - Training Loss: 0.5247\n",
            "Batch 123/4975 - Training Loss: 0.4997\n",
            "Batch 124/4975 - Training Loss: 0.5404\n",
            "Batch 125/4975 - Training Loss: 0.2186\n",
            "Batch 126/4975 - Training Loss: 0.5038\n",
            "Batch 127/4975 - Training Loss: 0.6618\n",
            "Batch 128/4975 - Training Loss: 0.4980\n",
            "Batch 129/4975 - Training Loss: 0.4527\n",
            "Batch 130/4975 - Training Loss: 0.6779\n",
            "Batch 131/4975 - Training Loss: 0.8222\n",
            "Batch 132/4975 - Training Loss: 0.6840\n",
            "Batch 133/4975 - Training Loss: 0.4486\n",
            "Batch 134/4975 - Training Loss: 0.3739\n",
            "Batch 135/4975 - Training Loss: 0.3304\n",
            "Batch 136/4975 - Training Loss: 0.6306\n",
            "Batch 137/4975 - Training Loss: 0.6816\n",
            "Batch 138/4975 - Training Loss: 0.5646\n",
            "Batch 139/4975 - Training Loss: 0.8090\n",
            "Batch 140/4975 - Training Loss: 0.5629\n",
            "Batch 141/4975 - Training Loss: 0.3201\n",
            "Batch 142/4975 - Training Loss: 0.7345\n",
            "Batch 143/4975 - Training Loss: 0.5997\n",
            "Batch 144/4975 - Training Loss: 0.3550\n",
            "Batch 145/4975 - Training Loss: 0.3942\n",
            "Batch 146/4975 - Training Loss: 0.6481\n",
            "Batch 147/4975 - Training Loss: 0.3865\n",
            "Batch 148/4975 - Training Loss: 0.4987\n",
            "Batch 149/4975 - Training Loss: 0.5589\n",
            "Batch 150/4975 - Training Loss: 0.4035\n",
            "Batch 151/4975 - Training Loss: 0.4313\n",
            "Batch 152/4975 - Training Loss: 0.6696\n",
            "Batch 153/4975 - Training Loss: 0.4257\n",
            "Batch 154/4975 - Training Loss: 0.7406\n",
            "Batch 155/4975 - Training Loss: 0.5976\n",
            "Batch 156/4975 - Training Loss: 0.5144\n",
            "Batch 157/4975 - Training Loss: 0.3926\n",
            "Batch 158/4975 - Training Loss: 0.7861\n",
            "Batch 159/4975 - Training Loss: 0.6491\n",
            "Batch 160/4975 - Training Loss: 0.5305\n",
            "Batch 161/4975 - Training Loss: 0.5467\n",
            "Batch 162/4975 - Training Loss: 0.5546\n",
            "Batch 163/4975 - Training Loss: 0.3143\n",
            "Batch 164/4975 - Training Loss: 0.5529\n",
            "Batch 165/4975 - Training Loss: 0.4892\n",
            "Batch 166/4975 - Training Loss: 0.7166\n",
            "Batch 167/4975 - Training Loss: 0.7064\n",
            "Batch 168/4975 - Training Loss: 0.3941\n",
            "Batch 169/4975 - Training Loss: 0.5432\n",
            "Batch 170/4975 - Training Loss: 0.5539\n",
            "Batch 171/4975 - Training Loss: 0.6010\n",
            "Batch 172/4975 - Training Loss: 0.5112\n",
            "Batch 173/4975 - Training Loss: 0.6964\n",
            "Batch 174/4975 - Training Loss: 0.5476\n",
            "Batch 175/4975 - Training Loss: 0.8173\n",
            "Batch 176/4975 - Training Loss: 0.7405\n",
            "Batch 177/4975 - Training Loss: 0.5414\n",
            "Batch 178/4975 - Training Loss: 0.6274\n",
            "Batch 179/4975 - Training Loss: 0.5284\n",
            "Batch 180/4975 - Training Loss: 0.3626\n",
            "Batch 181/4975 - Training Loss: 0.8143\n",
            "Batch 182/4975 - Training Loss: 0.5914\n",
            "Batch 183/4975 - Training Loss: 0.5003\n",
            "Batch 184/4975 - Training Loss: 0.6307\n",
            "Batch 185/4975 - Training Loss: 0.5171\n",
            "Batch 186/4975 - Training Loss: 0.6477\n",
            "Batch 187/4975 - Training Loss: 0.4064\n",
            "Batch 188/4975 - Training Loss: 0.6555\n",
            "Batch 189/4975 - Training Loss: 0.4432\n",
            "Batch 190/4975 - Training Loss: 0.5566\n",
            "Batch 191/4975 - Training Loss: 0.6033\n",
            "Batch 192/4975 - Training Loss: 0.5372\n",
            "Batch 193/4975 - Training Loss: 0.7125\n",
            "Batch 194/4975 - Training Loss: 0.4085\n",
            "Batch 195/4975 - Training Loss: 0.6026\n",
            "Batch 196/4975 - Training Loss: 0.9402\n",
            "Batch 197/4975 - Training Loss: 0.4243\n",
            "Batch 198/4975 - Training Loss: 0.5287\n",
            "Batch 199/4975 - Training Loss: 0.5286\n",
            "Batch 200/4975 - Training Loss: 0.8594\n",
            "Batch 201/4975 - Training Loss: 0.3519\n",
            "Batch 202/4975 - Training Loss: 0.5261\n",
            "Batch 203/4975 - Training Loss: 0.6708\n",
            "Batch 204/4975 - Training Loss: 0.4884\n",
            "Batch 205/4975 - Training Loss: 0.5175\n",
            "Batch 206/4975 - Training Loss: 0.3548\n",
            "Batch 207/4975 - Training Loss: 0.5123\n",
            "Batch 208/4975 - Training Loss: 0.8753\n",
            "Batch 209/4975 - Training Loss: 0.7275\n",
            "Batch 210/4975 - Training Loss: 0.6152\n",
            "Batch 211/4975 - Training Loss: 0.8435\n",
            "Batch 212/4975 - Training Loss: 0.7747\n",
            "Batch 213/4975 - Training Loss: 0.7578\n",
            "Batch 214/4975 - Training Loss: 0.8873\n",
            "Batch 215/4975 - Training Loss: 0.6276\n",
            "Batch 216/4975 - Training Loss: 0.4845\n",
            "Batch 217/4975 - Training Loss: 0.3546\n",
            "Batch 218/4975 - Training Loss: 0.5660\n",
            "Batch 219/4975 - Training Loss: 0.4450\n",
            "Batch 220/4975 - Training Loss: 0.4939\n",
            "Batch 221/4975 - Training Loss: 0.5950\n",
            "Batch 222/4975 - Training Loss: 0.6732\n",
            "Batch 223/4975 - Training Loss: 0.9042\n",
            "Batch 224/4975 - Training Loss: 0.5771\n",
            "Batch 225/4975 - Training Loss: 0.4677\n",
            "Batch 226/4975 - Training Loss: 0.5132\n",
            "Batch 227/4975 - Training Loss: 0.9415\n",
            "Batch 228/4975 - Training Loss: 0.5426\n",
            "Batch 229/4975 - Training Loss: 0.6358\n",
            "Batch 230/4975 - Training Loss: 0.4512\n",
            "Batch 231/4975 - Training Loss: 0.6346\n",
            "Batch 232/4975 - Training Loss: 0.4174\n",
            "Batch 233/4975 - Training Loss: 0.5566\n",
            "Batch 234/4975 - Training Loss: 0.7746\n",
            "Batch 235/4975 - Training Loss: 0.5769\n",
            "Batch 236/4975 - Training Loss: 0.4383\n",
            "Batch 237/4975 - Training Loss: 0.6135\n",
            "Batch 238/4975 - Training Loss: 0.7216\n",
            "Batch 239/4975 - Training Loss: 0.3429\n",
            "Batch 240/4975 - Training Loss: 0.6550\n",
            "Batch 241/4975 - Training Loss: 0.7578\n",
            "Batch 242/4975 - Training Loss: 0.4049\n",
            "Batch 243/4975 - Training Loss: 0.6759\n",
            "Batch 244/4975 - Training Loss: 0.5565\n",
            "Batch 245/4975 - Training Loss: 0.6463\n",
            "Batch 246/4975 - Training Loss: 0.2739\n",
            "Batch 247/4975 - Training Loss: 0.2201\n",
            "Batch 248/4975 - Training Loss: 0.6713\n",
            "Batch 249/4975 - Training Loss: 0.5012\n",
            "Batch 250/4975 - Training Loss: 0.9191\n",
            "Batch 251/4975 - Training Loss: 0.4830\n",
            "Batch 252/4975 - Training Loss: 0.6968\n",
            "Batch 253/4975 - Training Loss: 0.4970\n",
            "Batch 254/4975 - Training Loss: 0.5465\n",
            "Batch 255/4975 - Training Loss: 0.5918\n",
            "Batch 256/4975 - Training Loss: 0.6353\n",
            "Batch 257/4975 - Training Loss: 0.5168\n",
            "Batch 258/4975 - Training Loss: 0.6738\n",
            "Batch 259/4975 - Training Loss: 0.7443\n",
            "Batch 260/4975 - Training Loss: 0.7320\n",
            "Batch 261/4975 - Training Loss: 0.6800\n",
            "Batch 262/4975 - Training Loss: 0.5391\n",
            "Batch 263/4975 - Training Loss: 0.7869\n",
            "Batch 264/4975 - Training Loss: 0.2699\n",
            "Batch 265/4975 - Training Loss: 0.9339\n",
            "Batch 266/4975 - Training Loss: 0.5362\n",
            "Batch 267/4975 - Training Loss: 0.6221\n",
            "Batch 268/4975 - Training Loss: 0.5212\n",
            "Batch 269/4975 - Training Loss: 0.5524\n",
            "Batch 270/4975 - Training Loss: 0.6916\n",
            "Batch 271/4975 - Training Loss: 0.4878\n",
            "Batch 272/4975 - Training Loss: 0.5600\n",
            "Batch 273/4975 - Training Loss: 0.5283\n",
            "Batch 274/4975 - Training Loss: 0.7358\n",
            "Batch 275/4975 - Training Loss: 0.6184\n",
            "Batch 276/4975 - Training Loss: 0.3894\n",
            "Batch 277/4975 - Training Loss: 0.6582\n",
            "Batch 278/4975 - Training Loss: 0.7350\n",
            "Batch 279/4975 - Training Loss: 0.9749\n",
            "Batch 280/4975 - Training Loss: 0.4941\n",
            "Batch 281/4975 - Training Loss: 0.8964\n",
            "Batch 282/4975 - Training Loss: 0.5331\n",
            "Batch 283/4975 - Training Loss: 0.5574\n",
            "Batch 284/4975 - Training Loss: 0.7702\n",
            "Batch 285/4975 - Training Loss: 0.5099\n",
            "Batch 286/4975 - Training Loss: 0.6360\n",
            "Batch 287/4975 - Training Loss: 0.9595\n",
            "Batch 288/4975 - Training Loss: 0.8038\n",
            "Batch 289/4975 - Training Loss: 0.7339\n",
            "Batch 290/4975 - Training Loss: 0.6669\n",
            "Batch 291/4975 - Training Loss: 0.7053\n",
            "Batch 292/4975 - Training Loss: 0.4251\n",
            "Batch 293/4975 - Training Loss: 0.4830\n",
            "Batch 294/4975 - Training Loss: 0.4700\n",
            "Batch 295/4975 - Training Loss: 0.6040\n",
            "Batch 296/4975 - Training Loss: 0.5467\n",
            "Batch 297/4975 - Training Loss: 0.4925\n",
            "Batch 298/4975 - Training Loss: 0.6238\n",
            "Batch 299/4975 - Training Loss: 0.4723\n",
            "Batch 300/4975 - Training Loss: 0.5734\n",
            "Batch 301/4975 - Training Loss: 1.0799\n",
            "Batch 302/4975 - Training Loss: 0.3380\n",
            "Batch 303/4975 - Training Loss: 0.7341\n",
            "Batch 304/4975 - Training Loss: 0.5237\n",
            "Batch 305/4975 - Training Loss: 0.4845\n",
            "Batch 306/4975 - Training Loss: 0.5348\n",
            "Batch 307/4975 - Training Loss: 0.3943\n",
            "Batch 308/4975 - Training Loss: 0.3342\n",
            "Batch 309/4975 - Training Loss: 0.3847\n",
            "Batch 310/4975 - Training Loss: 0.6533\n",
            "Batch 311/4975 - Training Loss: 0.6354\n",
            "Batch 312/4975 - Training Loss: 0.6599\n",
            "Batch 313/4975 - Training Loss: 0.7194\n",
            "Batch 314/4975 - Training Loss: 0.6033\n",
            "Batch 315/4975 - Training Loss: 0.5064\n",
            "Batch 316/4975 - Training Loss: 0.6520\n",
            "Batch 317/4975 - Training Loss: 0.4963\n",
            "Batch 318/4975 - Training Loss: 0.6143\n",
            "Batch 319/4975 - Training Loss: 0.3169\n",
            "Batch 320/4975 - Training Loss: 0.5402\n",
            "Batch 321/4975 - Training Loss: 0.7296\n",
            "Batch 322/4975 - Training Loss: 0.5492\n",
            "Batch 323/4975 - Training Loss: 0.5515\n",
            "Batch 324/4975 - Training Loss: 0.4548\n",
            "Batch 325/4975 - Training Loss: 0.8146\n",
            "Batch 326/4975 - Training Loss: 0.4640\n",
            "Batch 327/4975 - Training Loss: 0.5674\n",
            "Batch 328/4975 - Training Loss: 0.5848\n",
            "Batch 329/4975 - Training Loss: 0.6781\n",
            "Batch 330/4975 - Training Loss: 0.7978\n",
            "Batch 331/4975 - Training Loss: 0.6282\n",
            "Batch 332/4975 - Training Loss: 0.5324\n",
            "Batch 333/4975 - Training Loss: 0.8434\n",
            "Batch 334/4975 - Training Loss: 0.7604\n",
            "Batch 335/4975 - Training Loss: 0.7141\n",
            "Batch 336/4975 - Training Loss: 0.4924\n",
            "Batch 337/4975 - Training Loss: 0.4516\n",
            "Batch 338/4975 - Training Loss: 0.5951\n",
            "Batch 339/4975 - Training Loss: 0.6321\n",
            "Batch 340/4975 - Training Loss: 0.5019\n",
            "Batch 341/4975 - Training Loss: 0.5651\n",
            "Batch 342/4975 - Training Loss: 0.6041\n",
            "Batch 343/4975 - Training Loss: 0.8120\n",
            "Batch 344/4975 - Training Loss: 0.5370\n",
            "Batch 345/4975 - Training Loss: 0.6558\n",
            "Batch 346/4975 - Training Loss: 0.6607\n",
            "Batch 347/4975 - Training Loss: 0.5426\n",
            "Batch 348/4975 - Training Loss: 0.4708\n",
            "Batch 349/4975 - Training Loss: 0.7090\n",
            "Batch 350/4975 - Training Loss: 0.7540\n",
            "Batch 351/4975 - Training Loss: 0.5573\n",
            "Batch 352/4975 - Training Loss: 0.7549\n",
            "Batch 353/4975 - Training Loss: 0.7225\n",
            "Batch 354/4975 - Training Loss: 0.3949\n",
            "Batch 355/4975 - Training Loss: 0.3466\n",
            "Batch 356/4975 - Training Loss: 0.8710\n",
            "Batch 357/4975 - Training Loss: 0.7944\n",
            "Batch 358/4975 - Training Loss: 0.4025\n",
            "Batch 359/4975 - Training Loss: 0.7294\n",
            "Batch 360/4975 - Training Loss: 0.8254\n",
            "Batch 361/4975 - Training Loss: 0.4121\n",
            "Batch 362/4975 - Training Loss: 0.4868\n",
            "Batch 363/4975 - Training Loss: 0.5393\n",
            "Batch 364/4975 - Training Loss: 0.8234\n",
            "Batch 365/4975 - Training Loss: 0.4946\n",
            "Batch 366/4975 - Training Loss: 0.5846\n",
            "Batch 367/4975 - Training Loss: 0.5227\n",
            "Batch 368/4975 - Training Loss: 0.8743\n",
            "Batch 369/4975 - Training Loss: 0.6775\n",
            "Batch 370/4975 - Training Loss: 0.4611\n",
            "Batch 371/4975 - Training Loss: 0.7244\n",
            "Batch 372/4975 - Training Loss: 0.6951\n",
            "Batch 373/4975 - Training Loss: 0.6642\n",
            "Batch 374/4975 - Training Loss: 0.5443\n",
            "Batch 375/4975 - Training Loss: 0.4507\n",
            "Batch 376/4975 - Training Loss: 0.9132\n",
            "Batch 377/4975 - Training Loss: 0.6252\n",
            "Batch 378/4975 - Training Loss: 0.4983\n",
            "Batch 379/4975 - Training Loss: 0.4963\n",
            "Batch 380/4975 - Training Loss: 0.5030\n",
            "Batch 381/4975 - Training Loss: 0.7429\n",
            "Batch 382/4975 - Training Loss: 0.5153\n",
            "Batch 383/4975 - Training Loss: 0.4143\n",
            "Batch 384/4975 - Training Loss: 0.7455\n",
            "Batch 385/4975 - Training Loss: 0.6310\n",
            "Batch 386/4975 - Training Loss: 0.4270\n",
            "Batch 387/4975 - Training Loss: 0.3135\n",
            "Batch 388/4975 - Training Loss: 0.4294\n",
            "Batch 389/4975 - Training Loss: 0.6238\n",
            "Batch 390/4975 - Training Loss: 0.5731\n",
            "Batch 391/4975 - Training Loss: 0.4367\n",
            "Batch 392/4975 - Training Loss: 0.5183\n",
            "Batch 393/4975 - Training Loss: 0.7714\n",
            "Batch 394/4975 - Training Loss: 0.4598\n",
            "Batch 395/4975 - Training Loss: 0.5982\n",
            "Batch 396/4975 - Training Loss: 0.7522\n",
            "Batch 397/4975 - Training Loss: 0.4813\n",
            "Batch 398/4975 - Training Loss: 0.5705\n",
            "Batch 399/4975 - Training Loss: 0.3899\n",
            "Batch 400/4975 - Training Loss: 0.5810\n",
            "Batch 401/4975 - Training Loss: 0.3959\n",
            "Batch 402/4975 - Training Loss: 0.4694\n",
            "Batch 403/4975 - Training Loss: 0.7477\n",
            "Batch 404/4975 - Training Loss: 0.6994\n",
            "Batch 405/4975 - Training Loss: 0.3748\n",
            "Batch 406/4975 - Training Loss: 0.4090\n",
            "Batch 407/4975 - Training Loss: 0.3566\n",
            "Batch 408/4975 - Training Loss: 0.5754\n",
            "Batch 409/4975 - Training Loss: 0.3743\n",
            "Batch 410/4975 - Training Loss: 0.5950\n",
            "Batch 411/4975 - Training Loss: 0.5723\n",
            "Batch 412/4975 - Training Loss: 0.6734\n",
            "Batch 413/4975 - Training Loss: 0.5808\n",
            "Batch 414/4975 - Training Loss: 0.5054\n",
            "Batch 415/4975 - Training Loss: 0.3840\n",
            "Batch 416/4975 - Training Loss: 0.9206\n",
            "Batch 417/4975 - Training Loss: 0.5520\n",
            "Batch 418/4975 - Training Loss: 0.7532\n",
            "Batch 419/4975 - Training Loss: 0.6272\n",
            "Batch 420/4975 - Training Loss: 0.3544\n",
            "Batch 421/4975 - Training Loss: 0.5912\n",
            "Batch 422/4975 - Training Loss: 0.3101\n",
            "Batch 423/4975 - Training Loss: 0.5177\n",
            "Batch 424/4975 - Training Loss: 0.6226\n",
            "Batch 425/4975 - Training Loss: 0.5129\n",
            "Batch 426/4975 - Training Loss: 0.4690\n",
            "Batch 427/4975 - Training Loss: 0.7620\n",
            "Batch 428/4975 - Training Loss: 0.5571\n",
            "Batch 429/4975 - Training Loss: 0.8312\n",
            "Batch 430/4975 - Training Loss: 0.4381\n",
            "Batch 431/4975 - Training Loss: 0.7031\n",
            "Batch 432/4975 - Training Loss: 0.3106\n",
            "Batch 433/4975 - Training Loss: 0.5860\n",
            "Batch 434/4975 - Training Loss: 0.5935\n",
            "Batch 435/4975 - Training Loss: 0.4148\n",
            "Batch 436/4975 - Training Loss: 0.8570\n",
            "Batch 437/4975 - Training Loss: 0.3355\n",
            "Batch 438/4975 - Training Loss: 0.7128\n",
            "Batch 439/4975 - Training Loss: 0.5692\n",
            "Batch 440/4975 - Training Loss: 0.3676\n",
            "Batch 441/4975 - Training Loss: 0.9003\n",
            "Batch 442/4975 - Training Loss: 0.8621\n",
            "Batch 443/4975 - Training Loss: 0.7591\n",
            "Batch 444/4975 - Training Loss: 0.4733\n",
            "Batch 445/4975 - Training Loss: 0.4151\n",
            "Batch 446/4975 - Training Loss: 0.6196\n",
            "Batch 447/4975 - Training Loss: 0.5504\n",
            "Batch 448/4975 - Training Loss: 0.6843\n",
            "Batch 449/4975 - Training Loss: 0.3788\n",
            "Batch 450/4975 - Training Loss: 0.6536\n",
            "Batch 451/4975 - Training Loss: 0.5789\n",
            "Batch 452/4975 - Training Loss: 0.8316\n",
            "Batch 453/4975 - Training Loss: 0.6294\n",
            "Batch 454/4975 - Training Loss: 0.5272\n",
            "Batch 455/4975 - Training Loss: 0.7173\n",
            "Batch 456/4975 - Training Loss: 0.5542\n",
            "Batch 457/4975 - Training Loss: 0.6907\n",
            "Batch 458/4975 - Training Loss: 0.4669\n",
            "Batch 459/4975 - Training Loss: 0.9366\n",
            "Batch 460/4975 - Training Loss: 0.6886\n",
            "Batch 461/4975 - Training Loss: 0.4928\n",
            "Batch 462/4975 - Training Loss: 0.4083\n",
            "Batch 463/4975 - Training Loss: 0.7628\n",
            "Batch 464/4975 - Training Loss: 0.6502\n",
            "Batch 465/4975 - Training Loss: 0.4584\n",
            "Batch 466/4975 - Training Loss: 0.5063\n",
            "Batch 467/4975 - Training Loss: 0.6472\n",
            "Batch 468/4975 - Training Loss: 0.5041\n",
            "Batch 469/4975 - Training Loss: 0.6501\n",
            "Batch 470/4975 - Training Loss: 0.9237\n",
            "Batch 471/4975 - Training Loss: 0.7085\n",
            "Batch 472/4975 - Training Loss: 0.5682\n",
            "Batch 473/4975 - Training Loss: 0.6889\n",
            "Batch 474/4975 - Training Loss: 0.6128\n",
            "Batch 475/4975 - Training Loss: 0.8704\n",
            "Batch 476/4975 - Training Loss: 0.5544\n",
            "Batch 477/4975 - Training Loss: 0.4994\n",
            "Batch 478/4975 - Training Loss: 0.3528\n",
            "Batch 479/4975 - Training Loss: 0.3967\n",
            "Batch 480/4975 - Training Loss: 0.3198\n",
            "Batch 481/4975 - Training Loss: 0.5168\n",
            "Batch 482/4975 - Training Loss: 0.9419\n",
            "Batch 483/4975 - Training Loss: 0.4620\n",
            "Batch 484/4975 - Training Loss: 0.5131\n",
            "Batch 485/4975 - Training Loss: 0.5440\n",
            "Batch 486/4975 - Training Loss: 0.6150\n",
            "Batch 487/4975 - Training Loss: 0.7012\n",
            "Batch 488/4975 - Training Loss: 0.5832\n",
            "Batch 489/4975 - Training Loss: 0.4552\n",
            "Batch 490/4975 - Training Loss: 0.5129\n",
            "Batch 491/4975 - Training Loss: 0.7360\n",
            "Batch 492/4975 - Training Loss: 0.4865\n",
            "Batch 493/4975 - Training Loss: 1.0460\n",
            "Batch 494/4975 - Training Loss: 0.5322\n",
            "Batch 495/4975 - Training Loss: 0.4322\n",
            "Batch 496/4975 - Training Loss: 0.5071\n",
            "Batch 497/4975 - Training Loss: 0.7782\n",
            "Batch 498/4975 - Training Loss: 0.6659\n",
            "Batch 499/4975 - Training Loss: 0.4364\n",
            "Batch 500/4975 - Training Loss: 0.4500\n",
            "Batch 501/4975 - Training Loss: 0.5450\n",
            "Batch 502/4975 - Training Loss: 0.5304\n",
            "Batch 503/4975 - Training Loss: 0.6132\n",
            "Batch 504/4975 - Training Loss: 0.7469\n",
            "Batch 505/4975 - Training Loss: 0.6877\n",
            "Batch 506/4975 - Training Loss: 0.6245\n",
            "Batch 507/4975 - Training Loss: 0.3984\n",
            "Batch 508/4975 - Training Loss: 0.5501\n",
            "Batch 509/4975 - Training Loss: 0.4636\n",
            "Batch 510/4975 - Training Loss: 0.5719\n",
            "Batch 511/4975 - Training Loss: 0.5378\n",
            "Batch 512/4975 - Training Loss: 0.6161\n",
            "Batch 513/4975 - Training Loss: 0.4974\n",
            "Batch 514/4975 - Training Loss: 0.5955\n",
            "Batch 515/4975 - Training Loss: 0.6590\n",
            "Batch 516/4975 - Training Loss: 0.7143\n",
            "Batch 517/4975 - Training Loss: 0.4154\n",
            "Batch 518/4975 - Training Loss: 0.3793\n",
            "Batch 519/4975 - Training Loss: 0.3703\n",
            "Batch 520/4975 - Training Loss: 0.6596\n",
            "Batch 521/4975 - Training Loss: 0.4223\n",
            "Batch 522/4975 - Training Loss: 0.9794\n",
            "Batch 523/4975 - Training Loss: 0.6997\n",
            "Batch 524/4975 - Training Loss: 0.7352\n",
            "Batch 525/4975 - Training Loss: 0.7442\n",
            "Batch 526/4975 - Training Loss: 0.5327\n",
            "Batch 527/4975 - Training Loss: 0.9193\n",
            "Batch 528/4975 - Training Loss: 0.8239\n",
            "Batch 529/4975 - Training Loss: 0.8027\n",
            "Batch 530/4975 - Training Loss: 0.6098\n",
            "Batch 531/4975 - Training Loss: 0.6233\n",
            "Batch 532/4975 - Training Loss: 0.3700\n",
            "Batch 533/4975 - Training Loss: 0.7325\n",
            "Batch 534/4975 - Training Loss: 0.5722\n",
            "Batch 535/4975 - Training Loss: 0.5821\n",
            "Batch 536/4975 - Training Loss: 0.4430\n",
            "Batch 537/4975 - Training Loss: 0.6703\n",
            "Batch 538/4975 - Training Loss: 0.6802\n",
            "Batch 539/4975 - Training Loss: 0.5168\n",
            "Batch 540/4975 - Training Loss: 0.5759\n",
            "Batch 541/4975 - Training Loss: 0.4973\n",
            "Batch 542/4975 - Training Loss: 0.6229\n",
            "Batch 543/4975 - Training Loss: 0.7522\n",
            "Batch 544/4975 - Training Loss: 0.5668\n",
            "Batch 545/4975 - Training Loss: 0.8062\n",
            "Batch 546/4975 - Training Loss: 0.4683\n",
            "Batch 547/4975 - Training Loss: 0.4807\n",
            "Batch 548/4975 - Training Loss: 0.4461\n",
            "Batch 549/4975 - Training Loss: 0.7412\n",
            "Batch 550/4975 - Training Loss: 0.5152\n",
            "Batch 551/4975 - Training Loss: 0.7890\n",
            "Batch 552/4975 - Training Loss: 0.3604\n",
            "Batch 553/4975 - Training Loss: 0.5541\n",
            "Batch 554/4975 - Training Loss: 0.9259\n",
            "Batch 555/4975 - Training Loss: 0.6444\n",
            "Batch 556/4975 - Training Loss: 0.4845\n",
            "Batch 557/4975 - Training Loss: 0.8313\n",
            "Batch 558/4975 - Training Loss: 0.6278\n",
            "Batch 559/4975 - Training Loss: 0.8749\n",
            "Batch 560/4975 - Training Loss: 0.6199\n",
            "Batch 561/4975 - Training Loss: 0.4466\n",
            "Batch 562/4975 - Training Loss: 0.7992\n",
            "Batch 563/4975 - Training Loss: 0.4103\n",
            "Batch 564/4975 - Training Loss: 0.5346\n",
            "Batch 565/4975 - Training Loss: 0.3443\n",
            "Batch 566/4975 - Training Loss: 0.4563\n",
            "Batch 567/4975 - Training Loss: 0.3935\n",
            "Batch 568/4975 - Training Loss: 0.4791\n",
            "Batch 569/4975 - Training Loss: 0.5206\n",
            "Batch 570/4975 - Training Loss: 0.6277\n",
            "Batch 571/4975 - Training Loss: 0.8964\n",
            "Batch 572/4975 - Training Loss: 0.4952\n",
            "Batch 573/4975 - Training Loss: 0.7898\n",
            "Batch 574/4975 - Training Loss: 0.8103\n",
            "Batch 575/4975 - Training Loss: 0.7565\n",
            "Batch 576/4975 - Training Loss: 0.5540\n",
            "Batch 577/4975 - Training Loss: 0.4637\n",
            "Batch 578/4975 - Training Loss: 0.4723\n",
            "Batch 579/4975 - Training Loss: 0.6964\n",
            "Batch 580/4975 - Training Loss: 0.7198\n",
            "Batch 581/4975 - Training Loss: 0.5758\n",
            "Batch 582/4975 - Training Loss: 0.7395\n",
            "Batch 583/4975 - Training Loss: 0.6910\n",
            "Batch 584/4975 - Training Loss: 0.4221\n",
            "Batch 585/4975 - Training Loss: 0.4706\n",
            "Batch 586/4975 - Training Loss: 0.3497\n",
            "Batch 587/4975 - Training Loss: 0.6272\n",
            "Batch 588/4975 - Training Loss: 0.5138\n",
            "Batch 589/4975 - Training Loss: 0.6157\n",
            "Batch 590/4975 - Training Loss: 0.3954\n",
            "Batch 591/4975 - Training Loss: 0.6898\n",
            "Batch 592/4975 - Training Loss: 0.7843\n",
            "Batch 593/4975 - Training Loss: 0.6516\n",
            "Batch 594/4975 - Training Loss: 0.9352\n",
            "Batch 595/4975 - Training Loss: 0.7968\n",
            "Batch 596/4975 - Training Loss: 0.5791\n",
            "Batch 597/4975 - Training Loss: 0.7847\n",
            "Batch 598/4975 - Training Loss: 0.7764\n",
            "Batch 599/4975 - Training Loss: 0.7247\n",
            "Batch 600/4975 - Training Loss: 0.6675\n",
            "Batch 601/4975 - Training Loss: 0.8797\n",
            "Batch 602/4975 - Training Loss: 0.5906\n",
            "Batch 603/4975 - Training Loss: 0.4747\n",
            "Batch 604/4975 - Training Loss: 0.3408\n",
            "Batch 605/4975 - Training Loss: 0.5013\n",
            "Batch 606/4975 - Training Loss: 0.5930\n",
            "Batch 607/4975 - Training Loss: 0.3750\n",
            "Batch 608/4975 - Training Loss: 0.4635\n",
            "Batch 609/4975 - Training Loss: 0.6126\n",
            "Batch 610/4975 - Training Loss: 0.4110\n",
            "Batch 611/4975 - Training Loss: 0.6626\n",
            "Batch 612/4975 - Training Loss: 0.7579\n",
            "Batch 613/4975 - Training Loss: 0.8001\n",
            "Batch 614/4975 - Training Loss: 0.9173\n",
            "Batch 615/4975 - Training Loss: 0.6463\n",
            "Batch 616/4975 - Training Loss: 0.6247\n",
            "Batch 617/4975 - Training Loss: 0.5022\n",
            "Batch 618/4975 - Training Loss: 0.4741\n",
            "Batch 619/4975 - Training Loss: 0.5358\n",
            "Batch 620/4975 - Training Loss: 0.5963\n",
            "Batch 621/4975 - Training Loss: 0.6582\n",
            "Batch 622/4975 - Training Loss: 0.5895\n",
            "Batch 623/4975 - Training Loss: 0.5631\n",
            "Batch 624/4975 - Training Loss: 0.4484\n",
            "Batch 625/4975 - Training Loss: 0.4943\n",
            "Batch 626/4975 - Training Loss: 0.4475\n",
            "Batch 627/4975 - Training Loss: 0.5586\n",
            "Batch 628/4975 - Training Loss: 0.4226\n",
            "Batch 629/4975 - Training Loss: 0.4213\n",
            "Batch 630/4975 - Training Loss: 0.6869\n",
            "Batch 631/4975 - Training Loss: 0.4379\n",
            "Batch 632/4975 - Training Loss: 0.5376\n",
            "Batch 633/4975 - Training Loss: 0.5183\n",
            "Batch 634/4975 - Training Loss: 0.5669\n",
            "Batch 635/4975 - Training Loss: 0.6782\n",
            "Batch 636/4975 - Training Loss: 0.3149\n",
            "Batch 637/4975 - Training Loss: 0.6207\n",
            "Batch 638/4975 - Training Loss: 0.7262\n",
            "Batch 639/4975 - Training Loss: 0.8180\n",
            "Batch 640/4975 - Training Loss: 0.7270\n",
            "Batch 641/4975 - Training Loss: 0.4837\n",
            "Batch 642/4975 - Training Loss: 0.4896\n",
            "Batch 643/4975 - Training Loss: 0.4373\n",
            "Batch 644/4975 - Training Loss: 0.7691\n",
            "Batch 645/4975 - Training Loss: 0.5240\n",
            "Batch 646/4975 - Training Loss: 0.6748\n",
            "Batch 647/4975 - Training Loss: 0.6973\n",
            "Batch 648/4975 - Training Loss: 0.5986\n",
            "Batch 649/4975 - Training Loss: 0.5634\n",
            "Batch 650/4975 - Training Loss: 0.6473\n",
            "Batch 651/4975 - Training Loss: 1.0720\n",
            "Batch 652/4975 - Training Loss: 0.5751\n",
            "Batch 653/4975 - Training Loss: 0.3977\n",
            "Batch 654/4975 - Training Loss: 0.6233\n",
            "Batch 655/4975 - Training Loss: 0.6052\n",
            "Batch 656/4975 - Training Loss: 0.8513\n",
            "Batch 657/4975 - Training Loss: 0.6241\n",
            "Batch 658/4975 - Training Loss: 0.4405\n",
            "Batch 659/4975 - Training Loss: 0.4004\n",
            "Batch 660/4975 - Training Loss: 0.6094\n",
            "Batch 661/4975 - Training Loss: 0.6065\n",
            "Batch 662/4975 - Training Loss: 0.5057\n",
            "Batch 663/4975 - Training Loss: 0.9739\n",
            "Batch 664/4975 - Training Loss: 0.7788\n",
            "Batch 665/4975 - Training Loss: 0.7279\n",
            "Batch 666/4975 - Training Loss: 0.6645\n",
            "Batch 667/4975 - Training Loss: 0.5583\n",
            "Batch 668/4975 - Training Loss: 0.5563\n",
            "Batch 669/4975 - Training Loss: 0.8196\n",
            "Batch 670/4975 - Training Loss: 0.3174\n",
            "Batch 671/4975 - Training Loss: 0.4269\n",
            "Batch 672/4975 - Training Loss: 0.9791\n",
            "Batch 673/4975 - Training Loss: 0.5848\n",
            "Batch 674/4975 - Training Loss: 0.4882\n",
            "Batch 675/4975 - Training Loss: 0.8839\n",
            "Batch 676/4975 - Training Loss: 0.8229\n",
            "Batch 677/4975 - Training Loss: 0.8106\n",
            "Batch 678/4975 - Training Loss: 0.5669\n",
            "Batch 679/4975 - Training Loss: 0.3902\n",
            "Batch 680/4975 - Training Loss: 0.7386\n",
            "Batch 681/4975 - Training Loss: 0.3200\n",
            "Batch 682/4975 - Training Loss: 0.5253\n",
            "Batch 683/4975 - Training Loss: 0.5212\n",
            "Batch 684/4975 - Training Loss: 0.6285\n",
            "Batch 685/4975 - Training Loss: 0.3275\n",
            "Batch 686/4975 - Training Loss: 0.5475\n",
            "Batch 687/4975 - Training Loss: 0.4357\n",
            "Batch 688/4975 - Training Loss: 0.4316\n",
            "Batch 689/4975 - Training Loss: 0.6512\n",
            "Batch 690/4975 - Training Loss: 0.7165\n",
            "Batch 691/4975 - Training Loss: 0.6284\n",
            "Batch 692/4975 - Training Loss: 0.7958\n",
            "Batch 693/4975 - Training Loss: 0.5081\n",
            "Batch 694/4975 - Training Loss: 0.5145\n",
            "Batch 695/4975 - Training Loss: 0.4759\n",
            "Batch 696/4975 - Training Loss: 0.6688\n",
            "Batch 697/4975 - Training Loss: 0.9844\n",
            "Batch 698/4975 - Training Loss: 0.4305\n",
            "Batch 699/4975 - Training Loss: 0.6173\n",
            "Batch 700/4975 - Training Loss: 0.2805\n",
            "Batch 701/4975 - Training Loss: 0.7236\n",
            "Batch 702/4975 - Training Loss: 0.5055\n",
            "Batch 703/4975 - Training Loss: 0.8427\n",
            "Batch 704/4975 - Training Loss: 0.3460\n",
            "Batch 705/4975 - Training Loss: 0.6055\n",
            "Batch 706/4975 - Training Loss: 0.7165\n",
            "Batch 707/4975 - Training Loss: 0.5858\n",
            "Batch 708/4975 - Training Loss: 0.7654\n",
            "Batch 709/4975 - Training Loss: 0.5826\n",
            "Batch 710/4975 - Training Loss: 0.8934\n",
            "Batch 711/4975 - Training Loss: 0.3948\n",
            "Batch 712/4975 - Training Loss: 0.7952\n",
            "Batch 713/4975 - Training Loss: 0.6343\n",
            "Batch 714/4975 - Training Loss: 0.4940\n",
            "Batch 715/4975 - Training Loss: 0.5341\n",
            "Batch 716/4975 - Training Loss: 0.4979\n",
            "Batch 717/4975 - Training Loss: 0.3927\n",
            "Batch 718/4975 - Training Loss: 0.7134\n",
            "Batch 719/4975 - Training Loss: 0.7979\n",
            "Batch 720/4975 - Training Loss: 0.5755\n",
            "Batch 721/4975 - Training Loss: 0.5307\n",
            "Batch 722/4975 - Training Loss: 0.5775\n",
            "Batch 723/4975 - Training Loss: 0.6983\n",
            "Batch 724/4975 - Training Loss: 0.4703\n",
            "Batch 725/4975 - Training Loss: 0.4444\n",
            "Batch 726/4975 - Training Loss: 0.5569\n",
            "Batch 727/4975 - Training Loss: 0.8426\n",
            "Batch 728/4975 - Training Loss: 0.5425\n",
            "Batch 729/4975 - Training Loss: 0.4309\n",
            "Batch 730/4975 - Training Loss: 0.6726\n",
            "Batch 731/4975 - Training Loss: 0.5198\n",
            "Batch 732/4975 - Training Loss: 0.7040\n",
            "Batch 733/4975 - Training Loss: 0.6740\n",
            "Batch 734/4975 - Training Loss: 0.3922\n",
            "Batch 735/4975 - Training Loss: 0.3812\n",
            "Batch 736/4975 - Training Loss: 0.6720\n",
            "Batch 737/4975 - Training Loss: 0.6543\n",
            "Batch 738/4975 - Training Loss: 0.6005\n",
            "Batch 739/4975 - Training Loss: 0.5736\n",
            "Batch 740/4975 - Training Loss: 0.7760\n",
            "Batch 741/4975 - Training Loss: 0.3994\n",
            "Batch 742/4975 - Training Loss: 0.8307\n",
            "Batch 743/4975 - Training Loss: 0.5811\n",
            "Batch 744/4975 - Training Loss: 0.5954\n",
            "Batch 745/4975 - Training Loss: 0.5241\n",
            "Batch 746/4975 - Training Loss: 0.6658\n",
            "Batch 747/4975 - Training Loss: 0.8107\n",
            "Batch 748/4975 - Training Loss: 0.9120\n",
            "Batch 749/4975 - Training Loss: 0.4307\n",
            "Batch 750/4975 - Training Loss: 0.5221\n",
            "Batch 751/4975 - Training Loss: 0.4046\n",
            "Batch 752/4975 - Training Loss: 0.4344\n",
            "Batch 753/4975 - Training Loss: 0.8487\n",
            "Batch 754/4975 - Training Loss: 0.4934\n",
            "Batch 755/4975 - Training Loss: 0.8988\n",
            "Batch 756/4975 - Training Loss: 0.6316\n",
            "Batch 757/4975 - Training Loss: 0.4572\n",
            "Batch 758/4975 - Training Loss: 0.5422\n",
            "Batch 759/4975 - Training Loss: 0.4198\n",
            "Batch 760/4975 - Training Loss: 0.4965\n",
            "Batch 761/4975 - Training Loss: 0.4011\n",
            "Batch 762/4975 - Training Loss: 1.2021\n",
            "Batch 763/4975 - Training Loss: 0.3387\n",
            "Batch 764/4975 - Training Loss: 0.8729\n",
            "Batch 765/4975 - Training Loss: 0.5494\n",
            "Batch 766/4975 - Training Loss: 0.6142\n",
            "Batch 767/4975 - Training Loss: 0.6315\n",
            "Batch 768/4975 - Training Loss: 0.5112\n",
            "Batch 769/4975 - Training Loss: 0.7211\n",
            "Batch 770/4975 - Training Loss: 0.9109\n",
            "Batch 771/4975 - Training Loss: 0.5868\n",
            "Batch 772/4975 - Training Loss: 0.4494\n",
            "Batch 773/4975 - Training Loss: 0.5362\n",
            "Batch 774/4975 - Training Loss: 0.2982\n",
            "Batch 775/4975 - Training Loss: 0.4216\n",
            "Batch 776/4975 - Training Loss: 0.3950\n",
            "Batch 777/4975 - Training Loss: 0.4367\n",
            "Batch 778/4975 - Training Loss: 0.5648\n",
            "Batch 779/4975 - Training Loss: 0.7656\n",
            "Batch 780/4975 - Training Loss: 0.8146\n",
            "Batch 781/4975 - Training Loss: 0.4532\n",
            "Batch 782/4975 - Training Loss: 0.7087\n",
            "Batch 783/4975 - Training Loss: 0.5479\n",
            "Batch 784/4975 - Training Loss: 0.7328\n",
            "Batch 785/4975 - Training Loss: 0.4935\n",
            "Batch 786/4975 - Training Loss: 0.6404\n",
            "Batch 787/4975 - Training Loss: 0.8031\n",
            "Batch 788/4975 - Training Loss: 0.6963\n",
            "Batch 789/4975 - Training Loss: 0.7072\n",
            "Batch 790/4975 - Training Loss: 0.7937\n",
            "Batch 791/4975 - Training Loss: 0.8718\n",
            "Batch 792/4975 - Training Loss: 0.4749\n",
            "Batch 793/4975 - Training Loss: 1.0875\n",
            "Batch 794/4975 - Training Loss: 0.7463\n",
            "Batch 795/4975 - Training Loss: 0.2395\n",
            "Batch 796/4975 - Training Loss: 0.4826\n",
            "Batch 797/4975 - Training Loss: 0.6746\n",
            "Batch 798/4975 - Training Loss: 0.8370\n",
            "Batch 799/4975 - Training Loss: 0.3649\n",
            "Batch 800/4975 - Training Loss: 0.3975\n",
            "Batch 801/4975 - Training Loss: 0.4609\n",
            "Batch 802/4975 - Training Loss: 0.5906\n",
            "Batch 803/4975 - Training Loss: 0.6132\n",
            "Batch 804/4975 - Training Loss: 0.8294\n",
            "Batch 805/4975 - Training Loss: 0.7596\n",
            "Batch 806/4975 - Training Loss: 0.5257\n",
            "Batch 807/4975 - Training Loss: 0.6259\n",
            "Batch 808/4975 - Training Loss: 0.5820\n",
            "Batch 809/4975 - Training Loss: 0.6457\n",
            "Batch 810/4975 - Training Loss: 0.5984\n",
            "Batch 811/4975 - Training Loss: 0.5084\n",
            "Batch 812/4975 - Training Loss: 0.4704\n",
            "Batch 813/4975 - Training Loss: 0.5933\n",
            "Batch 814/4975 - Training Loss: 0.6506\n",
            "Batch 815/4975 - Training Loss: 0.6015\n",
            "Batch 816/4975 - Training Loss: 0.6382\n",
            "Batch 817/4975 - Training Loss: 0.5274\n",
            "Batch 818/4975 - Training Loss: 0.6128\n",
            "Batch 819/4975 - Training Loss: 0.4891\n",
            "Batch 820/4975 - Training Loss: 0.8560\n",
            "Batch 821/4975 - Training Loss: 0.4619\n",
            "Batch 822/4975 - Training Loss: 0.9084\n",
            "Batch 823/4975 - Training Loss: 0.7921\n",
            "Batch 824/4975 - Training Loss: 0.6771\n",
            "Batch 825/4975 - Training Loss: 0.7387\n",
            "Batch 826/4975 - Training Loss: 0.9331\n",
            "Batch 827/4975 - Training Loss: 1.0171\n",
            "Batch 828/4975 - Training Loss: 0.3943\n",
            "Batch 829/4975 - Training Loss: 0.5837\n",
            "Batch 830/4975 - Training Loss: 0.4002\n",
            "Batch 831/4975 - Training Loss: 0.6444\n",
            "Batch 832/4975 - Training Loss: 0.5369\n",
            "Batch 833/4975 - Training Loss: 0.4904\n",
            "Batch 834/4975 - Training Loss: 1.1265\n",
            "Batch 835/4975 - Training Loss: 0.6010\n",
            "Batch 836/4975 - Training Loss: 0.6991\n",
            "Batch 837/4975 - Training Loss: 0.4730\n",
            "Batch 838/4975 - Training Loss: 0.5852\n",
            "Batch 839/4975 - Training Loss: 0.4864\n",
            "Batch 840/4975 - Training Loss: 0.5919\n",
            "Batch 841/4975 - Training Loss: 0.4798\n",
            "Batch 842/4975 - Training Loss: 0.6323\n",
            "Batch 843/4975 - Training Loss: 0.8754\n",
            "Batch 844/4975 - Training Loss: 0.5642\n",
            "Batch 845/4975 - Training Loss: 0.7672\n",
            "Batch 846/4975 - Training Loss: 0.5847\n",
            "Batch 847/4975 - Training Loss: 0.4332\n",
            "Batch 848/4975 - Training Loss: 0.6594\n",
            "Batch 849/4975 - Training Loss: 0.4435\n",
            "Batch 850/4975 - Training Loss: 0.4504\n",
            "Batch 851/4975 - Training Loss: 0.7088\n",
            "Batch 852/4975 - Training Loss: 0.4542\n",
            "Batch 853/4975 - Training Loss: 0.5754\n",
            "Batch 854/4975 - Training Loss: 0.5899\n",
            "Batch 855/4975 - Training Loss: 0.6631\n",
            "Batch 856/4975 - Training Loss: 0.5851\n",
            "Batch 857/4975 - Training Loss: 0.5808\n",
            "Batch 858/4975 - Training Loss: 0.8716\n",
            "Batch 859/4975 - Training Loss: 0.5631\n",
            "Batch 860/4975 - Training Loss: 0.4323\n",
            "Batch 861/4975 - Training Loss: 0.7505\n",
            "Batch 862/4975 - Training Loss: 0.6737\n",
            "Batch 863/4975 - Training Loss: 0.6113\n",
            "Batch 864/4975 - Training Loss: 0.6664\n",
            "Batch 865/4975 - Training Loss: 0.6298\n",
            "Batch 866/4975 - Training Loss: 0.2854\n",
            "Batch 867/4975 - Training Loss: 0.5039\n",
            "Batch 868/4975 - Training Loss: 0.5258\n",
            "Batch 869/4975 - Training Loss: 0.5249\n",
            "Batch 870/4975 - Training Loss: 0.5104\n",
            "Batch 871/4975 - Training Loss: 0.5985\n",
            "Batch 872/4975 - Training Loss: 0.6944\n",
            "Batch 873/4975 - Training Loss: 0.6375\n",
            "Batch 874/4975 - Training Loss: 0.3414\n",
            "Batch 875/4975 - Training Loss: 0.5449\n",
            "Batch 876/4975 - Training Loss: 0.6386\n",
            "Batch 877/4975 - Training Loss: 0.5491\n",
            "Batch 878/4975 - Training Loss: 0.5513\n",
            "Batch 879/4975 - Training Loss: 0.8165\n",
            "Batch 880/4975 - Training Loss: 0.8655\n",
            "Batch 881/4975 - Training Loss: 0.6935\n",
            "Batch 882/4975 - Training Loss: 0.6989\n",
            "Batch 883/4975 - Training Loss: 0.4013\n",
            "Batch 884/4975 - Training Loss: 0.7081\n",
            "Batch 885/4975 - Training Loss: 0.3981\n",
            "Batch 886/4975 - Training Loss: 0.7325\n",
            "Batch 887/4975 - Training Loss: 0.6191\n",
            "Batch 888/4975 - Training Loss: 0.7732\n",
            "Batch 889/4975 - Training Loss: 0.6336\n",
            "Batch 890/4975 - Training Loss: 0.7116\n",
            "Batch 891/4975 - Training Loss: 0.4893\n",
            "Batch 892/4975 - Training Loss: 0.6966\n",
            "Batch 893/4975 - Training Loss: 0.4863\n",
            "Batch 894/4975 - Training Loss: 0.5496\n",
            "Batch 895/4975 - Training Loss: 0.6636\n",
            "Batch 896/4975 - Training Loss: 0.3495\n",
            "Batch 897/4975 - Training Loss: 0.5956\n",
            "Batch 898/4975 - Training Loss: 0.4535\n",
            "Batch 899/4975 - Training Loss: 0.5074\n",
            "Batch 900/4975 - Training Loss: 0.4418\n",
            "Batch 901/4975 - Training Loss: 0.6367\n",
            "Batch 902/4975 - Training Loss: 0.4869\n",
            "Batch 903/4975 - Training Loss: 0.6865\n",
            "Batch 904/4975 - Training Loss: 0.4866\n",
            "Batch 905/4975 - Training Loss: 0.8827\n",
            "Batch 906/4975 - Training Loss: 0.5367\n",
            "Batch 907/4975 - Training Loss: 0.7023\n",
            "Batch 908/4975 - Training Loss: 0.4986\n",
            "Batch 909/4975 - Training Loss: 0.3311\n",
            "Batch 910/4975 - Training Loss: 0.4678\n",
            "Batch 911/4975 - Training Loss: 0.5436\n",
            "Batch 912/4975 - Training Loss: 0.5013\n",
            "Batch 913/4975 - Training Loss: 0.4764\n",
            "Batch 914/4975 - Training Loss: 0.5005\n",
            "Batch 915/4975 - Training Loss: 0.4145\n",
            "Batch 916/4975 - Training Loss: 0.7184\n",
            "Batch 917/4975 - Training Loss: 0.6125\n",
            "Batch 918/4975 - Training Loss: 0.5902\n",
            "Batch 919/4975 - Training Loss: 0.5337\n",
            "Batch 920/4975 - Training Loss: 0.4512\n",
            "Batch 921/4975 - Training Loss: 0.6476\n",
            "Batch 922/4975 - Training Loss: 0.6594\n",
            "Batch 923/4975 - Training Loss: 0.4802\n",
            "Batch 924/4975 - Training Loss: 0.5217\n",
            "Batch 925/4975 - Training Loss: 0.6031\n",
            "Batch 926/4975 - Training Loss: 0.7017\n",
            "Batch 927/4975 - Training Loss: 0.8402\n",
            "Batch 928/4975 - Training Loss: 0.5071\n",
            "Batch 929/4975 - Training Loss: 0.5506\n",
            "Batch 930/4975 - Training Loss: 0.4353\n",
            "Batch 931/4975 - Training Loss: 0.5155\n",
            "Batch 932/4975 - Training Loss: 0.5119\n",
            "Batch 933/4975 - Training Loss: 0.4565\n",
            "Batch 934/4975 - Training Loss: 0.6271\n",
            "Batch 935/4975 - Training Loss: 0.4611\n",
            "Batch 936/4975 - Training Loss: 0.9242\n",
            "Batch 937/4975 - Training Loss: 0.4617\n",
            "Batch 938/4975 - Training Loss: 0.4673\n",
            "Batch 939/4975 - Training Loss: 0.7469\n",
            "Batch 940/4975 - Training Loss: 0.5690\n",
            "Batch 941/4975 - Training Loss: 0.4516\n",
            "Batch 942/4975 - Training Loss: 0.4779\n",
            "Batch 943/4975 - Training Loss: 0.8770\n",
            "Batch 944/4975 - Training Loss: 0.4821\n",
            "Batch 945/4975 - Training Loss: 0.6319\n",
            "Batch 946/4975 - Training Loss: 1.0392\n",
            "Batch 947/4975 - Training Loss: 0.5340\n",
            "Batch 948/4975 - Training Loss: 0.6612\n",
            "Batch 949/4975 - Training Loss: 0.8784\n",
            "Batch 950/4975 - Training Loss: 0.3379\n",
            "Batch 951/4975 - Training Loss: 0.5814\n",
            "Batch 952/4975 - Training Loss: 0.6983\n",
            "Batch 953/4975 - Training Loss: 0.6501\n",
            "Batch 954/4975 - Training Loss: 0.6658\n",
            "Batch 955/4975 - Training Loss: 0.5808\n",
            "Batch 956/4975 - Training Loss: 0.7021\n",
            "Batch 957/4975 - Training Loss: 0.7466\n",
            "Batch 958/4975 - Training Loss: 0.4143\n",
            "Batch 959/4975 - Training Loss: 0.6151\n",
            "Batch 960/4975 - Training Loss: 0.4104\n",
            "Batch 961/4975 - Training Loss: 0.4396\n",
            "Batch 962/4975 - Training Loss: 0.4278\n",
            "Batch 963/4975 - Training Loss: 0.4264\n",
            "Batch 964/4975 - Training Loss: 0.7032\n",
            "Batch 965/4975 - Training Loss: 0.7285\n",
            "Batch 966/4975 - Training Loss: 0.7403\n",
            "Batch 967/4975 - Training Loss: 0.3482\n",
            "Batch 968/4975 - Training Loss: 0.7423\n",
            "Batch 969/4975 - Training Loss: 0.4578\n",
            "Batch 970/4975 - Training Loss: 0.6431\n",
            "Batch 971/4975 - Training Loss: 0.6069\n",
            "Batch 972/4975 - Training Loss: 0.7386\n",
            "Batch 973/4975 - Training Loss: 0.4710\n",
            "Batch 974/4975 - Training Loss: 0.9589\n",
            "Batch 975/4975 - Training Loss: 0.5764\n",
            "Batch 976/4975 - Training Loss: 0.5630\n",
            "Batch 977/4975 - Training Loss: 0.3042\n",
            "Batch 978/4975 - Training Loss: 0.4975\n",
            "Batch 979/4975 - Training Loss: 0.7872\n",
            "Batch 980/4975 - Training Loss: 0.2650\n",
            "Batch 981/4975 - Training Loss: 0.5130\n",
            "Batch 982/4975 - Training Loss: 0.6704\n",
            "Batch 983/4975 - Training Loss: 0.8014\n",
            "Batch 984/4975 - Training Loss: 0.9334\n",
            "Batch 985/4975 - Training Loss: 0.4539\n",
            "Batch 986/4975 - Training Loss: 0.8256\n",
            "Batch 987/4975 - Training Loss: 0.5534\n",
            "Batch 988/4975 - Training Loss: 0.6830\n",
            "Batch 989/4975 - Training Loss: 0.6529\n",
            "Batch 990/4975 - Training Loss: 0.6310\n",
            "Batch 991/4975 - Training Loss: 1.0332\n",
            "Batch 992/4975 - Training Loss: 0.7357\n",
            "Batch 993/4975 - Training Loss: 0.8569\n",
            "Batch 994/4975 - Training Loss: 0.6777\n",
            "Batch 995/4975 - Training Loss: 0.7709\n",
            "Batch 996/4975 - Training Loss: 0.5090\n",
            "Batch 997/4975 - Training Loss: 0.7658\n",
            "Batch 998/4975 - Training Loss: 0.4074\n",
            "Batch 999/4975 - Training Loss: 0.6394\n",
            "Batch 1000/4975 - Training Loss: 0.4791\n",
            "Batch 1001/4975 - Training Loss: 0.5899\n",
            "Batch 1002/4975 - Training Loss: 0.5757\n",
            "Batch 1003/4975 - Training Loss: 0.7721\n",
            "Batch 1004/4975 - Training Loss: 0.4874\n",
            "Batch 1005/4975 - Training Loss: 0.4057\n",
            "Batch 1006/4975 - Training Loss: 0.8692\n",
            "Batch 1007/4975 - Training Loss: 0.6879\n",
            "Batch 1008/4975 - Training Loss: 0.4293\n",
            "Batch 1009/4975 - Training Loss: 0.4100\n",
            "Batch 1010/4975 - Training Loss: 0.5742\n",
            "Batch 1011/4975 - Training Loss: 0.5261\n",
            "Batch 1012/4975 - Training Loss: 0.7657\n",
            "Batch 1013/4975 - Training Loss: 0.7581\n",
            "Batch 1014/4975 - Training Loss: 0.4613\n",
            "Batch 1015/4975 - Training Loss: 0.7474\n",
            "Batch 1016/4975 - Training Loss: 0.5492\n",
            "Batch 1017/4975 - Training Loss: 0.7348\n",
            "Batch 1018/4975 - Training Loss: 0.6897\n",
            "Batch 1019/4975 - Training Loss: 0.8474\n",
            "Batch 1020/4975 - Training Loss: 0.4981\n",
            "Batch 1021/4975 - Training Loss: 0.6377\n",
            "Batch 1022/4975 - Training Loss: 0.5426\n",
            "Batch 1023/4975 - Training Loss: 0.3716\n",
            "Batch 1024/4975 - Training Loss: 0.5510\n",
            "Batch 1025/4975 - Training Loss: 0.7298\n",
            "Batch 1026/4975 - Training Loss: 0.5072\n",
            "Batch 1027/4975 - Training Loss: 0.5653\n",
            "Batch 1028/4975 - Training Loss: 0.6181\n",
            "Batch 1029/4975 - Training Loss: 0.8361\n",
            "Batch 1030/4975 - Training Loss: 0.5565\n",
            "Batch 1031/4975 - Training Loss: 0.5527\n",
            "Batch 1032/4975 - Training Loss: 0.6733\n",
            "Batch 1033/4975 - Training Loss: 0.7788\n",
            "Batch 1034/4975 - Training Loss: 0.4307\n",
            "Batch 1035/4975 - Training Loss: 0.5197\n",
            "Batch 1036/4975 - Training Loss: 0.3373\n",
            "Batch 1037/4975 - Training Loss: 0.6516\n",
            "Batch 1038/4975 - Training Loss: 0.5608\n",
            "Batch 1039/4975 - Training Loss: 0.6214\n",
            "Batch 1040/4975 - Training Loss: 0.9229\n",
            "Batch 1041/4975 - Training Loss: 0.4848\n",
            "Batch 1042/4975 - Training Loss: 0.4466\n",
            "Batch 1043/4975 - Training Loss: 0.3786\n",
            "Batch 1044/4975 - Training Loss: 0.3706\n",
            "Batch 1045/4975 - Training Loss: 0.7833\n",
            "Batch 1046/4975 - Training Loss: 0.3422\n",
            "Batch 1047/4975 - Training Loss: 0.5825\n",
            "Batch 1048/4975 - Training Loss: 0.4719\n",
            "Batch 1049/4975 - Training Loss: 0.6444\n",
            "Batch 1050/4975 - Training Loss: 0.5246\n",
            "Batch 1051/4975 - Training Loss: 0.5297\n",
            "Batch 1052/4975 - Training Loss: 0.6935\n",
            "Batch 1053/4975 - Training Loss: 0.7289\n",
            "Batch 1054/4975 - Training Loss: 0.5754\n",
            "Batch 1055/4975 - Training Loss: 0.5245\n",
            "Batch 1056/4975 - Training Loss: 0.8324\n",
            "Batch 1057/4975 - Training Loss: 0.5853\n",
            "Batch 1058/4975 - Training Loss: 0.3206\n",
            "Batch 1059/4975 - Training Loss: 0.4824\n",
            "Batch 1060/4975 - Training Loss: 0.7384\n",
            "Batch 1061/4975 - Training Loss: 0.3480\n",
            "Batch 1062/4975 - Training Loss: 0.6644\n",
            "Batch 1063/4975 - Training Loss: 0.4843\n",
            "Batch 1064/4975 - Training Loss: 0.3276\n",
            "Batch 1065/4975 - Training Loss: 0.6730\n",
            "Batch 1066/4975 - Training Loss: 0.6828\n",
            "Batch 1067/4975 - Training Loss: 0.3532\n",
            "Batch 1068/4975 - Training Loss: 0.4828\n",
            "Batch 1069/4975 - Training Loss: 0.5225\n",
            "Batch 1070/4975 - Training Loss: 0.7605\n",
            "Batch 1071/4975 - Training Loss: 0.4672\n",
            "Batch 1072/4975 - Training Loss: 0.7998\n",
            "Batch 1073/4975 - Training Loss: 0.5506\n",
            "Batch 1074/4975 - Training Loss: 0.5423\n",
            "Batch 1075/4975 - Training Loss: 0.8005\n",
            "Batch 1076/4975 - Training Loss: 0.4954\n",
            "Batch 1077/4975 - Training Loss: 0.5647\n",
            "Batch 1078/4975 - Training Loss: 1.0542\n",
            "Batch 1079/4975 - Training Loss: 0.5520\n",
            "Batch 1080/4975 - Training Loss: 0.5616\n",
            "Batch 1081/4975 - Training Loss: 0.6524\n",
            "Batch 1082/4975 - Training Loss: 0.3703\n",
            "Batch 1083/4975 - Training Loss: 0.5544\n",
            "Batch 1084/4975 - Training Loss: 0.7786\n",
            "Batch 1085/4975 - Training Loss: 0.7932\n",
            "Batch 1086/4975 - Training Loss: 0.4053\n",
            "Batch 1087/4975 - Training Loss: 1.0143\n",
            "Batch 1088/4975 - Training Loss: 0.7730\n",
            "Batch 1089/4975 - Training Loss: 0.7183\n",
            "Batch 1090/4975 - Training Loss: 0.8694\n",
            "Batch 1091/4975 - Training Loss: 0.6748\n",
            "Batch 1092/4975 - Training Loss: 0.5995\n",
            "Batch 1093/4975 - Training Loss: 0.7575\n",
            "Batch 1094/4975 - Training Loss: 0.5577\n",
            "Batch 1095/4975 - Training Loss: 0.7671\n",
            "Batch 1096/4975 - Training Loss: 0.6904\n",
            "Batch 1097/4975 - Training Loss: 0.7219\n",
            "Batch 1098/4975 - Training Loss: 0.8265\n",
            "Batch 1099/4975 - Training Loss: 0.7785\n",
            "Batch 1100/4975 - Training Loss: 0.8630\n",
            "Batch 1101/4975 - Training Loss: 0.4863\n",
            "Batch 1102/4975 - Training Loss: 0.7332\n",
            "Batch 1103/4975 - Training Loss: 0.5561\n",
            "Batch 1104/4975 - Training Loss: 0.8867\n",
            "Batch 1105/4975 - Training Loss: 0.3394\n",
            "Batch 1106/4975 - Training Loss: 0.5671\n",
            "Batch 1107/4975 - Training Loss: 0.5050\n",
            "Batch 1108/4975 - Training Loss: 0.7488\n",
            "Batch 1109/4975 - Training Loss: 0.5060\n",
            "Batch 1110/4975 - Training Loss: 0.6126\n",
            "Batch 1111/4975 - Training Loss: 0.7207\n",
            "Batch 1112/4975 - Training Loss: 1.3916\n",
            "Batch 1113/4975 - Training Loss: 0.6239\n",
            "Batch 1114/4975 - Training Loss: 0.5527\n",
            "Batch 1115/4975 - Training Loss: 0.6051\n",
            "Batch 1116/4975 - Training Loss: 0.4924\n",
            "Batch 1117/4975 - Training Loss: 0.6229\n",
            "Batch 1118/4975 - Training Loss: 0.4369\n",
            "Batch 1119/4975 - Training Loss: 0.7174\n",
            "Batch 1120/4975 - Training Loss: 0.7309\n",
            "Batch 1121/4975 - Training Loss: 0.6129\n",
            "Batch 1122/4975 - Training Loss: 0.7450\n",
            "Batch 1123/4975 - Training Loss: 0.6880\n",
            "Batch 1124/4975 - Training Loss: 0.4177\n",
            "Batch 1125/4975 - Training Loss: 0.6649\n",
            "Batch 1126/4975 - Training Loss: 0.7356\n",
            "Batch 1127/4975 - Training Loss: 0.6372\n",
            "Batch 1128/4975 - Training Loss: 0.5853\n",
            "Batch 1129/4975 - Training Loss: 0.5033\n",
            "Batch 1130/4975 - Training Loss: 0.7108\n",
            "Batch 1131/4975 - Training Loss: 0.5660\n",
            "Batch 1132/4975 - Training Loss: 0.6021\n",
            "Batch 1133/4975 - Training Loss: 0.5703\n",
            "Batch 1134/4975 - Training Loss: 0.5782\n",
            "Batch 1135/4975 - Training Loss: 0.4472\n",
            "Batch 1136/4975 - Training Loss: 0.5894\n",
            "Batch 1137/4975 - Training Loss: 0.9126\n",
            "Batch 1138/4975 - Training Loss: 0.6533\n",
            "Batch 1139/4975 - Training Loss: 0.6725\n",
            "Batch 1140/4975 - Training Loss: 0.6976\n",
            "Batch 1141/4975 - Training Loss: 0.6749\n",
            "Batch 1142/4975 - Training Loss: 0.6312\n",
            "Batch 1143/4975 - Training Loss: 0.5177\n",
            "Batch 1144/4975 - Training Loss: 0.6345\n",
            "Batch 1145/4975 - Training Loss: 0.5475\n",
            "Batch 1146/4975 - Training Loss: 0.7825\n",
            "Batch 1147/4975 - Training Loss: 0.4885\n",
            "Batch 1148/4975 - Training Loss: 0.8992\n",
            "Batch 1149/4975 - Training Loss: 0.6408\n",
            "Batch 1150/4975 - Training Loss: 0.8877\n",
            "Batch 1151/4975 - Training Loss: 0.3910\n",
            "Batch 1152/4975 - Training Loss: 0.5026\n",
            "Batch 1153/4975 - Training Loss: 0.2146\n",
            "Batch 1154/4975 - Training Loss: 0.4271\n",
            "Batch 1155/4975 - Training Loss: 0.5274\n",
            "Batch 1156/4975 - Training Loss: 0.6195\n",
            "Batch 1157/4975 - Training Loss: 0.4854\n",
            "Batch 1158/4975 - Training Loss: 0.7029\n",
            "Batch 1159/4975 - Training Loss: 0.4195\n",
            "Batch 1160/4975 - Training Loss: 0.6767\n",
            "Batch 1161/4975 - Training Loss: 0.4223\n",
            "Batch 1162/4975 - Training Loss: 0.7574\n",
            "Batch 1163/4975 - Training Loss: 0.6911\n",
            "Batch 1164/4975 - Training Loss: 0.6138\n",
            "Batch 1165/4975 - Training Loss: 0.4380\n",
            "Batch 1166/4975 - Training Loss: 0.4981\n",
            "Batch 1167/4975 - Training Loss: 0.4937\n",
            "Batch 1168/4975 - Training Loss: 0.6302\n",
            "Batch 1169/4975 - Training Loss: 0.3828\n",
            "Batch 1170/4975 - Training Loss: 0.4380\n",
            "Batch 1171/4975 - Training Loss: 1.0219\n",
            "Batch 1172/4975 - Training Loss: 0.5635\n",
            "Batch 1173/4975 - Training Loss: 0.7587\n",
            "Batch 1174/4975 - Training Loss: 0.8218\n",
            "Batch 1175/4975 - Training Loss: 0.5990\n",
            "Batch 1176/4975 - Training Loss: 0.7167\n",
            "Batch 1177/4975 - Training Loss: 0.7845\n",
            "Batch 1178/4975 - Training Loss: 0.5977\n",
            "Batch 1179/4975 - Training Loss: 0.6639\n",
            "Batch 1180/4975 - Training Loss: 0.5156\n",
            "Batch 1181/4975 - Training Loss: 0.6362\n",
            "Batch 1182/4975 - Training Loss: 0.6523\n",
            "Batch 1183/4975 - Training Loss: 0.7336\n",
            "Batch 1184/4975 - Training Loss: 0.6182\n",
            "Batch 1185/4975 - Training Loss: 0.5817\n",
            "Batch 1186/4975 - Training Loss: 0.6048\n",
            "Batch 1187/4975 - Training Loss: 0.4977\n",
            "Batch 1188/4975 - Training Loss: 1.0857\n",
            "Batch 1189/4975 - Training Loss: 0.4554\n",
            "Batch 1190/4975 - Training Loss: 0.6435\n",
            "Batch 1191/4975 - Training Loss: 0.4077\n",
            "Batch 1192/4975 - Training Loss: 0.7165\n",
            "Batch 1193/4975 - Training Loss: 0.6315\n",
            "Batch 1194/4975 - Training Loss: 0.6092\n",
            "Batch 1195/4975 - Training Loss: 0.6264\n",
            "Batch 1196/4975 - Training Loss: 0.5162\n",
            "Batch 1197/4975 - Training Loss: 0.5306\n",
            "Batch 1198/4975 - Training Loss: 0.5412\n",
            "Batch 1199/4975 - Training Loss: 0.8000\n",
            "Batch 1200/4975 - Training Loss: 0.4747\n",
            "Batch 1201/4975 - Training Loss: 0.5632\n",
            "Batch 1202/4975 - Training Loss: 0.6767\n",
            "Batch 1203/4975 - Training Loss: 0.5738\n",
            "Batch 1204/4975 - Training Loss: 0.5795\n",
            "Batch 1205/4975 - Training Loss: 0.2372\n",
            "Batch 1206/4975 - Training Loss: 0.4454\n",
            "Batch 1207/4975 - Training Loss: 0.5706\n",
            "Batch 1208/4975 - Training Loss: 0.9552\n",
            "Batch 1209/4975 - Training Loss: 0.3907\n",
            "Batch 1210/4975 - Training Loss: 0.4804\n",
            "Batch 1211/4975 - Training Loss: 0.4997\n",
            "Batch 1212/4975 - Training Loss: 0.5169\n",
            "Batch 1213/4975 - Training Loss: 0.4568\n",
            "Batch 1214/4975 - Training Loss: 0.5877\n",
            "Batch 1215/4975 - Training Loss: 0.7066\n",
            "Batch 1216/4975 - Training Loss: 0.3147\n",
            "Batch 1217/4975 - Training Loss: 0.4929\n",
            "Batch 1218/4975 - Training Loss: 0.4613\n",
            "Batch 1219/4975 - Training Loss: 0.6886\n",
            "Batch 1220/4975 - Training Loss: 0.6490\n",
            "Batch 1221/4975 - Training Loss: 0.6578\n",
            "Batch 1222/4975 - Training Loss: 0.7068\n",
            "Batch 1223/4975 - Training Loss: 0.6732\n",
            "Batch 1224/4975 - Training Loss: 0.2967\n",
            "Batch 1225/4975 - Training Loss: 0.6990\n",
            "Batch 1226/4975 - Training Loss: 0.5297\n",
            "Batch 1227/4975 - Training Loss: 0.4817\n",
            "Batch 1228/4975 - Training Loss: 0.6762\n",
            "Batch 1229/4975 - Training Loss: 0.6410\n",
            "Batch 1230/4975 - Training Loss: 0.4183\n",
            "Batch 1231/4975 - Training Loss: 0.6169\n",
            "Batch 1232/4975 - Training Loss: 0.6158\n",
            "Batch 1233/4975 - Training Loss: 0.4179\n",
            "Batch 1234/4975 - Training Loss: 0.4685\n",
            "Batch 1235/4975 - Training Loss: 0.6933\n",
            "Batch 1236/4975 - Training Loss: 0.8519\n",
            "Batch 1237/4975 - Training Loss: 0.4031\n",
            "Batch 1238/4975 - Training Loss: 0.5470\n",
            "Batch 1239/4975 - Training Loss: 0.7677\n",
            "Batch 1240/4975 - Training Loss: 0.4868\n",
            "Batch 1241/4975 - Training Loss: 0.6151\n",
            "Batch 1242/4975 - Training Loss: 1.1634\n",
            "Batch 1243/4975 - Training Loss: 0.7321\n",
            "Batch 1244/4975 - Training Loss: 0.5152\n",
            "Batch 1245/4975 - Training Loss: 0.4786\n",
            "Batch 1246/4975 - Training Loss: 0.7091\n",
            "Batch 1247/4975 - Training Loss: 0.5871\n",
            "Batch 1248/4975 - Training Loss: 0.5649\n",
            "Batch 1249/4975 - Training Loss: 0.3476\n",
            "Batch 1250/4975 - Training Loss: 0.5962\n",
            "Batch 1251/4975 - Training Loss: 0.8676\n",
            "Batch 1252/4975 - Training Loss: 0.4987\n",
            "Batch 1253/4975 - Training Loss: 0.7226\n",
            "Batch 1254/4975 - Training Loss: 0.7351\n",
            "Batch 1255/4975 - Training Loss: 0.4847\n",
            "Batch 1256/4975 - Training Loss: 0.8460\n",
            "Batch 1257/4975 - Training Loss: 0.9682\n",
            "Batch 1258/4975 - Training Loss: 0.7249\n",
            "Batch 1259/4975 - Training Loss: 0.7280\n",
            "Batch 1260/4975 - Training Loss: 0.8345\n",
            "Batch 1261/4975 - Training Loss: 0.4697\n",
            "Batch 1262/4975 - Training Loss: 0.5095\n",
            "Batch 1263/4975 - Training Loss: 0.4474\n",
            "Batch 1264/4975 - Training Loss: 0.7143\n",
            "Batch 1265/4975 - Training Loss: 0.7267\n",
            "Batch 1266/4975 - Training Loss: 0.7386\n",
            "Batch 1267/4975 - Training Loss: 0.6201\n",
            "Batch 1268/4975 - Training Loss: 0.5420\n",
            "Batch 1269/4975 - Training Loss: 0.5877\n",
            "Batch 1270/4975 - Training Loss: 0.8555\n",
            "Batch 1271/4975 - Training Loss: 0.4696\n",
            "Batch 1272/4975 - Training Loss: 0.4238\n",
            "Batch 1273/4975 - Training Loss: 0.8330\n",
            "Batch 1274/4975 - Training Loss: 0.7789\n",
            "Batch 1275/4975 - Training Loss: 0.4714\n",
            "Batch 1276/4975 - Training Loss: 0.5445\n",
            "Batch 1277/4975 - Training Loss: 0.7393\n",
            "Batch 1278/4975 - Training Loss: 0.7947\n",
            "Batch 1279/4975 - Training Loss: 0.6428\n",
            "Batch 1280/4975 - Training Loss: 0.7467\n",
            "Batch 1281/4975 - Training Loss: 0.7422\n",
            "Batch 1282/4975 - Training Loss: 0.8436\n",
            "Batch 1283/4975 - Training Loss: 0.4670\n",
            "Batch 1284/4975 - Training Loss: 0.8468\n",
            "Batch 1285/4975 - Training Loss: 0.6712\n",
            "Batch 1286/4975 - Training Loss: 0.4248\n",
            "Batch 1287/4975 - Training Loss: 1.1698\n",
            "Batch 1288/4975 - Training Loss: 0.6554\n",
            "Batch 1289/4975 - Training Loss: 0.7062\n",
            "Batch 1290/4975 - Training Loss: 0.5076\n",
            "Batch 1291/4975 - Training Loss: 0.3970\n",
            "Batch 1292/4975 - Training Loss: 0.8248\n",
            "Batch 1293/4975 - Training Loss: 0.7573\n",
            "Batch 1294/4975 - Training Loss: 0.3223\n",
            "Batch 1295/4975 - Training Loss: 0.7696\n",
            "Batch 1296/4975 - Training Loss: 0.4275\n",
            "Batch 1297/4975 - Training Loss: 1.0791\n",
            "Batch 1298/4975 - Training Loss: 0.5203\n",
            "Batch 1299/4975 - Training Loss: 0.5730\n",
            "Batch 1300/4975 - Training Loss: 0.5717\n",
            "Batch 1301/4975 - Training Loss: 0.6700\n",
            "Batch 1302/4975 - Training Loss: 0.7918\n",
            "Batch 1303/4975 - Training Loss: 0.7903\n",
            "Batch 1304/4975 - Training Loss: 0.5790\n",
            "Batch 1305/4975 - Training Loss: 0.7725\n",
            "Batch 1306/4975 - Training Loss: 0.6223\n",
            "Batch 1307/4975 - Training Loss: 0.7755\n",
            "Batch 1308/4975 - Training Loss: 0.4547\n",
            "Batch 1309/4975 - Training Loss: 0.8665\n",
            "Batch 1310/4975 - Training Loss: 0.5213\n",
            "Batch 1311/4975 - Training Loss: 0.4293\n",
            "Batch 1312/4975 - Training Loss: 0.8889\n",
            "Batch 1313/4975 - Training Loss: 0.6773\n",
            "Batch 1314/4975 - Training Loss: 0.6365\n",
            "Batch 1315/4975 - Training Loss: 0.7383\n",
            "Batch 1316/4975 - Training Loss: 0.5544\n",
            "Batch 1317/4975 - Training Loss: 0.8993\n",
            "Batch 1318/4975 - Training Loss: 0.5919\n",
            "Batch 1319/4975 - Training Loss: 0.7120\n",
            "Batch 1320/4975 - Training Loss: 0.5419\n",
            "Batch 1321/4975 - Training Loss: 0.4631\n",
            "Batch 1322/4975 - Training Loss: 0.5732\n",
            "Batch 1323/4975 - Training Loss: 0.6488\n",
            "Batch 1324/4975 - Training Loss: 0.6380\n",
            "Batch 1325/4975 - Training Loss: 0.4972\n",
            "Batch 1326/4975 - Training Loss: 0.5205\n",
            "Batch 1327/4975 - Training Loss: 0.6382\n",
            "Batch 1328/4975 - Training Loss: 0.4744\n",
            "Batch 1329/4975 - Training Loss: 0.5273\n",
            "Batch 1330/4975 - Training Loss: 0.4297\n",
            "Batch 1331/4975 - Training Loss: 0.7206\n",
            "Batch 1332/4975 - Training Loss: 0.5086\n",
            "Batch 1333/4975 - Training Loss: 0.7298\n",
            "Batch 1334/4975 - Training Loss: 0.6085\n",
            "Batch 1335/4975 - Training Loss: 0.4620\n",
            "Batch 1336/4975 - Training Loss: 0.3246\n",
            "Batch 1337/4975 - Training Loss: 0.5750\n",
            "Batch 1338/4975 - Training Loss: 0.4592\n",
            "Batch 1339/4975 - Training Loss: 0.9038\n",
            "Batch 1340/4975 - Training Loss: 0.5434\n",
            "Batch 1341/4975 - Training Loss: 0.5183\n",
            "Batch 1342/4975 - Training Loss: 0.8800\n",
            "Batch 1343/4975 - Training Loss: 0.6906\n",
            "Batch 1344/4975 - Training Loss: 0.7870\n",
            "Batch 1345/4975 - Training Loss: 0.7992\n",
            "Batch 1346/4975 - Training Loss: 0.8268\n",
            "Batch 1347/4975 - Training Loss: 0.7042\n",
            "Batch 1348/4975 - Training Loss: 0.4341\n",
            "Batch 1349/4975 - Training Loss: 0.7880\n",
            "Batch 1350/4975 - Training Loss: 0.6013\n",
            "Batch 1351/4975 - Training Loss: 0.4536\n",
            "Batch 1352/4975 - Training Loss: 0.9341\n",
            "Batch 1353/4975 - Training Loss: 0.3870\n",
            "Batch 1354/4975 - Training Loss: 0.7084\n",
            "Batch 1355/4975 - Training Loss: 0.4515\n",
            "Batch 1356/4975 - Training Loss: 0.5721\n",
            "Batch 1357/4975 - Training Loss: 0.3966\n",
            "Batch 1358/4975 - Training Loss: 0.4362\n",
            "Batch 1359/4975 - Training Loss: 0.6081\n",
            "Batch 1360/4975 - Training Loss: 0.8243\n",
            "Batch 1361/4975 - Training Loss: 0.7085\n",
            "Batch 1362/4975 - Training Loss: 0.6669\n",
            "Batch 1363/4975 - Training Loss: 0.5829\n",
            "Batch 1364/4975 - Training Loss: 0.9212\n",
            "Batch 1365/4975 - Training Loss: 0.4476\n",
            "Batch 1366/4975 - Training Loss: 0.5501\n",
            "Batch 1367/4975 - Training Loss: 0.8598\n",
            "Batch 1368/4975 - Training Loss: 0.9058\n",
            "Batch 1369/4975 - Training Loss: 0.7701\n",
            "Batch 1370/4975 - Training Loss: 0.6517\n",
            "Batch 1371/4975 - Training Loss: 0.8408\n",
            "Batch 1372/4975 - Training Loss: 0.6162\n",
            "Batch 1373/4975 - Training Loss: 0.5328\n",
            "Batch 1374/4975 - Training Loss: 0.7094\n",
            "Batch 1375/4975 - Training Loss: 1.0729\n",
            "Batch 1376/4975 - Training Loss: 0.4950\n",
            "Batch 1377/4975 - Training Loss: 0.4424\n",
            "Batch 1378/4975 - Training Loss: 0.5144\n",
            "Batch 1379/4975 - Training Loss: 0.5547\n",
            "Batch 1380/4975 - Training Loss: 0.9048\n",
            "Batch 1381/4975 - Training Loss: 0.7981\n",
            "Batch 1382/4975 - Training Loss: 0.7078\n",
            "Batch 1383/4975 - Training Loss: 0.5581\n",
            "Batch 1384/4975 - Training Loss: 0.4701\n",
            "Batch 1385/4975 - Training Loss: 0.4157\n",
            "Batch 1386/4975 - Training Loss: 0.6477\n",
            "Batch 1387/4975 - Training Loss: 0.6082\n",
            "Batch 1388/4975 - Training Loss: 0.6062\n",
            "Batch 1389/4975 - Training Loss: 0.7758\n",
            "Batch 1390/4975 - Training Loss: 0.7225\n",
            "Batch 1391/4975 - Training Loss: 0.5007\n",
            "Batch 1392/4975 - Training Loss: 0.6436\n",
            "Batch 1393/4975 - Training Loss: 0.4141\n",
            "Batch 1394/4975 - Training Loss: 0.4314\n",
            "Batch 1395/4975 - Training Loss: 0.5930\n",
            "Batch 1396/4975 - Training Loss: 0.5955\n",
            "Batch 1397/4975 - Training Loss: 0.7045\n",
            "Batch 1398/4975 - Training Loss: 0.6503\n",
            "Batch 1399/4975 - Training Loss: 0.8266\n",
            "Batch 1400/4975 - Training Loss: 0.5341\n",
            "Batch 1401/4975 - Training Loss: 0.5181\n",
            "Batch 1402/4975 - Training Loss: 0.7205\n",
            "Batch 1403/4975 - Training Loss: 0.3397\n",
            "Batch 1404/4975 - Training Loss: 0.5944\n",
            "Batch 1405/4975 - Training Loss: 0.7907\n",
            "Batch 1406/4975 - Training Loss: 0.5185\n",
            "Batch 1407/4975 - Training Loss: 0.6321\n",
            "Batch 1408/4975 - Training Loss: 0.4485\n",
            "Batch 1409/4975 - Training Loss: 0.5975\n",
            "Batch 1410/4975 - Training Loss: 0.7270\n",
            "Batch 1411/4975 - Training Loss: 0.6925\n",
            "Batch 1412/4975 - Training Loss: 0.4109\n",
            "Batch 1413/4975 - Training Loss: 0.9216\n",
            "Batch 1414/4975 - Training Loss: 0.5347\n",
            "Batch 1415/4975 - Training Loss: 1.0451\n",
            "Batch 1416/4975 - Training Loss: 0.8795\n",
            "Batch 1417/4975 - Training Loss: 0.6885\n",
            "Batch 1418/4975 - Training Loss: 0.7089\n",
            "Batch 1419/4975 - Training Loss: 0.7039\n",
            "Batch 1420/4975 - Training Loss: 0.7781\n",
            "Batch 1421/4975 - Training Loss: 0.6938\n",
            "Batch 1422/4975 - Training Loss: 0.8504\n",
            "Batch 1423/4975 - Training Loss: 0.7015\n",
            "Batch 1424/4975 - Training Loss: 0.3815\n",
            "Batch 1425/4975 - Training Loss: 0.6266\n",
            "Batch 1426/4975 - Training Loss: 0.7485\n",
            "Batch 1427/4975 - Training Loss: 0.7006\n",
            "Batch 1428/4975 - Training Loss: 0.6977\n",
            "Batch 1429/4975 - Training Loss: 0.7177\n",
            "Batch 1430/4975 - Training Loss: 0.8800\n",
            "Batch 1431/4975 - Training Loss: 0.6292\n",
            "Batch 1432/4975 - Training Loss: 0.5796\n",
            "Batch 1433/4975 - Training Loss: 0.6813\n",
            "Batch 1434/4975 - Training Loss: 0.5454\n",
            "Batch 1435/4975 - Training Loss: 0.5603\n",
            "Batch 1436/4975 - Training Loss: 0.6076\n",
            "Batch 1437/4975 - Training Loss: 0.7530\n",
            "Batch 1438/4975 - Training Loss: 0.7068\n",
            "Batch 1439/4975 - Training Loss: 0.5167\n",
            "Batch 1440/4975 - Training Loss: 0.6187\n",
            "Batch 1441/4975 - Training Loss: 0.4957\n",
            "Batch 1442/4975 - Training Loss: 0.6881\n",
            "Batch 1443/4975 - Training Loss: 0.6530\n",
            "Batch 1444/4975 - Training Loss: 0.3692\n",
            "Batch 1445/4975 - Training Loss: 0.6213\n",
            "Batch 1446/4975 - Training Loss: 0.3473\n",
            "Batch 1447/4975 - Training Loss: 0.4483\n",
            "Batch 1448/4975 - Training Loss: 0.6582\n",
            "Batch 1449/4975 - Training Loss: 0.7537\n",
            "Batch 1450/4975 - Training Loss: 0.5799\n",
            "Batch 1451/4975 - Training Loss: 0.6427\n",
            "Batch 1452/4975 - Training Loss: 0.5432\n",
            "Batch 1453/4975 - Training Loss: 0.5340\n",
            "Batch 1454/4975 - Training Loss: 0.6334\n",
            "Batch 1455/4975 - Training Loss: 0.6274\n",
            "Batch 1456/4975 - Training Loss: 0.4834\n",
            "Batch 1457/4975 - Training Loss: 0.6152\n",
            "Batch 1458/4975 - Training Loss: 0.5389\n",
            "Batch 1459/4975 - Training Loss: 0.3705\n",
            "Batch 1460/4975 - Training Loss: 0.6672\n",
            "Batch 1461/4975 - Training Loss: 0.4723\n",
            "Batch 1462/4975 - Training Loss: 0.7121\n",
            "Batch 1463/4975 - Training Loss: 0.5700\n",
            "Batch 1464/4975 - Training Loss: 0.3239\n",
            "Batch 1465/4975 - Training Loss: 0.4382\n",
            "Batch 1466/4975 - Training Loss: 0.7639\n",
            "Batch 1467/4975 - Training Loss: 0.8230\n",
            "Batch 1468/4975 - Training Loss: 0.5868\n",
            "Batch 1469/4975 - Training Loss: 0.7977\n",
            "Batch 1470/4975 - Training Loss: 0.5771\n",
            "Batch 1471/4975 - Training Loss: 0.5397\n",
            "Batch 1472/4975 - Training Loss: 0.8273\n",
            "Batch 1473/4975 - Training Loss: 0.5680\n",
            "Batch 1474/4975 - Training Loss: 0.5638\n",
            "Batch 1475/4975 - Training Loss: 0.5628\n",
            "Batch 1476/4975 - Training Loss: 0.3953\n",
            "Batch 1477/4975 - Training Loss: 0.5861\n",
            "Batch 1478/4975 - Training Loss: 0.7977\n",
            "Batch 1479/4975 - Training Loss: 0.5601\n",
            "Batch 1480/4975 - Training Loss: 0.5447\n",
            "Batch 1481/4975 - Training Loss: 0.6704\n",
            "Batch 1482/4975 - Training Loss: 0.5729\n",
            "Batch 1483/4975 - Training Loss: 0.7294\n",
            "Batch 1484/4975 - Training Loss: 0.6466\n",
            "Batch 1485/4975 - Training Loss: 0.4024\n",
            "Batch 1486/4975 - Training Loss: 0.6570\n",
            "Batch 1487/4975 - Training Loss: 0.7488\n",
            "Batch 1488/4975 - Training Loss: 0.3434\n",
            "Batch 1489/4975 - Training Loss: 0.6827\n",
            "Batch 1490/4975 - Training Loss: 0.5696\n",
            "Batch 1491/4975 - Training Loss: 0.5665\n",
            "Batch 1492/4975 - Training Loss: 0.6137\n",
            "Batch 1493/4975 - Training Loss: 0.6289\n",
            "Batch 1494/4975 - Training Loss: 0.7012\n",
            "Batch 1495/4975 - Training Loss: 0.6350\n",
            "Batch 1496/4975 - Training Loss: 0.7868\n",
            "Batch 1497/4975 - Training Loss: 0.4277\n",
            "Batch 1498/4975 - Training Loss: 0.7762\n",
            "Batch 1499/4975 - Training Loss: 0.6093\n",
            "Batch 1500/4975 - Training Loss: 0.6345\n",
            "Batch 1501/4975 - Training Loss: 0.6544\n",
            "Batch 1502/4975 - Training Loss: 0.5396\n",
            "Batch 1503/4975 - Training Loss: 0.3989\n",
            "Batch 1504/4975 - Training Loss: 0.7705\n",
            "Batch 1505/4975 - Training Loss: 0.5208\n",
            "Batch 1506/4975 - Training Loss: 0.2446\n",
            "Batch 1507/4975 - Training Loss: 0.5611\n",
            "Batch 1508/4975 - Training Loss: 0.3375\n",
            "Batch 1509/4975 - Training Loss: 0.4436\n",
            "Batch 1510/4975 - Training Loss: 0.5875\n",
            "Batch 1511/4975 - Training Loss: 0.8951\n",
            "Batch 1512/4975 - Training Loss: 0.6442\n",
            "Batch 1513/4975 - Training Loss: 0.5197\n",
            "Batch 1514/4975 - Training Loss: 0.4393\n",
            "Batch 1515/4975 - Training Loss: 0.7136\n",
            "Batch 1516/4975 - Training Loss: 0.8212\n",
            "Batch 1517/4975 - Training Loss: 0.6906\n",
            "Batch 1518/4975 - Training Loss: 0.4684\n",
            "Batch 1519/4975 - Training Loss: 0.5243\n",
            "Batch 1520/4975 - Training Loss: 0.3073\n",
            "Batch 1521/4975 - Training Loss: 0.4939\n",
            "Batch 1522/4975 - Training Loss: 0.4062\n",
            "Batch 1523/4975 - Training Loss: 0.7932\n",
            "Batch 1524/4975 - Training Loss: 0.7820\n",
            "Batch 1525/4975 - Training Loss: 0.6407\n",
            "Batch 1526/4975 - Training Loss: 0.9421\n",
            "Batch 1527/4975 - Training Loss: 0.6616\n",
            "Batch 1528/4975 - Training Loss: 0.3808\n",
            "Batch 1529/4975 - Training Loss: 0.4243\n",
            "Batch 1530/4975 - Training Loss: 0.6386\n",
            "Batch 1531/4975 - Training Loss: 0.6627\n",
            "Batch 1532/4975 - Training Loss: 0.7925\n",
            "Batch 1533/4975 - Training Loss: 0.8276\n",
            "Batch 1534/4975 - Training Loss: 0.6303\n",
            "Batch 1535/4975 - Training Loss: 0.4849\n",
            "Batch 1536/4975 - Training Loss: 0.4799\n",
            "Batch 1537/4975 - Training Loss: 0.5721\n",
            "Batch 1538/4975 - Training Loss: 0.6806\n",
            "Batch 1539/4975 - Training Loss: 0.9034\n",
            "Batch 1540/4975 - Training Loss: 0.7619\n",
            "Batch 1541/4975 - Training Loss: 0.4955\n",
            "Batch 1542/4975 - Training Loss: 0.5360\n",
            "Batch 1543/4975 - Training Loss: 0.6246\n",
            "Batch 1544/4975 - Training Loss: 0.6296\n",
            "Batch 1545/4975 - Training Loss: 0.4403\n",
            "Batch 1546/4975 - Training Loss: 0.5240\n",
            "Batch 1547/4975 - Training Loss: 0.5942\n",
            "Batch 1548/4975 - Training Loss: 0.8267\n",
            "Batch 1549/4975 - Training Loss: 0.8505\n",
            "Batch 1550/4975 - Training Loss: 0.3548\n",
            "Batch 1551/4975 - Training Loss: 0.7281\n",
            "Batch 1552/4975 - Training Loss: 0.5885\n",
            "Batch 1553/4975 - Training Loss: 0.8435\n",
            "Batch 1554/4975 - Training Loss: 0.5945\n",
            "Batch 1555/4975 - Training Loss: 0.5956\n",
            "Batch 1556/4975 - Training Loss: 0.9832\n",
            "Batch 1557/4975 - Training Loss: 0.4745\n",
            "Batch 1558/4975 - Training Loss: 0.4456\n",
            "Batch 1559/4975 - Training Loss: 0.7382\n",
            "Batch 1560/4975 - Training Loss: 0.7010\n",
            "Batch 1561/4975 - Training Loss: 0.6960\n",
            "Batch 1562/4975 - Training Loss: 0.4332\n",
            "Batch 1563/4975 - Training Loss: 0.8272\n",
            "Batch 1564/4975 - Training Loss: 0.7180\n",
            "Batch 1565/4975 - Training Loss: 0.6772\n",
            "Batch 1566/4975 - Training Loss: 0.9330\n",
            "Batch 1567/4975 - Training Loss: 0.8741\n",
            "Batch 1568/4975 - Training Loss: 0.9496\n",
            "Batch 1569/4975 - Training Loss: 1.3179\n",
            "Batch 1570/4975 - Training Loss: 0.7650\n",
            "Batch 1571/4975 - Training Loss: 0.6137\n",
            "Batch 1572/4975 - Training Loss: 1.4143\n",
            "Batch 1573/4975 - Training Loss: 0.6745\n",
            "Batch 1574/4975 - Training Loss: 1.6292\n",
            "Batch 1575/4975 - Training Loss: 0.7427\n",
            "Batch 1576/4975 - Training Loss: 0.8642\n",
            "Batch 1577/4975 - Training Loss: 1.2938\n",
            "Batch 1578/4975 - Training Loss: 1.1564\n",
            "Batch 1579/4975 - Training Loss: 0.9255\n",
            "Batch 1580/4975 - Training Loss: 0.8841\n",
            "Batch 1581/4975 - Training Loss: 0.7081\n",
            "Batch 1582/4975 - Training Loss: 0.9973\n",
            "Batch 1583/4975 - Training Loss: 0.4784\n",
            "Batch 1584/4975 - Training Loss: 0.6423\n",
            "Batch 1585/4975 - Training Loss: 0.6681\n",
            "Batch 1586/4975 - Training Loss: 0.8909\n",
            "Batch 1587/4975 - Training Loss: 0.5473\n",
            "Batch 1588/4975 - Training Loss: 0.9165\n",
            "Batch 1589/4975 - Training Loss: 0.9048\n",
            "Batch 1590/4975 - Training Loss: 0.8419\n",
            "Batch 1591/4975 - Training Loss: 0.7407\n",
            "Batch 1592/4975 - Training Loss: 0.8639\n",
            "Batch 1593/4975 - Training Loss: 0.7090\n",
            "Batch 1594/4975 - Training Loss: 0.5400\n",
            "Batch 1595/4975 - Training Loss: 0.6931\n",
            "Batch 1596/4975 - Training Loss: 0.8309\n",
            "Batch 1597/4975 - Training Loss: 0.4458\n",
            "Batch 1598/4975 - Training Loss: 0.5361\n",
            "Batch 1599/4975 - Training Loss: 0.6961\n",
            "Batch 1600/4975 - Training Loss: 0.7453\n",
            "Batch 1601/4975 - Training Loss: 0.5786\n",
            "Batch 1602/4975 - Training Loss: 0.8605\n",
            "Batch 1603/4975 - Training Loss: 0.7046\n",
            "Batch 1604/4975 - Training Loss: 0.9031\n",
            "Batch 1605/4975 - Training Loss: 0.6791\n",
            "Batch 1606/4975 - Training Loss: 0.6628\n",
            "Batch 1607/4975 - Training Loss: 0.8559\n",
            "Batch 1608/4975 - Training Loss: 0.5918\n",
            "Batch 1609/4975 - Training Loss: 1.2673\n",
            "Batch 1610/4975 - Training Loss: 1.0938\n",
            "Batch 1611/4975 - Training Loss: 0.8570\n",
            "Batch 1612/4975 - Training Loss: 0.5838\n",
            "Batch 1613/4975 - Training Loss: 0.3847\n",
            "Batch 1614/4975 - Training Loss: 0.4892\n",
            "Batch 1615/4975 - Training Loss: 0.7791\n",
            "Batch 1616/4975 - Training Loss: 0.4593\n",
            "Batch 1617/4975 - Training Loss: 0.5787\n",
            "Batch 1618/4975 - Training Loss: 0.7172\n",
            "Batch 1619/4975 - Training Loss: 0.6425\n",
            "Batch 1620/4975 - Training Loss: 0.7701\n",
            "Batch 1621/4975 - Training Loss: 0.6406\n",
            "Batch 1622/4975 - Training Loss: 0.6348\n",
            "Batch 1623/4975 - Training Loss: 0.9333\n",
            "Batch 1624/4975 - Training Loss: 0.4907\n",
            "Batch 1625/4975 - Training Loss: 0.4627\n",
            "Batch 1626/4975 - Training Loss: 0.7628\n",
            "Batch 1627/4975 - Training Loss: 0.7742\n",
            "Batch 1628/4975 - Training Loss: 0.4672\n",
            "Batch 1629/4975 - Training Loss: 0.7832\n",
            "Batch 1630/4975 - Training Loss: 0.6132\n",
            "Batch 1631/4975 - Training Loss: 0.6603\n",
            "Batch 1632/4975 - Training Loss: 0.5612\n",
            "Batch 1633/4975 - Training Loss: 0.7968\n",
            "Batch 1634/4975 - Training Loss: 0.7055\n",
            "Batch 1635/4975 - Training Loss: 0.6161\n",
            "Batch 1636/4975 - Training Loss: 0.7865\n",
            "Batch 1637/4975 - Training Loss: 0.7797\n",
            "Batch 1638/4975 - Training Loss: 0.9338\n",
            "Batch 1639/4975 - Training Loss: 0.5641\n",
            "Batch 1640/4975 - Training Loss: 0.7466\n",
            "Batch 1641/4975 - Training Loss: 0.7202\n",
            "Batch 1642/4975 - Training Loss: 0.3368\n",
            "Batch 1643/4975 - Training Loss: 0.8534\n",
            "Batch 1644/4975 - Training Loss: 0.7479\n",
            "Batch 1645/4975 - Training Loss: 0.6569\n",
            "Batch 1646/4975 - Training Loss: 0.3987\n",
            "Batch 1647/4975 - Training Loss: 0.7581\n",
            "Batch 1648/4975 - Training Loss: 0.8544\n",
            "Batch 1649/4975 - Training Loss: 0.7629\n",
            "Batch 1650/4975 - Training Loss: 0.5941\n",
            "Batch 1651/4975 - Training Loss: 0.4377\n",
            "Batch 1652/4975 - Training Loss: 0.6017\n",
            "Batch 1653/4975 - Training Loss: 0.9093\n",
            "Batch 1654/4975 - Training Loss: 0.6546\n",
            "Batch 1655/4975 - Training Loss: 0.5406\n",
            "Batch 1656/4975 - Training Loss: 0.6025\n",
            "Batch 1657/4975 - Training Loss: 0.8392\n",
            "Batch 1658/4975 - Training Loss: 0.4336\n",
            "Batch 1659/4975 - Training Loss: 0.8323\n",
            "Batch 1660/4975 - Training Loss: 0.6477\n",
            "Batch 1661/4975 - Training Loss: 0.7523\n",
            "Batch 1662/4975 - Training Loss: 0.5865\n",
            "Batch 1663/4975 - Training Loss: 0.5510\n",
            "Batch 1664/4975 - Training Loss: 0.8198\n",
            "Batch 1665/4975 - Training Loss: 0.3600\n",
            "Batch 1666/4975 - Training Loss: 0.8082\n",
            "Batch 1667/4975 - Training Loss: 0.7836\n",
            "Batch 1668/4975 - Training Loss: 0.5102\n",
            "Batch 1669/4975 - Training Loss: 0.5547\n",
            "Batch 1670/4975 - Training Loss: 0.3854\n",
            "Batch 1671/4975 - Training Loss: 0.5999\n",
            "Batch 1672/4975 - Training Loss: 0.7387\n",
            "Batch 1673/4975 - Training Loss: 0.4363\n",
            "Batch 1674/4975 - Training Loss: 0.3857\n",
            "Batch 1675/4975 - Training Loss: 0.4793\n",
            "Batch 1676/4975 - Training Loss: 0.8751\n",
            "Batch 1677/4975 - Training Loss: 0.8375\n",
            "Batch 1678/4975 - Training Loss: 0.5367\n",
            "Batch 1679/4975 - Training Loss: 0.6374\n",
            "Batch 1680/4975 - Training Loss: 0.4561\n",
            "Batch 1681/4975 - Training Loss: 0.4744\n",
            "Batch 1682/4975 - Training Loss: 0.3296\n",
            "Batch 1683/4975 - Training Loss: 0.5039\n",
            "Batch 1684/4975 - Training Loss: 0.8573\n",
            "Batch 1685/4975 - Training Loss: 0.3877\n",
            "Batch 1686/4975 - Training Loss: 0.5830\n",
            "Batch 1687/4975 - Training Loss: 0.6470\n",
            "Batch 1688/4975 - Training Loss: 0.4408\n",
            "Batch 1689/4975 - Training Loss: 0.7480\n",
            "Batch 1690/4975 - Training Loss: 0.4332\n",
            "Batch 1691/4975 - Training Loss: 0.8248\n",
            "Batch 1692/4975 - Training Loss: 0.6365\n",
            "Batch 1693/4975 - Training Loss: 0.6423\n",
            "Batch 1694/4975 - Training Loss: 0.7923\n",
            "Batch 1695/4975 - Training Loss: 0.4588\n",
            "Batch 1696/4975 - Training Loss: 0.5987\n",
            "Batch 1697/4975 - Training Loss: 0.7572\n",
            "Batch 1698/4975 - Training Loss: 0.6146\n",
            "Batch 1699/4975 - Training Loss: 0.6330\n",
            "Batch 1700/4975 - Training Loss: 0.6582\n",
            "Batch 1701/4975 - Training Loss: 0.3331\n",
            "Batch 1702/4975 - Training Loss: 0.6839\n",
            "Batch 1703/4975 - Training Loss: 0.5706\n",
            "Batch 1704/4975 - Training Loss: 0.7540\n",
            "Batch 1705/4975 - Training Loss: 0.7621\n",
            "Batch 1706/4975 - Training Loss: 0.7448\n",
            "Batch 1707/4975 - Training Loss: 0.7025\n",
            "Batch 1708/4975 - Training Loss: 0.6818\n",
            "Batch 1709/4975 - Training Loss: 0.4577\n",
            "Batch 1710/4975 - Training Loss: 0.5453\n",
            "Batch 1711/4975 - Training Loss: 0.6124\n",
            "Batch 1712/4975 - Training Loss: 0.5577\n",
            "Batch 1713/4975 - Training Loss: 0.7730\n",
            "Batch 1714/4975 - Training Loss: 0.8203\n",
            "Batch 1715/4975 - Training Loss: 0.7037\n",
            "Batch 1716/4975 - Training Loss: 0.7243\n",
            "Batch 1717/4975 - Training Loss: 0.5727\n",
            "Batch 1718/4975 - Training Loss: 0.7076\n",
            "Batch 1719/4975 - Training Loss: 0.7965\n",
            "Batch 1720/4975 - Training Loss: 0.7971\n",
            "Batch 1721/4975 - Training Loss: 0.7032\n",
            "Batch 1722/4975 - Training Loss: 0.6276\n",
            "Batch 1723/4975 - Training Loss: 0.3962\n",
            "Batch 1724/4975 - Training Loss: 0.3979\n",
            "Batch 1725/4975 - Training Loss: 0.6246\n",
            "Batch 1726/4975 - Training Loss: 0.4463\n",
            "Batch 1727/4975 - Training Loss: 0.5715\n",
            "Batch 1728/4975 - Training Loss: 0.5082\n",
            "Batch 1729/4975 - Training Loss: 0.6683\n",
            "Batch 1730/4975 - Training Loss: 0.6333\n",
            "Batch 1731/4975 - Training Loss: 0.8314\n",
            "Batch 1732/4975 - Training Loss: 0.5691\n",
            "Batch 1733/4975 - Training Loss: 0.5989\n",
            "Batch 1734/4975 - Training Loss: 0.7405\n",
            "Batch 1735/4975 - Training Loss: 0.5344\n",
            "Batch 1736/4975 - Training Loss: 0.7425\n",
            "Batch 1737/4975 - Training Loss: 0.7489\n",
            "Batch 1738/4975 - Training Loss: 0.4422\n",
            "Batch 1739/4975 - Training Loss: 0.4408\n",
            "Batch 1740/4975 - Training Loss: 0.5003\n",
            "Batch 1741/4975 - Training Loss: 0.8041\n",
            "Batch 1742/4975 - Training Loss: 0.3949\n",
            "Batch 1743/4975 - Training Loss: 1.1097\n",
            "Batch 1744/4975 - Training Loss: 0.7395\n",
            "Batch 1745/4975 - Training Loss: 0.5184\n",
            "Batch 1746/4975 - Training Loss: 0.7008\n",
            "Batch 1747/4975 - Training Loss: 0.5065\n",
            "Batch 1748/4975 - Training Loss: 0.8410\n",
            "Batch 1749/4975 - Training Loss: 0.5008\n",
            "Batch 1750/4975 - Training Loss: 0.7611\n",
            "Batch 1751/4975 - Training Loss: 0.7870\n",
            "Batch 1752/4975 - Training Loss: 0.8311\n",
            "Batch 1753/4975 - Training Loss: 0.8175\n",
            "Batch 1754/4975 - Training Loss: 0.4384\n",
            "Batch 1755/4975 - Training Loss: 0.5794\n",
            "Batch 1756/4975 - Training Loss: 0.6614\n",
            "Batch 1757/4975 - Training Loss: 0.6270\n",
            "Batch 1758/4975 - Training Loss: 0.5198\n",
            "Batch 1759/4975 - Training Loss: 0.8590\n",
            "Batch 1760/4975 - Training Loss: 0.9496\n",
            "Batch 1761/4975 - Training Loss: 0.4902\n",
            "Batch 1762/4975 - Training Loss: 0.5694\n",
            "Batch 1763/4975 - Training Loss: 0.5379\n",
            "Batch 1764/4975 - Training Loss: 0.7996\n",
            "Batch 1765/4975 - Training Loss: 0.4118\n",
            "Batch 1766/4975 - Training Loss: 0.5009\n",
            "Batch 1767/4975 - Training Loss: 0.4164\n",
            "Batch 1768/4975 - Training Loss: 0.3673\n",
            "Batch 1769/4975 - Training Loss: 0.8201\n",
            "Batch 1770/4975 - Training Loss: 0.9492\n",
            "Batch 1771/4975 - Training Loss: 0.4618\n",
            "Batch 1772/4975 - Training Loss: 0.6605\n",
            "Batch 1773/4975 - Training Loss: 0.6342\n",
            "Batch 1774/4975 - Training Loss: 0.5195\n",
            "Batch 1775/4975 - Training Loss: 0.5649\n",
            "Batch 1776/4975 - Training Loss: 0.6735\n",
            "Batch 1777/4975 - Training Loss: 0.5788\n",
            "Batch 1778/4975 - Training Loss: 0.5054\n",
            "Batch 1779/4975 - Training Loss: 0.8468\n",
            "Batch 1780/4975 - Training Loss: 0.6597\n",
            "Batch 1781/4975 - Training Loss: 0.6432\n",
            "Batch 1782/4975 - Training Loss: 0.6711\n",
            "Batch 1783/4975 - Training Loss: 0.4379\n",
            "Batch 1784/4975 - Training Loss: 0.7159\n",
            "Batch 1785/4975 - Training Loss: 0.7757\n",
            "Batch 1786/4975 - Training Loss: 0.4849\n",
            "Batch 1787/4975 - Training Loss: 0.5844\n",
            "Batch 1788/4975 - Training Loss: 0.6336\n",
            "Batch 1789/4975 - Training Loss: 0.4068\n",
            "Batch 1790/4975 - Training Loss: 0.7700\n",
            "Batch 1791/4975 - Training Loss: 0.5334\n",
            "Batch 1792/4975 - Training Loss: 0.6785\n",
            "Batch 1793/4975 - Training Loss: 0.4930\n",
            "Batch 1794/4975 - Training Loss: 0.7492\n",
            "Batch 1795/4975 - Training Loss: 0.7186\n",
            "Batch 1796/4975 - Training Loss: 0.5482\n",
            "Batch 1797/4975 - Training Loss: 0.6241\n",
            "Batch 1798/4975 - Training Loss: 0.7635\n",
            "Batch 1799/4975 - Training Loss: 0.7318\n",
            "Batch 1800/4975 - Training Loss: 0.6571\n",
            "Batch 1801/4975 - Training Loss: 0.5796\n",
            "Batch 1802/4975 - Training Loss: 0.3757\n",
            "Batch 1803/4975 - Training Loss: 1.0493\n",
            "Batch 1804/4975 - Training Loss: 0.5077\n",
            "Batch 1805/4975 - Training Loss: 0.7590\n",
            "Batch 1806/4975 - Training Loss: 0.4624\n",
            "Batch 1807/4975 - Training Loss: 0.5551\n",
            "Batch 1808/4975 - Training Loss: 0.9468\n",
            "Batch 1809/4975 - Training Loss: 0.6649\n",
            "Batch 1810/4975 - Training Loss: 0.5116\n",
            "Batch 1811/4975 - Training Loss: 0.5409\n",
            "Batch 1812/4975 - Training Loss: 0.5230\n",
            "Batch 1813/4975 - Training Loss: 0.7595\n",
            "Batch 1814/4975 - Training Loss: 0.6330\n",
            "Batch 1815/4975 - Training Loss: 0.8365\n",
            "Batch 1816/4975 - Training Loss: 0.5231\n",
            "Batch 1817/4975 - Training Loss: 0.3484\n",
            "Batch 1818/4975 - Training Loss: 0.6147\n",
            "Batch 1819/4975 - Training Loss: 0.5048\n",
            "Batch 1820/4975 - Training Loss: 0.7331\n",
            "Batch 1821/4975 - Training Loss: 0.4218\n",
            "Batch 1822/4975 - Training Loss: 0.7880\n",
            "Batch 1823/4975 - Training Loss: 0.6783\n",
            "Batch 1824/4975 - Training Loss: 0.7078\n",
            "Batch 1825/4975 - Training Loss: 0.3913\n",
            "Batch 1826/4975 - Training Loss: 0.3780\n",
            "Batch 1827/4975 - Training Loss: 0.3522\n",
            "Batch 1828/4975 - Training Loss: 0.6276\n",
            "Batch 1829/4975 - Training Loss: 0.7917\n",
            "Batch 1830/4975 - Training Loss: 0.6578\n",
            "Batch 1831/4975 - Training Loss: 0.5833\n",
            "Batch 1832/4975 - Training Loss: 0.4937\n",
            "Batch 1833/4975 - Training Loss: 0.7360\n",
            "Batch 1834/4975 - Training Loss: 0.5666\n",
            "Batch 1835/4975 - Training Loss: 0.5080\n",
            "Batch 1836/4975 - Training Loss: 0.4403\n",
            "Batch 1837/4975 - Training Loss: 0.5202\n",
            "Batch 1838/4975 - Training Loss: 0.3838\n",
            "Batch 1839/4975 - Training Loss: 0.5401\n",
            "Batch 1840/4975 - Training Loss: 0.4702\n",
            "Batch 1841/4975 - Training Loss: 0.5214\n",
            "Batch 1842/4975 - Training Loss: 0.7799\n",
            "Batch 1843/4975 - Training Loss: 0.5815\n",
            "Batch 1844/4975 - Training Loss: 1.0245\n",
            "Batch 1845/4975 - Training Loss: 0.7661\n",
            "Batch 1846/4975 - Training Loss: 0.7225\n",
            "Batch 1847/4975 - Training Loss: 0.6091\n",
            "Batch 1848/4975 - Training Loss: 0.5545\n",
            "Batch 1849/4975 - Training Loss: 0.6557\n",
            "Batch 1850/4975 - Training Loss: 0.5055\n",
            "Batch 1851/4975 - Training Loss: 0.5707\n",
            "Batch 1852/4975 - Training Loss: 0.5107\n",
            "Batch 1853/4975 - Training Loss: 0.6100\n",
            "Batch 1854/4975 - Training Loss: 0.4348\n",
            "Batch 1855/4975 - Training Loss: 0.4945\n",
            "Batch 1856/4975 - Training Loss: 0.4707\n",
            "Batch 1857/4975 - Training Loss: 0.7803\n",
            "Batch 1858/4975 - Training Loss: 0.8139\n",
            "Batch 1859/4975 - Training Loss: 0.7943\n",
            "Batch 1860/4975 - Training Loss: 0.3405\n",
            "Batch 1861/4975 - Training Loss: 0.7725\n",
            "Batch 1862/4975 - Training Loss: 0.5675\n",
            "Batch 1863/4975 - Training Loss: 0.5935\n",
            "Batch 1864/4975 - Training Loss: 0.7144\n",
            "Batch 1865/4975 - Training Loss: 0.4385\n",
            "Batch 1866/4975 - Training Loss: 0.3621\n",
            "Batch 1867/4975 - Training Loss: 0.7323\n",
            "Batch 1868/4975 - Training Loss: 0.4684\n",
            "Batch 1869/4975 - Training Loss: 0.4319\n",
            "Batch 1870/4975 - Training Loss: 0.3195\n",
            "Batch 1871/4975 - Training Loss: 0.7055\n",
            "Batch 1872/4975 - Training Loss: 0.6368\n",
            "Batch 1873/4975 - Training Loss: 1.0494\n",
            "Batch 1874/4975 - Training Loss: 0.7861\n",
            "Batch 1875/4975 - Training Loss: 0.7300\n",
            "Batch 1876/4975 - Training Loss: 0.6989\n",
            "Batch 1877/4975 - Training Loss: 0.8469\n",
            "Batch 1878/4975 - Training Loss: 0.6054\n",
            "Batch 1879/4975 - Training Loss: 0.3811\n",
            "Batch 1880/4975 - Training Loss: 0.4205\n",
            "Batch 1881/4975 - Training Loss: 0.3593\n",
            "Batch 1882/4975 - Training Loss: 0.7144\n",
            "Batch 1883/4975 - Training Loss: 0.5151\n",
            "Batch 1884/4975 - Training Loss: 0.5550\n",
            "Batch 1885/4975 - Training Loss: 0.7812\n",
            "Batch 1886/4975 - Training Loss: 0.5673\n",
            "Batch 1887/4975 - Training Loss: 0.8523\n",
            "Batch 1888/4975 - Training Loss: 0.3421\n",
            "Batch 1889/4975 - Training Loss: 0.3824\n",
            "Batch 1890/4975 - Training Loss: 0.6174\n",
            "Batch 1891/4975 - Training Loss: 0.7053\n",
            "Batch 1892/4975 - Training Loss: 0.6593\n",
            "Batch 1893/4975 - Training Loss: 0.6922\n",
            "Batch 1894/4975 - Training Loss: 0.3081\n",
            "Batch 1895/4975 - Training Loss: 0.6465\n",
            "Batch 1896/4975 - Training Loss: 0.6640\n",
            "Batch 1897/4975 - Training Loss: 0.4614\n",
            "Batch 1898/4975 - Training Loss: 0.6314\n",
            "Batch 1899/4975 - Training Loss: 0.4669\n",
            "Batch 1900/4975 - Training Loss: 0.3965\n",
            "Batch 1901/4975 - Training Loss: 0.7187\n",
            "Batch 1902/4975 - Training Loss: 0.8891\n",
            "Batch 1903/4975 - Training Loss: 0.6604\n",
            "Batch 1904/4975 - Training Loss: 0.5649\n",
            "Batch 1905/4975 - Training Loss: 0.6482\n",
            "Batch 1906/4975 - Training Loss: 0.5824\n",
            "Batch 1907/4975 - Training Loss: 0.7559\n",
            "Batch 1908/4975 - Training Loss: 0.5761\n",
            "Batch 1909/4975 - Training Loss: 0.6416\n",
            "Batch 1910/4975 - Training Loss: 0.8523\n",
            "Batch 1911/4975 - Training Loss: 0.6179\n",
            "Batch 1912/4975 - Training Loss: 0.7750\n",
            "Batch 1913/4975 - Training Loss: 0.5370\n",
            "Batch 1914/4975 - Training Loss: 0.6332\n",
            "Batch 1915/4975 - Training Loss: 0.3391\n",
            "Batch 1916/4975 - Training Loss: 0.6253\n",
            "Batch 1917/4975 - Training Loss: 0.4847\n",
            "Batch 1918/4975 - Training Loss: 0.2608\n",
            "Batch 1919/4975 - Training Loss: 0.7725\n",
            "Batch 1920/4975 - Training Loss: 0.5680\n",
            "Batch 1921/4975 - Training Loss: 0.3572\n",
            "Batch 1922/4975 - Training Loss: 0.7409\n",
            "Batch 1923/4975 - Training Loss: 0.3968\n",
            "Batch 1924/4975 - Training Loss: 0.5454\n",
            "Batch 1925/4975 - Training Loss: 0.4537\n",
            "Batch 1926/4975 - Training Loss: 0.3567\n",
            "Batch 1927/4975 - Training Loss: 0.5905\n",
            "Batch 1928/4975 - Training Loss: 0.7989\n",
            "Batch 1929/4975 - Training Loss: 0.4609\n",
            "Batch 1930/4975 - Training Loss: 0.8019\n",
            "Batch 1931/4975 - Training Loss: 0.8368\n",
            "Batch 1932/4975 - Training Loss: 0.5343\n",
            "Batch 1933/4975 - Training Loss: 0.4615\n",
            "Batch 1934/4975 - Training Loss: 0.4299\n",
            "Batch 1935/4975 - Training Loss: 0.7202\n",
            "Batch 1936/4975 - Training Loss: 0.4077\n",
            "Batch 1937/4975 - Training Loss: 0.4753\n",
            "Batch 1938/4975 - Training Loss: 0.8850\n",
            "Batch 1939/4975 - Training Loss: 0.6135\n",
            "Batch 1940/4975 - Training Loss: 0.7277\n",
            "Batch 1941/4975 - Training Loss: 0.4534\n",
            "Batch 1942/4975 - Training Loss: 0.4243\n",
            "Batch 1943/4975 - Training Loss: 0.4976\n",
            "Batch 1944/4975 - Training Loss: 0.5529\n",
            "Batch 1945/4975 - Training Loss: 0.4064\n",
            "Batch 1946/4975 - Training Loss: 0.6711\n",
            "Batch 1947/4975 - Training Loss: 0.5025\n",
            "Batch 1948/4975 - Training Loss: 0.7521\n",
            "Batch 1949/4975 - Training Loss: 0.6474\n",
            "Batch 1950/4975 - Training Loss: 0.6803\n",
            "Batch 1951/4975 - Training Loss: 0.5378\n",
            "Batch 1952/4975 - Training Loss: 0.5533\n",
            "Batch 1953/4975 - Training Loss: 0.6822\n",
            "Batch 1954/4975 - Training Loss: 0.5463\n",
            "Batch 1955/4975 - Training Loss: 0.4329\n",
            "Batch 1956/4975 - Training Loss: 0.6068\n",
            "Batch 1957/4975 - Training Loss: 0.7324\n",
            "Batch 1958/4975 - Training Loss: 0.5608\n",
            "Batch 1959/4975 - Training Loss: 0.5098\n",
            "Batch 1960/4975 - Training Loss: 0.5281\n",
            "Batch 1961/4975 - Training Loss: 0.5495\n",
            "Batch 1962/4975 - Training Loss: 0.5235\n",
            "Batch 1963/4975 - Training Loss: 0.4861\n",
            "Batch 1964/4975 - Training Loss: 0.5982\n",
            "Batch 1965/4975 - Training Loss: 0.5188\n",
            "Batch 1966/4975 - Training Loss: 0.5651\n",
            "Batch 1967/4975 - Training Loss: 0.3813\n",
            "Batch 1968/4975 - Training Loss: 0.3936\n",
            "Batch 1969/4975 - Training Loss: 0.7830\n",
            "Batch 1970/4975 - Training Loss: 0.7885\n",
            "Batch 1971/4975 - Training Loss: 0.8140\n",
            "Batch 1972/4975 - Training Loss: 0.3239\n",
            "Batch 1973/4975 - Training Loss: 0.6608\n",
            "Batch 1974/4975 - Training Loss: 0.3564\n",
            "Batch 1975/4975 - Training Loss: 0.4573\n",
            "Batch 1976/4975 - Training Loss: 0.7364\n",
            "Batch 1977/4975 - Training Loss: 0.2546\n",
            "Batch 1978/4975 - Training Loss: 0.6316\n",
            "Batch 1979/4975 - Training Loss: 0.4794\n",
            "Batch 1980/4975 - Training Loss: 0.3936\n",
            "Batch 1981/4975 - Training Loss: 0.5872\n",
            "Batch 1982/4975 - Training Loss: 0.5794\n",
            "Batch 1983/4975 - Training Loss: 0.6295\n",
            "Batch 1984/4975 - Training Loss: 0.6651\n",
            "Batch 1985/4975 - Training Loss: 0.7440\n",
            "Batch 1986/4975 - Training Loss: 0.4905\n",
            "Batch 1987/4975 - Training Loss: 0.7270\n",
            "Batch 1988/4975 - Training Loss: 0.5550\n",
            "Batch 1989/4975 - Training Loss: 0.6798\n",
            "Batch 1990/4975 - Training Loss: 0.4494\n",
            "Batch 1991/4975 - Training Loss: 0.7889\n",
            "Batch 1992/4975 - Training Loss: 0.5863\n",
            "Batch 1993/4975 - Training Loss: 0.6704\n",
            "Batch 1994/4975 - Training Loss: 0.3182\n",
            "Batch 1995/4975 - Training Loss: 0.4025\n",
            "Batch 1996/4975 - Training Loss: 0.4069\n",
            "Batch 1997/4975 - Training Loss: 0.4253\n",
            "Batch 1998/4975 - Training Loss: 0.4573\n",
            "Batch 1999/4975 - Training Loss: 0.2837\n",
            "Batch 2000/4975 - Training Loss: 0.9097\n",
            "Batch 2001/4975 - Training Loss: 0.5281\n",
            "Batch 2002/4975 - Training Loss: 0.5664\n",
            "Batch 2003/4975 - Training Loss: 0.3659\n",
            "Batch 2004/4975 - Training Loss: 0.7903\n",
            "Batch 2005/4975 - Training Loss: 0.5079\n",
            "Batch 2006/4975 - Training Loss: 0.3845\n",
            "Batch 2007/4975 - Training Loss: 0.6963\n",
            "Batch 2008/4975 - Training Loss: 0.3735\n",
            "Batch 2009/4975 - Training Loss: 0.9112\n",
            "Batch 2010/4975 - Training Loss: 0.7411\n",
            "Batch 2011/4975 - Training Loss: 0.7958\n",
            "Batch 2012/4975 - Training Loss: 0.7267\n",
            "Batch 2013/4975 - Training Loss: 0.3461\n",
            "Batch 2014/4975 - Training Loss: 0.3702\n",
            "Batch 2015/4975 - Training Loss: 0.6118\n",
            "Batch 2016/4975 - Training Loss: 0.5832\n",
            "Batch 2017/4975 - Training Loss: 0.7405\n",
            "Batch 2018/4975 - Training Loss: 0.6184\n",
            "Batch 2019/4975 - Training Loss: 0.4611\n",
            "Batch 2020/4975 - Training Loss: 0.5833\n",
            "Batch 2021/4975 - Training Loss: 0.6671\n",
            "Batch 2022/4975 - Training Loss: 0.5587\n",
            "Batch 2023/4975 - Training Loss: 0.8328\n",
            "Batch 2024/4975 - Training Loss: 0.5617\n",
            "Batch 2025/4975 - Training Loss: 0.4768\n",
            "Batch 2026/4975 - Training Loss: 0.6173\n",
            "Batch 2027/4975 - Training Loss: 0.7625\n",
            "Batch 2028/4975 - Training Loss: 0.6003\n",
            "Batch 2029/4975 - Training Loss: 0.3438\n",
            "Batch 2030/4975 - Training Loss: 0.7020\n",
            "Batch 2031/4975 - Training Loss: 0.5158\n",
            "Batch 2032/4975 - Training Loss: 0.7126\n",
            "Batch 2033/4975 - Training Loss: 0.8770\n",
            "Batch 2034/4975 - Training Loss: 0.4268\n",
            "Batch 2035/4975 - Training Loss: 0.6720\n",
            "Batch 2036/4975 - Training Loss: 0.5279\n",
            "Batch 2037/4975 - Training Loss: 0.4957\n",
            "Batch 2038/4975 - Training Loss: 0.6480\n",
            "Batch 2039/4975 - Training Loss: 0.3319\n",
            "Batch 2040/4975 - Training Loss: 0.8792\n",
            "Batch 2041/4975 - Training Loss: 0.5250\n",
            "Batch 2042/4975 - Training Loss: 0.3569\n",
            "Batch 2043/4975 - Training Loss: 0.5117\n",
            "Batch 2044/4975 - Training Loss: 0.5616\n",
            "Batch 2045/4975 - Training Loss: 0.7235\n",
            "Batch 2046/4975 - Training Loss: 0.6840\n",
            "Batch 2047/4975 - Training Loss: 0.5839\n",
            "Batch 2048/4975 - Training Loss: 0.4558\n",
            "Batch 2049/4975 - Training Loss: 0.7273\n",
            "Batch 2050/4975 - Training Loss: 0.8219\n",
            "Batch 2051/4975 - Training Loss: 1.2224\n",
            "Batch 2052/4975 - Training Loss: 0.6356\n",
            "Batch 2053/4975 - Training Loss: 0.5262\n",
            "Batch 2054/4975 - Training Loss: 0.6780\n",
            "Batch 2055/4975 - Training Loss: 0.5532\n",
            "Batch 2056/4975 - Training Loss: 0.6655\n",
            "Batch 2057/4975 - Training Loss: 0.7625\n",
            "Batch 2058/4975 - Training Loss: 0.4631\n",
            "Batch 2059/4975 - Training Loss: 0.7972\n",
            "Batch 2060/4975 - Training Loss: 0.7352\n",
            "Batch 2061/4975 - Training Loss: 0.6593\n",
            "Batch 2062/4975 - Training Loss: 0.6672\n",
            "Batch 2063/4975 - Training Loss: 0.7609\n",
            "Batch 2064/4975 - Training Loss: 0.6489\n",
            "Batch 2065/4975 - Training Loss: 0.1956\n",
            "Batch 2066/4975 - Training Loss: 0.4244\n",
            "Batch 2067/4975 - Training Loss: 0.4409\n",
            "Batch 2068/4975 - Training Loss: 0.5495\n",
            "Batch 2069/4975 - Training Loss: 0.2872\n",
            "Batch 2070/4975 - Training Loss: 0.5112\n",
            "Batch 2071/4975 - Training Loss: 0.6094\n",
            "Batch 2072/4975 - Training Loss: 0.3335\n",
            "Batch 2073/4975 - Training Loss: 0.3047\n",
            "Batch 2074/4975 - Training Loss: 0.4452\n",
            "Batch 2075/4975 - Training Loss: 0.5811\n",
            "Batch 2076/4975 - Training Loss: 0.5287\n",
            "Batch 2077/4975 - Training Loss: 0.7298\n",
            "Batch 2078/4975 - Training Loss: 0.5572\n",
            "Batch 2079/4975 - Training Loss: 0.3400\n",
            "Batch 2080/4975 - Training Loss: 0.5608\n",
            "Batch 2081/4975 - Training Loss: 0.5555\n",
            "Batch 2082/4975 - Training Loss: 0.5001\n",
            "Batch 2083/4975 - Training Loss: 0.7509\n",
            "Batch 2084/4975 - Training Loss: 1.1902\n",
            "Batch 2085/4975 - Training Loss: 0.8355\n",
            "Batch 2086/4975 - Training Loss: 0.6075\n",
            "Batch 2087/4975 - Training Loss: 0.6261\n",
            "Batch 2088/4975 - Training Loss: 0.3081\n",
            "Batch 2089/4975 - Training Loss: 0.4858\n",
            "Batch 2090/4975 - Training Loss: 0.9369\n",
            "Batch 2091/4975 - Training Loss: 0.6463\n",
            "Batch 2092/4975 - Training Loss: 1.0429\n",
            "Batch 2093/4975 - Training Loss: 0.6695\n",
            "Batch 2094/4975 - Training Loss: 0.6361\n",
            "Batch 2095/4975 - Training Loss: 0.4786\n",
            "Batch 2096/4975 - Training Loss: 0.8920\n",
            "Batch 2097/4975 - Training Loss: 0.4772\n",
            "Batch 2098/4975 - Training Loss: 0.4012\n",
            "Batch 2099/4975 - Training Loss: 0.5347\n",
            "Batch 2100/4975 - Training Loss: 0.9398\n",
            "Batch 2101/4975 - Training Loss: 0.4761\n",
            "Batch 2102/4975 - Training Loss: 0.8013\n",
            "Batch 2103/4975 - Training Loss: 0.8946\n",
            "Batch 2104/4975 - Training Loss: 0.6107\n",
            "Batch 2105/4975 - Training Loss: 0.7957\n",
            "Batch 2106/4975 - Training Loss: 0.4943\n",
            "Batch 2107/4975 - Training Loss: 0.4591\n",
            "Batch 2108/4975 - Training Loss: 0.4556\n",
            "Batch 2109/4975 - Training Loss: 0.5669\n",
            "Batch 2110/4975 - Training Loss: 0.4860\n",
            "Batch 2111/4975 - Training Loss: 0.8768\n",
            "Batch 2112/4975 - Training Loss: 0.4595\n",
            "Batch 2113/4975 - Training Loss: 0.6090\n",
            "Batch 2114/4975 - Training Loss: 0.8115\n",
            "Batch 2115/4975 - Training Loss: 0.5390\n",
            "Batch 2116/4975 - Training Loss: 0.4379\n",
            "Batch 2117/4975 - Training Loss: 0.6331\n",
            "Batch 2118/4975 - Training Loss: 0.4871\n",
            "Batch 2119/4975 - Training Loss: 0.7773\n",
            "Batch 2120/4975 - Training Loss: 0.5206\n",
            "Batch 2121/4975 - Training Loss: 0.4623\n",
            "Batch 2122/4975 - Training Loss: 0.5988\n",
            "Batch 2123/4975 - Training Loss: 0.6649\n",
            "Batch 2124/4975 - Training Loss: 0.5955\n",
            "Batch 2125/4975 - Training Loss: 0.5895\n",
            "Batch 2126/4975 - Training Loss: 0.8341\n",
            "Batch 2127/4975 - Training Loss: 0.7219\n",
            "Batch 2128/4975 - Training Loss: 0.6939\n",
            "Batch 2129/4975 - Training Loss: 0.6050\n",
            "Batch 2130/4975 - Training Loss: 0.7275\n",
            "Batch 2131/4975 - Training Loss: 0.5793\n",
            "Batch 2132/4975 - Training Loss: 0.6055\n",
            "Batch 2133/4975 - Training Loss: 0.4341\n",
            "Batch 2134/4975 - Training Loss: 0.7206\n",
            "Batch 2135/4975 - Training Loss: 0.9072\n",
            "Batch 2136/4975 - Training Loss: 0.7670\n",
            "Batch 2137/4975 - Training Loss: 0.8315\n",
            "Batch 2138/4975 - Training Loss: 0.5130\n",
            "Batch 2139/4975 - Training Loss: 0.5894\n",
            "Batch 2140/4975 - Training Loss: 0.8443\n",
            "Batch 2141/4975 - Training Loss: 0.5247\n",
            "Batch 2142/4975 - Training Loss: 0.4487\n",
            "Batch 2143/4975 - Training Loss: 0.2410\n",
            "Batch 2144/4975 - Training Loss: 0.3866\n",
            "Batch 2145/4975 - Training Loss: 0.6406\n",
            "Batch 2146/4975 - Training Loss: 0.7905\n",
            "Batch 2147/4975 - Training Loss: 0.9595\n",
            "Batch 2148/4975 - Training Loss: 0.5241\n",
            "Batch 2149/4975 - Training Loss: 0.5841\n",
            "Batch 2150/4975 - Training Loss: 0.5678\n",
            "Batch 2151/4975 - Training Loss: 0.5904\n",
            "Batch 2152/4975 - Training Loss: 0.3846\n",
            "Batch 2153/4975 - Training Loss: 0.9032\n",
            "Batch 2154/4975 - Training Loss: 0.5893\n",
            "Batch 2155/4975 - Training Loss: 0.7140\n",
            "Batch 2156/4975 - Training Loss: 0.6190\n",
            "Batch 2157/4975 - Training Loss: 0.7360\n",
            "Batch 2158/4975 - Training Loss: 0.6369\n",
            "Batch 2159/4975 - Training Loss: 0.7758\n",
            "Batch 2160/4975 - Training Loss: 0.8353\n",
            "Batch 2161/4975 - Training Loss: 0.7885\n",
            "Batch 2162/4975 - Training Loss: 0.8050\n",
            "Batch 2163/4975 - Training Loss: 0.7592\n",
            "Batch 2164/4975 - Training Loss: 0.8088\n",
            "Batch 2165/4975 - Training Loss: 0.2929\n",
            "Batch 2166/4975 - Training Loss: 0.4849\n",
            "Batch 2167/4975 - Training Loss: 0.7106\n",
            "Batch 2168/4975 - Training Loss: 0.7254\n",
            "Batch 2169/4975 - Training Loss: 0.4179\n",
            "Batch 2170/4975 - Training Loss: 0.4513\n",
            "Batch 2171/4975 - Training Loss: 0.3825\n",
            "Batch 2172/4975 - Training Loss: 0.8552\n",
            "Batch 2173/4975 - Training Loss: 0.3620\n",
            "Batch 2174/4975 - Training Loss: 0.7011\n",
            "Batch 2175/4975 - Training Loss: 0.5181\n",
            "Batch 2176/4975 - Training Loss: 0.4715\n",
            "Batch 2177/4975 - Training Loss: 0.5941\n",
            "Batch 2178/4975 - Training Loss: 0.8587\n",
            "Batch 2179/4975 - Training Loss: 0.4515\n",
            "Batch 2180/4975 - Training Loss: 0.5028\n",
            "Batch 2181/4975 - Training Loss: 0.7144\n",
            "Batch 2182/4975 - Training Loss: 0.5417\n",
            "Batch 2183/4975 - Training Loss: 0.8400\n",
            "Batch 2184/4975 - Training Loss: 0.6820\n",
            "Batch 2185/4975 - Training Loss: 0.3369\n",
            "Batch 2186/4975 - Training Loss: 0.5254\n",
            "Batch 2187/4975 - Training Loss: 0.8386\n",
            "Batch 2188/4975 - Training Loss: 0.7802\n",
            "Batch 2189/4975 - Training Loss: 0.5657\n",
            "Batch 2190/4975 - Training Loss: 0.5315\n",
            "Batch 2191/4975 - Training Loss: 0.7366\n",
            "Batch 2192/4975 - Training Loss: 0.7607\n",
            "Batch 2193/4975 - Training Loss: 0.8034\n",
            "Batch 2194/4975 - Training Loss: 0.4893\n",
            "Batch 2195/4975 - Training Loss: 0.4894\n",
            "Batch 2196/4975 - Training Loss: 0.5029\n",
            "Batch 2197/4975 - Training Loss: 0.4816\n",
            "Batch 2198/4975 - Training Loss: 0.7402\n",
            "Batch 2199/4975 - Training Loss: 0.6953\n",
            "Batch 2200/4975 - Training Loss: 0.9869\n",
            "Batch 2201/4975 - Training Loss: 0.5484\n",
            "Batch 2202/4975 - Training Loss: 0.4170\n",
            "Batch 2203/4975 - Training Loss: 0.8474\n",
            "Batch 2204/4975 - Training Loss: 0.6865\n",
            "Batch 2205/4975 - Training Loss: 0.6887\n",
            "Batch 2206/4975 - Training Loss: 0.6653\n",
            "Batch 2207/4975 - Training Loss: 0.6004\n",
            "Batch 2208/4975 - Training Loss: 0.5102\n",
            "Batch 2209/4975 - Training Loss: 0.6110\n",
            "Batch 2210/4975 - Training Loss: 0.5918\n",
            "Batch 2211/4975 - Training Loss: 0.8035\n",
            "Batch 2212/4975 - Training Loss: 0.7473\n",
            "Batch 2213/4975 - Training Loss: 0.7906\n",
            "Batch 2214/4975 - Training Loss: 0.8916\n",
            "Batch 2215/4975 - Training Loss: 0.5532\n",
            "Batch 2216/4975 - Training Loss: 0.4279\n",
            "Batch 2217/4975 - Training Loss: 0.4820\n",
            "Batch 2218/4975 - Training Loss: 0.8643\n",
            "Batch 2219/4975 - Training Loss: 0.7938\n",
            "Batch 2220/4975 - Training Loss: 0.8021\n",
            "Batch 2221/4975 - Training Loss: 0.7512\n",
            "Batch 2222/4975 - Training Loss: 1.0241\n",
            "Batch 2223/4975 - Training Loss: 0.8028\n",
            "Batch 2224/4975 - Training Loss: 0.5289\n",
            "Batch 2225/4975 - Training Loss: 0.7843\n",
            "Batch 2226/4975 - Training Loss: 0.5809\n",
            "Batch 2227/4975 - Training Loss: 0.4310\n",
            "Batch 2228/4975 - Training Loss: 0.4833\n",
            "Batch 2229/4975 - Training Loss: 0.7387\n",
            "Batch 2230/4975 - Training Loss: 0.6088\n",
            "Batch 2231/4975 - Training Loss: 0.5897\n",
            "Batch 2232/4975 - Training Loss: 0.6080\n",
            "Batch 2233/4975 - Training Loss: 0.5146\n",
            "Batch 2234/4975 - Training Loss: 0.9357\n",
            "Batch 2235/4975 - Training Loss: 0.6006\n",
            "Batch 2236/4975 - Training Loss: 1.0216\n",
            "Batch 2237/4975 - Training Loss: 0.6384\n",
            "Batch 2238/4975 - Training Loss: 0.7912\n",
            "Batch 2239/4975 - Training Loss: 0.8456\n",
            "Batch 2240/4975 - Training Loss: 0.6216\n",
            "Batch 2241/4975 - Training Loss: 0.7123\n",
            "Batch 2242/4975 - Training Loss: 0.8760\n",
            "Batch 2243/4975 - Training Loss: 0.5915\n",
            "Batch 2244/4975 - Training Loss: 0.5597\n",
            "Batch 2245/4975 - Training Loss: 0.5105\n",
            "Batch 2246/4975 - Training Loss: 0.7202\n",
            "Batch 2247/4975 - Training Loss: 0.4477\n",
            "Batch 2248/4975 - Training Loss: 0.7415\n",
            "Batch 2249/4975 - Training Loss: 0.7179\n",
            "Batch 2250/4975 - Training Loss: 0.4479\n",
            "Batch 2251/4975 - Training Loss: 0.7503\n",
            "Batch 2252/4975 - Training Loss: 0.5615\n",
            "Batch 2253/4975 - Training Loss: 0.6128\n",
            "Batch 2254/4975 - Training Loss: 0.8758\n",
            "Batch 2255/4975 - Training Loss: 0.3871\n",
            "Batch 2256/4975 - Training Loss: 0.6926\n",
            "Batch 2257/4975 - Training Loss: 0.7569\n",
            "Batch 2258/4975 - Training Loss: 0.7077\n",
            "Batch 2259/4975 - Training Loss: 0.3782\n",
            "Batch 2260/4975 - Training Loss: 0.3501\n",
            "Batch 2261/4975 - Training Loss: 0.6634\n",
            "Batch 2262/4975 - Training Loss: 0.6245\n",
            "Batch 2263/4975 - Training Loss: 0.5442\n",
            "Batch 2264/4975 - Training Loss: 0.4144\n",
            "Batch 2265/4975 - Training Loss: 0.5624\n",
            "Batch 2266/4975 - Training Loss: 0.8694\n",
            "Batch 2267/4975 - Training Loss: 0.6831\n",
            "Batch 2268/4975 - Training Loss: 0.8368\n",
            "Batch 2269/4975 - Training Loss: 0.6527\n",
            "Batch 2270/4975 - Training Loss: 0.5790\n",
            "Batch 2271/4975 - Training Loss: 0.6493\n",
            "Batch 2272/4975 - Training Loss: 0.5381\n",
            "Batch 2273/4975 - Training Loss: 0.7161\n",
            "Batch 2274/4975 - Training Loss: 0.7411\n",
            "Batch 2275/4975 - Training Loss: 0.7529\n",
            "Batch 2276/4975 - Training Loss: 0.5977\n",
            "Batch 2277/4975 - Training Loss: 0.4982\n",
            "Batch 2278/4975 - Training Loss: 0.6745\n",
            "Batch 2279/4975 - Training Loss: 0.5949\n",
            "Batch 2280/4975 - Training Loss: 0.8035\n",
            "Batch 2281/4975 - Training Loss: 0.5753\n",
            "Batch 2282/4975 - Training Loss: 0.5997\n",
            "Batch 2283/4975 - Training Loss: 0.6157\n",
            "Batch 2284/4975 - Training Loss: 0.7476\n",
            "Batch 2285/4975 - Training Loss: 0.4372\n",
            "Batch 2286/4975 - Training Loss: 0.7469\n",
            "Batch 2287/4975 - Training Loss: 0.7084\n",
            "Batch 2288/4975 - Training Loss: 0.5782\n",
            "Batch 2289/4975 - Training Loss: 0.7594\n",
            "Batch 2290/4975 - Training Loss: 0.6994\n",
            "Batch 2291/4975 - Training Loss: 0.6827\n",
            "Batch 2292/4975 - Training Loss: 0.6442\n",
            "Batch 2293/4975 - Training Loss: 0.8918\n",
            "Batch 2294/4975 - Training Loss: 1.0277\n",
            "Batch 2295/4975 - Training Loss: 0.7227\n",
            "Batch 2296/4975 - Training Loss: 0.5897\n",
            "Batch 2297/4975 - Training Loss: 0.5462\n",
            "Batch 2298/4975 - Training Loss: 0.7984\n",
            "Batch 2299/4975 - Training Loss: 0.6900\n",
            "Batch 2300/4975 - Training Loss: 0.4408\n",
            "Batch 2301/4975 - Training Loss: 0.5286\n",
            "Batch 2302/4975 - Training Loss: 0.4901\n",
            "Batch 2303/4975 - Training Loss: 0.9135\n",
            "Batch 2304/4975 - Training Loss: 0.6087\n",
            "Batch 2305/4975 - Training Loss: 0.5510\n",
            "Batch 2306/4975 - Training Loss: 0.5775\n",
            "Batch 2307/4975 - Training Loss: 0.5230\n",
            "Batch 2308/4975 - Training Loss: 0.5977\n",
            "Batch 2309/4975 - Training Loss: 0.6154\n",
            "Batch 2310/4975 - Training Loss: 0.5254\n",
            "Batch 2311/4975 - Training Loss: 0.4915\n",
            "Batch 2312/4975 - Training Loss: 0.6699\n",
            "Batch 2313/4975 - Training Loss: 0.6382\n",
            "Batch 2314/4975 - Training Loss: 0.6067\n",
            "Batch 2315/4975 - Training Loss: 0.6807\n",
            "Batch 2316/4975 - Training Loss: 0.3974\n",
            "Batch 2317/4975 - Training Loss: 0.5718\n",
            "Batch 2318/4975 - Training Loss: 0.8618\n",
            "Batch 2319/4975 - Training Loss: 0.5384\n",
            "Batch 2320/4975 - Training Loss: 0.8847\n",
            "Batch 2321/4975 - Training Loss: 0.4848\n",
            "Batch 2322/4975 - Training Loss: 0.4357\n",
            "Batch 2323/4975 - Training Loss: 0.3459\n",
            "Batch 2324/4975 - Training Loss: 0.6185\n",
            "Batch 2325/4975 - Training Loss: 0.9439\n",
            "Batch 2326/4975 - Training Loss: 0.5498\n",
            "Batch 2327/4975 - Training Loss: 0.6216\n",
            "Batch 2328/4975 - Training Loss: 0.9918\n",
            "Batch 2329/4975 - Training Loss: 0.9040\n",
            "Batch 2330/4975 - Training Loss: 0.4488\n",
            "Batch 2331/4975 - Training Loss: 0.5237\n",
            "Batch 2332/4975 - Training Loss: 0.3681\n",
            "Batch 2333/4975 - Training Loss: 0.7444\n",
            "Batch 2334/4975 - Training Loss: 0.6005\n",
            "Batch 2335/4975 - Training Loss: 0.6905\n",
            "Batch 2336/4975 - Training Loss: 0.3024\n",
            "Batch 2337/4975 - Training Loss: 0.9059\n",
            "Batch 2338/4975 - Training Loss: 0.8465\n",
            "Batch 2339/4975 - Training Loss: 0.6604\n",
            "Batch 2340/4975 - Training Loss: 0.5913\n",
            "Batch 2341/4975 - Training Loss: 0.4367\n",
            "Batch 2342/4975 - Training Loss: 0.5505\n",
            "Batch 2343/4975 - Training Loss: 0.6157\n",
            "Batch 2344/4975 - Training Loss: 0.5568\n",
            "Batch 2345/4975 - Training Loss: 0.4342\n",
            "Batch 2346/4975 - Training Loss: 0.5832\n",
            "Batch 2347/4975 - Training Loss: 0.5790\n",
            "Batch 2348/4975 - Training Loss: 0.6021\n",
            "Batch 2349/4975 - Training Loss: 0.6719\n",
            "Batch 2350/4975 - Training Loss: 0.8023\n",
            "Batch 2351/4975 - Training Loss: 0.7859\n",
            "Batch 2352/4975 - Training Loss: 0.6155\n",
            "Batch 2353/4975 - Training Loss: 0.5257\n",
            "Batch 2354/4975 - Training Loss: 0.6433\n",
            "Batch 2355/4975 - Training Loss: 0.6392\n",
            "Batch 2356/4975 - Training Loss: 0.7054\n",
            "Batch 2357/4975 - Training Loss: 0.4554\n",
            "Batch 2358/4975 - Training Loss: 0.6909\n",
            "Batch 2359/4975 - Training Loss: 1.1093\n",
            "Batch 2360/4975 - Training Loss: 0.6397\n",
            "Batch 2361/4975 - Training Loss: 0.8359\n",
            "Batch 2362/4975 - Training Loss: 0.6542\n",
            "Batch 2363/4975 - Training Loss: 0.8841\n",
            "Batch 2364/4975 - Training Loss: 0.6836\n",
            "Batch 2365/4975 - Training Loss: 0.7456\n",
            "Batch 2366/4975 - Training Loss: 0.6456\n",
            "Batch 2367/4975 - Training Loss: 0.8918\n",
            "Batch 2368/4975 - Training Loss: 0.4053\n",
            "Batch 2369/4975 - Training Loss: 0.4147\n",
            "Batch 2370/4975 - Training Loss: 0.7363\n",
            "Batch 2371/4975 - Training Loss: 0.6661\n",
            "Batch 2372/4975 - Training Loss: 0.7127\n",
            "Batch 2373/4975 - Training Loss: 0.6355\n",
            "Batch 2374/4975 - Training Loss: 0.5312\n",
            "Batch 2375/4975 - Training Loss: 0.7658\n",
            "Batch 2376/4975 - Training Loss: 0.5846\n",
            "Batch 2377/4975 - Training Loss: 0.7322\n",
            "Batch 2378/4975 - Training Loss: 0.4867\n",
            "Batch 2379/4975 - Training Loss: 0.6456\n",
            "Batch 2380/4975 - Training Loss: 0.7544\n",
            "Batch 2381/4975 - Training Loss: 0.4465\n",
            "Batch 2382/4975 - Training Loss: 0.6259\n",
            "Batch 2383/4975 - Training Loss: 0.5803\n",
            "Batch 2384/4975 - Training Loss: 0.4535\n",
            "Batch 2385/4975 - Training Loss: 0.6481\n",
            "Batch 2386/4975 - Training Loss: 0.7018\n",
            "Batch 2387/4975 - Training Loss: 0.7050\n",
            "Batch 2388/4975 - Training Loss: 0.4084\n",
            "Batch 2389/4975 - Training Loss: 0.7955\n",
            "Batch 2390/4975 - Training Loss: 0.5726\n",
            "Batch 2391/4975 - Training Loss: 0.4004\n",
            "Batch 2392/4975 - Training Loss: 0.8185\n",
            "Batch 2393/4975 - Training Loss: 0.4293\n",
            "Batch 2394/4975 - Training Loss: 0.3469\n",
            "Batch 2395/4975 - Training Loss: 0.5585\n",
            "Batch 2396/4975 - Training Loss: 0.6012\n",
            "Batch 2397/4975 - Training Loss: 0.6899\n",
            "Batch 2398/4975 - Training Loss: 0.6407\n",
            "Batch 2399/4975 - Training Loss: 0.8554\n",
            "Batch 2400/4975 - Training Loss: 0.6161\n",
            "Batch 2401/4975 - Training Loss: 0.6819\n",
            "Batch 2402/4975 - Training Loss: 0.8416\n",
            "Batch 2403/4975 - Training Loss: 0.7185\n",
            "Batch 2404/4975 - Training Loss: 0.5281\n",
            "Batch 2405/4975 - Training Loss: 0.5771\n",
            "Batch 2406/4975 - Training Loss: 0.5936\n",
            "Batch 2407/4975 - Training Loss: 0.3825\n",
            "Batch 2408/4975 - Training Loss: 0.6435\n",
            "Batch 2409/4975 - Training Loss: 0.4926\n",
            "Batch 2410/4975 - Training Loss: 0.4607\n",
            "Batch 2411/4975 - Training Loss: 0.7915\n",
            "Batch 2412/4975 - Training Loss: 0.8843\n",
            "Batch 2413/4975 - Training Loss: 0.5714\n",
            "Batch 2414/4975 - Training Loss: 0.4421\n",
            "Batch 2415/4975 - Training Loss: 0.4645\n",
            "Batch 2416/4975 - Training Loss: 0.6683\n",
            "Batch 2417/4975 - Training Loss: 0.5519\n",
            "Batch 2418/4975 - Training Loss: 0.7118\n",
            "Batch 2419/4975 - Training Loss: 0.5445\n",
            "Batch 2420/4975 - Training Loss: 0.6502\n",
            "Batch 2421/4975 - Training Loss: 0.4738\n",
            "Batch 2422/4975 - Training Loss: 0.4722\n",
            "Batch 2423/4975 - Training Loss: 0.8078\n",
            "Batch 2424/4975 - Training Loss: 0.4901\n",
            "Batch 2425/4975 - Training Loss: 0.5226\n",
            "Batch 2426/4975 - Training Loss: 0.5203\n",
            "Batch 2427/4975 - Training Loss: 0.8443\n",
            "Batch 2428/4975 - Training Loss: 0.5091\n",
            "Batch 2429/4975 - Training Loss: 0.4730\n",
            "Batch 2430/4975 - Training Loss: 0.3821\n",
            "Batch 2431/4975 - Training Loss: 0.6398\n",
            "Batch 2432/4975 - Training Loss: 1.0365\n",
            "Batch 2433/4975 - Training Loss: 0.5736\n",
            "Batch 2434/4975 - Training Loss: 0.5594\n",
            "Batch 2435/4975 - Training Loss: 0.5984\n",
            "Batch 2436/4975 - Training Loss: 0.6527\n",
            "Batch 2437/4975 - Training Loss: 0.6760\n",
            "Batch 2438/4975 - Training Loss: 0.5084\n",
            "Batch 2439/4975 - Training Loss: 0.7474\n",
            "Batch 2440/4975 - Training Loss: 0.4563\n",
            "Batch 2441/4975 - Training Loss: 0.7176\n",
            "Batch 2442/4975 - Training Loss: 0.7199\n",
            "Batch 2443/4975 - Training Loss: 0.9470\n",
            "Batch 2444/4975 - Training Loss: 0.5056\n",
            "Batch 2445/4975 - Training Loss: 0.4632\n",
            "Batch 2446/4975 - Training Loss: 0.5656\n",
            "Batch 2447/4975 - Training Loss: 0.5992\n",
            "Batch 2448/4975 - Training Loss: 0.6619\n",
            "Batch 2449/4975 - Training Loss: 0.6097\n",
            "Batch 2450/4975 - Training Loss: 0.6314\n",
            "Batch 2451/4975 - Training Loss: 0.9323\n",
            "Batch 2452/4975 - Training Loss: 0.7994\n",
            "Batch 2453/4975 - Training Loss: 0.5217\n",
            "Batch 2454/4975 - Training Loss: 0.4648\n",
            "Batch 2455/4975 - Training Loss: 0.8037\n",
            "Batch 2456/4975 - Training Loss: 0.4966\n",
            "Batch 2457/4975 - Training Loss: 0.5455\n",
            "Batch 2458/4975 - Training Loss: 0.7206\n",
            "Batch 2459/4975 - Training Loss: 0.8444\n",
            "Batch 2460/4975 - Training Loss: 0.5335\n",
            "Batch 2461/4975 - Training Loss: 0.5486\n",
            "Batch 2462/4975 - Training Loss: 0.8522\n",
            "Batch 2463/4975 - Training Loss: 0.5914\n",
            "Batch 2464/4975 - Training Loss: 0.4317\n",
            "Batch 2465/4975 - Training Loss: 0.7137\n",
            "Batch 2466/4975 - Training Loss: 0.5223\n",
            "Batch 2467/4975 - Training Loss: 0.5046\n",
            "Batch 2468/4975 - Training Loss: 0.9810\n",
            "Batch 2469/4975 - Training Loss: 0.6853\n",
            "Batch 2470/4975 - Training Loss: 0.5546\n",
            "Batch 2471/4975 - Training Loss: 0.5510\n",
            "Batch 2472/4975 - Training Loss: 0.8359\n",
            "Batch 2473/4975 - Training Loss: 0.8026\n",
            "Batch 2474/4975 - Training Loss: 0.5045\n",
            "Batch 2475/4975 - Training Loss: 0.6840\n",
            "Batch 2476/4975 - Training Loss: 0.5521\n",
            "Batch 2477/4975 - Training Loss: 0.5417\n",
            "Batch 2478/4975 - Training Loss: 0.7451\n",
            "Batch 2479/4975 - Training Loss: 0.5070\n",
            "Batch 2480/4975 - Training Loss: 0.7050\n",
            "Batch 2481/4975 - Training Loss: 0.5096\n",
            "Batch 2482/4975 - Training Loss: 0.6362\n",
            "Batch 2483/4975 - Training Loss: 0.8807\n",
            "Batch 2484/4975 - Training Loss: 0.4992\n",
            "Batch 2485/4975 - Training Loss: 0.3636\n",
            "Batch 2486/4975 - Training Loss: 0.8162\n",
            "Batch 2487/4975 - Training Loss: 0.4946\n",
            "Batch 2488/4975 - Training Loss: 0.7935\n",
            "Batch 2489/4975 - Training Loss: 0.6742\n",
            "Batch 2490/4975 - Training Loss: 0.3592\n",
            "Batch 2491/4975 - Training Loss: 0.5418\n",
            "Batch 2492/4975 - Training Loss: 0.6643\n",
            "Batch 2493/4975 - Training Loss: 0.7186\n",
            "Batch 2494/4975 - Training Loss: 0.7482\n",
            "Batch 2495/4975 - Training Loss: 0.4279\n",
            "Batch 2496/4975 - Training Loss: 0.7658\n",
            "Batch 2497/4975 - Training Loss: 0.6131\n",
            "Batch 2498/4975 - Training Loss: 0.8836\n",
            "Batch 2499/4975 - Training Loss: 0.3591\n",
            "Batch 2500/4975 - Training Loss: 0.6966\n",
            "Batch 2501/4975 - Training Loss: 0.8730\n",
            "Batch 2502/4975 - Training Loss: 0.6648\n",
            "Batch 2503/4975 - Training Loss: 0.3660\n",
            "Batch 2504/4975 - Training Loss: 0.6725\n",
            "Batch 2505/4975 - Training Loss: 0.6061\n",
            "Batch 2506/4975 - Training Loss: 0.6982\n",
            "Batch 2507/4975 - Training Loss: 0.4694\n",
            "Batch 2508/4975 - Training Loss: 0.6863\n",
            "Batch 2509/4975 - Training Loss: 0.8092\n",
            "Batch 2510/4975 - Training Loss: 0.4429\n",
            "Batch 2511/4975 - Training Loss: 0.5217\n",
            "Batch 2512/4975 - Training Loss: 0.4005\n",
            "Batch 2513/4975 - Training Loss: 0.5134\n",
            "Batch 2514/4975 - Training Loss: 0.2794\n",
            "Batch 2515/4975 - Training Loss: 0.4290\n",
            "Batch 2516/4975 - Training Loss: 0.6124\n",
            "Batch 2517/4975 - Training Loss: 0.5332\n",
            "Batch 2518/4975 - Training Loss: 0.9025\n",
            "Batch 2519/4975 - Training Loss: 0.7261\n",
            "Batch 2520/4975 - Training Loss: 0.4282\n",
            "Batch 2521/4975 - Training Loss: 0.7901\n",
            "Batch 2522/4975 - Training Loss: 0.5615\n",
            "Batch 2523/4975 - Training Loss: 0.9504\n",
            "Batch 2524/4975 - Training Loss: 0.7181\n",
            "Batch 2525/4975 - Training Loss: 0.5251\n",
            "Batch 2526/4975 - Training Loss: 0.7564\n",
            "Batch 2527/4975 - Training Loss: 0.7211\n",
            "Batch 2528/4975 - Training Loss: 0.7354\n",
            "Batch 2529/4975 - Training Loss: 0.4164\n",
            "Batch 2530/4975 - Training Loss: 0.7844\n",
            "Batch 2531/4975 - Training Loss: 0.6784\n",
            "Batch 2532/4975 - Training Loss: 1.0227\n",
            "Batch 2533/4975 - Training Loss: 0.6542\n",
            "Batch 2534/4975 - Training Loss: 0.3694\n",
            "Batch 2535/4975 - Training Loss: 0.6764\n",
            "Batch 2536/4975 - Training Loss: 0.6492\n",
            "Batch 2537/4975 - Training Loss: 0.4386\n",
            "Batch 2538/4975 - Training Loss: 0.8418\n",
            "Batch 2539/4975 - Training Loss: 1.0131\n",
            "Batch 2540/4975 - Training Loss: 1.4475\n",
            "Batch 2541/4975 - Training Loss: 0.8005\n",
            "Batch 2542/4975 - Training Loss: 0.6388\n",
            "Batch 2543/4975 - Training Loss: 1.0291\n",
            "Batch 2544/4975 - Training Loss: 0.9588\n",
            "Batch 2545/4975 - Training Loss: 0.5129\n",
            "Batch 2546/4975 - Training Loss: 0.6780\n",
            "Batch 2547/4975 - Training Loss: 0.5957\n",
            "Batch 2548/4975 - Training Loss: 0.5078\n",
            "Batch 2549/4975 - Training Loss: 0.5366\n",
            "Batch 2550/4975 - Training Loss: 0.4916\n",
            "Batch 2551/4975 - Training Loss: 0.6643\n",
            "Batch 2552/4975 - Training Loss: 0.8664\n",
            "Batch 2553/4975 - Training Loss: 0.6271\n",
            "Batch 2554/4975 - Training Loss: 0.7337\n",
            "Batch 2555/4975 - Training Loss: 0.4986\n",
            "Batch 2556/4975 - Training Loss: 0.6988\n",
            "Batch 2557/4975 - Training Loss: 0.3814\n",
            "Batch 2558/4975 - Training Loss: 0.6005\n",
            "Batch 2559/4975 - Training Loss: 0.6149\n",
            "Batch 2560/4975 - Training Loss: 0.7286\n",
            "Batch 2561/4975 - Training Loss: 0.7122\n",
            "Batch 2562/4975 - Training Loss: 0.5509\n",
            "Batch 2563/4975 - Training Loss: 0.5757\n",
            "Batch 2564/4975 - Training Loss: 0.5748\n",
            "Batch 2565/4975 - Training Loss: 0.5821\n",
            "Batch 2566/4975 - Training Loss: 1.0116\n",
            "Batch 2567/4975 - Training Loss: 0.6650\n",
            "Batch 2568/4975 - Training Loss: 0.8228\n",
            "Batch 2569/4975 - Training Loss: 0.4385\n",
            "Batch 2570/4975 - Training Loss: 0.5949\n",
            "Batch 2571/4975 - Training Loss: 0.4705\n",
            "Batch 2572/4975 - Training Loss: 0.4909\n",
            "Batch 2573/4975 - Training Loss: 0.7056\n",
            "Batch 2574/4975 - Training Loss: 0.7427\n",
            "Batch 2575/4975 - Training Loss: 0.6121\n",
            "Batch 2576/4975 - Training Loss: 0.7018\n",
            "Batch 2577/4975 - Training Loss: 0.6754\n",
            "Batch 2578/4975 - Training Loss: 0.6177\n",
            "Batch 2579/4975 - Training Loss: 0.5807\n",
            "Batch 2580/4975 - Training Loss: 0.6179\n",
            "Batch 2581/4975 - Training Loss: 0.6363\n",
            "Batch 2582/4975 - Training Loss: 0.5753\n",
            "Batch 2583/4975 - Training Loss: 0.6641\n",
            "Batch 2584/4975 - Training Loss: 0.5677\n",
            "Batch 2585/4975 - Training Loss: 0.7359\n",
            "Batch 2586/4975 - Training Loss: 0.5210\n",
            "Batch 2587/4975 - Training Loss: 0.7212\n",
            "Batch 2588/4975 - Training Loss: 0.4822\n",
            "Batch 2589/4975 - Training Loss: 0.6542\n",
            "Batch 2590/4975 - Training Loss: 0.5762\n",
            "Batch 2591/4975 - Training Loss: 0.7071\n",
            "Batch 2592/4975 - Training Loss: 0.4681\n",
            "Batch 2593/4975 - Training Loss: 0.4531\n",
            "Batch 2594/4975 - Training Loss: 0.9457\n",
            "Batch 2595/4975 - Training Loss: 0.7339\n",
            "Batch 2596/4975 - Training Loss: 0.4729\n",
            "Batch 2597/4975 - Training Loss: 0.6064\n",
            "Batch 2598/4975 - Training Loss: 0.4917\n",
            "Batch 2599/4975 - Training Loss: 0.6316\n",
            "Batch 2600/4975 - Training Loss: 0.3582\n",
            "Batch 2601/4975 - Training Loss: 0.5349\n",
            "Batch 2602/4975 - Training Loss: 0.5504\n",
            "Batch 2603/4975 - Training Loss: 0.6459\n",
            "Batch 2604/4975 - Training Loss: 0.5987\n",
            "Batch 2605/4975 - Training Loss: 0.9344\n",
            "Batch 2606/4975 - Training Loss: 0.6844\n",
            "Batch 2607/4975 - Training Loss: 0.3523\n",
            "Batch 2608/4975 - Training Loss: 0.5973\n",
            "Batch 2609/4975 - Training Loss: 1.0798\n",
            "Batch 2610/4975 - Training Loss: 0.6329\n",
            "Batch 2611/4975 - Training Loss: 0.2985\n",
            "Batch 2612/4975 - Training Loss: 0.7406\n",
            "Batch 2613/4975 - Training Loss: 0.6800\n",
            "Batch 2614/4975 - Training Loss: 0.5369\n",
            "Batch 2615/4975 - Training Loss: 0.3899\n",
            "Batch 2616/4975 - Training Loss: 0.3665\n",
            "Batch 2617/4975 - Training Loss: 0.6327\n",
            "Batch 2618/4975 - Training Loss: 0.7535\n",
            "Batch 2619/4975 - Training Loss: 0.5127\n",
            "Batch 2620/4975 - Training Loss: 0.7321\n",
            "Batch 2621/4975 - Training Loss: 0.7926\n",
            "Batch 2622/4975 - Training Loss: 0.3995\n",
            "Batch 2623/4975 - Training Loss: 0.8252\n",
            "Batch 2624/4975 - Training Loss: 0.8103\n",
            "Batch 2625/4975 - Training Loss: 0.6408\n",
            "Batch 2626/4975 - Training Loss: 0.4334\n",
            "Batch 2627/4975 - Training Loss: 1.0752\n",
            "Batch 2628/4975 - Training Loss: 0.6317\n",
            "Batch 2629/4975 - Training Loss: 0.7611\n",
            "Batch 2630/4975 - Training Loss: 0.6027\n",
            "Batch 2631/4975 - Training Loss: 0.7102\n",
            "Batch 2632/4975 - Training Loss: 0.7300\n",
            "Batch 2633/4975 - Training Loss: 0.5714\n",
            "Batch 2634/4975 - Training Loss: 1.0024\n",
            "Batch 2635/4975 - Training Loss: 0.5183\n",
            "Batch 2636/4975 - Training Loss: 0.7147\n",
            "Batch 2637/4975 - Training Loss: 0.6849\n",
            "Batch 2638/4975 - Training Loss: 0.7274\n",
            "Batch 2639/4975 - Training Loss: 1.2344\n",
            "Batch 2640/4975 - Training Loss: 0.8544\n",
            "Batch 2641/4975 - Training Loss: 0.4719\n",
            "Batch 2642/4975 - Training Loss: 0.4664\n",
            "Batch 2643/4975 - Training Loss: 0.6559\n",
            "Batch 2644/4975 - Training Loss: 0.6598\n",
            "Batch 2645/4975 - Training Loss: 0.4694\n",
            "Batch 2646/4975 - Training Loss: 0.7023\n",
            "Batch 2647/4975 - Training Loss: 0.6347\n",
            "Batch 2648/4975 - Training Loss: 0.5197\n",
            "Batch 2649/4975 - Training Loss: 0.4295\n",
            "Batch 2650/4975 - Training Loss: 0.5035\n",
            "Batch 2651/4975 - Training Loss: 0.4395\n",
            "Batch 2652/4975 - Training Loss: 0.3124\n",
            "Batch 2653/4975 - Training Loss: 0.3444\n",
            "Batch 2654/4975 - Training Loss: 0.7686\n",
            "Batch 2655/4975 - Training Loss: 0.5367\n",
            "Batch 2656/4975 - Training Loss: 0.2426\n",
            "Batch 2657/4975 - Training Loss: 0.6933\n",
            "Batch 2658/4975 - Training Loss: 0.7295\n",
            "Batch 2659/4975 - Training Loss: 0.3953\n",
            "Batch 2660/4975 - Training Loss: 0.5910\n",
            "Batch 2661/4975 - Training Loss: 0.8327\n",
            "Batch 2662/4975 - Training Loss: 0.5841\n",
            "Batch 2663/4975 - Training Loss: 0.6004\n",
            "Batch 2664/4975 - Training Loss: 0.3012\n",
            "Batch 2665/4975 - Training Loss: 0.4982\n",
            "Batch 2666/4975 - Training Loss: 0.7054\n",
            "Batch 2667/4975 - Training Loss: 0.4364\n",
            "Batch 2668/4975 - Training Loss: 0.4277\n",
            "Batch 2669/4975 - Training Loss: 0.4941\n",
            "Batch 2670/4975 - Training Loss: 0.6163\n",
            "Batch 2671/4975 - Training Loss: 0.7374\n",
            "Batch 2672/4975 - Training Loss: 0.3467\n",
            "Batch 2673/4975 - Training Loss: 0.6005\n",
            "Batch 2674/4975 - Training Loss: 0.6087\n",
            "Batch 2675/4975 - Training Loss: 0.7190\n",
            "Batch 2676/4975 - Training Loss: 0.8020\n",
            "Batch 2677/4975 - Training Loss: 1.0782\n",
            "Batch 2678/4975 - Training Loss: 0.6044\n",
            "Batch 2679/4975 - Training Loss: 0.3994\n",
            "Batch 2680/4975 - Training Loss: 0.4113\n",
            "Batch 2681/4975 - Training Loss: 0.9023\n",
            "Batch 2682/4975 - Training Loss: 0.2815\n",
            "Batch 2683/4975 - Training Loss: 0.5511\n",
            "Batch 2684/4975 - Training Loss: 0.5727\n",
            "Batch 2685/4975 - Training Loss: 0.5997\n",
            "Batch 2686/4975 - Training Loss: 0.5902\n",
            "Batch 2687/4975 - Training Loss: 0.4027\n",
            "Batch 2688/4975 - Training Loss: 0.6026\n",
            "Batch 2689/4975 - Training Loss: 0.8258\n",
            "Batch 2690/4975 - Training Loss: 0.5327\n",
            "Batch 2691/4975 - Training Loss: 0.6231\n",
            "Batch 2692/4975 - Training Loss: 0.7217\n",
            "Batch 2693/4975 - Training Loss: 0.2744\n",
            "Batch 2694/4975 - Training Loss: 0.6431\n",
            "Batch 2695/4975 - Training Loss: 0.7104\n",
            "Batch 2696/4975 - Training Loss: 0.6477\n",
            "Batch 2697/4975 - Training Loss: 0.6728\n",
            "Batch 2698/4975 - Training Loss: 0.6974\n",
            "Batch 2699/4975 - Training Loss: 0.6406\n",
            "Batch 2700/4975 - Training Loss: 0.8704\n",
            "Batch 2701/4975 - Training Loss: 0.4955\n",
            "Batch 2702/4975 - Training Loss: 0.5961\n",
            "Batch 2703/4975 - Training Loss: 0.8063\n",
            "Batch 2704/4975 - Training Loss: 0.5597\n",
            "Batch 2705/4975 - Training Loss: 0.8048\n",
            "Batch 2706/4975 - Training Loss: 0.8032\n",
            "Batch 2707/4975 - Training Loss: 0.6662\n",
            "Batch 2708/4975 - Training Loss: 0.5534\n",
            "Batch 2709/4975 - Training Loss: 0.6621\n",
            "Batch 2710/4975 - Training Loss: 0.6970\n",
            "Batch 2711/4975 - Training Loss: 0.6214\n",
            "Batch 2712/4975 - Training Loss: 0.7983\n",
            "Batch 2713/4975 - Training Loss: 0.5449\n",
            "Batch 2714/4975 - Training Loss: 0.7620\n",
            "Batch 2715/4975 - Training Loss: 0.8923\n",
            "Batch 2716/4975 - Training Loss: 0.4580\n",
            "Batch 2717/4975 - Training Loss: 0.8289\n",
            "Batch 2718/4975 - Training Loss: 0.4320\n",
            "Batch 2719/4975 - Training Loss: 0.4130\n",
            "Batch 2720/4975 - Training Loss: 0.6301\n",
            "Batch 2721/4975 - Training Loss: 0.7337\n",
            "Batch 2722/4975 - Training Loss: 0.9938\n",
            "Batch 2723/4975 - Training Loss: 0.7313\n",
            "Batch 2724/4975 - Training Loss: 0.5920\n",
            "Batch 2725/4975 - Training Loss: 0.5330\n",
            "Batch 2726/4975 - Training Loss: 0.4034\n",
            "Batch 2727/4975 - Training Loss: 0.6817\n",
            "Batch 2728/4975 - Training Loss: 0.5452\n",
            "Batch 2729/4975 - Training Loss: 0.5302\n",
            "Batch 2730/4975 - Training Loss: 0.5804\n",
            "Batch 2731/4975 - Training Loss: 0.3142\n",
            "Batch 2732/4975 - Training Loss: 0.5558\n",
            "Batch 2733/4975 - Training Loss: 0.4520\n",
            "Batch 2734/4975 - Training Loss: 0.4585\n",
            "Batch 2735/4975 - Training Loss: 0.4897\n",
            "Batch 2736/4975 - Training Loss: 0.4258\n",
            "Batch 2737/4975 - Training Loss: 0.3141\n",
            "Batch 2738/4975 - Training Loss: 0.6658\n",
            "Batch 2739/4975 - Training Loss: 0.5577\n",
            "Batch 2740/4975 - Training Loss: 0.6352\n",
            "Batch 2741/4975 - Training Loss: 0.9134\n",
            "Batch 2742/4975 - Training Loss: 0.4831\n",
            "Batch 2743/4975 - Training Loss: 0.4318\n",
            "Batch 2744/4975 - Training Loss: 0.7905\n",
            "Batch 2745/4975 - Training Loss: 0.4678\n",
            "Batch 2746/4975 - Training Loss: 0.4366\n",
            "Batch 2747/4975 - Training Loss: 0.4766\n",
            "Batch 2748/4975 - Training Loss: 0.4989\n",
            "Batch 2749/4975 - Training Loss: 1.0856\n",
            "Batch 2750/4975 - Training Loss: 0.7760\n",
            "Batch 2751/4975 - Training Loss: 0.5334\n",
            "Batch 2752/4975 - Training Loss: 0.7753\n",
            "Batch 2753/4975 - Training Loss: 0.5594\n",
            "Batch 2754/4975 - Training Loss: 0.4529\n",
            "Batch 2755/4975 - Training Loss: 0.7540\n",
            "Batch 2756/4975 - Training Loss: 0.9840\n",
            "Batch 2757/4975 - Training Loss: 0.8469\n",
            "Batch 2758/4975 - Training Loss: 0.6201\n",
            "Batch 2759/4975 - Training Loss: 0.4240\n",
            "Batch 2760/4975 - Training Loss: 0.5277\n",
            "Batch 2761/4975 - Training Loss: 0.5865\n",
            "Batch 2762/4975 - Training Loss: 0.3869\n",
            "Batch 2763/4975 - Training Loss: 0.4589\n",
            "Batch 2764/4975 - Training Loss: 0.2871\n",
            "Batch 2765/4975 - Training Loss: 0.4189\n",
            "Batch 2766/4975 - Training Loss: 0.6583\n",
            "Batch 2767/4975 - Training Loss: 0.4765\n",
            "Batch 2768/4975 - Training Loss: 0.4148\n",
            "Batch 2769/4975 - Training Loss: 0.5159\n",
            "Batch 2770/4975 - Training Loss: 0.6918\n",
            "Batch 2771/4975 - Training Loss: 0.5513\n",
            "Batch 2772/4975 - Training Loss: 0.9039\n",
            "Batch 2773/4975 - Training Loss: 0.7449\n",
            "Batch 2774/4975 - Training Loss: 0.4377\n",
            "Batch 2775/4975 - Training Loss: 0.6852\n",
            "Batch 2776/4975 - Training Loss: 0.5051\n",
            "Batch 2777/4975 - Training Loss: 0.5790\n",
            "Batch 2778/4975 - Training Loss: 0.8928\n",
            "Batch 2779/4975 - Training Loss: 0.8612\n",
            "Batch 2780/4975 - Training Loss: 0.7560\n",
            "Batch 2781/4975 - Training Loss: 0.6123\n",
            "Batch 2782/4975 - Training Loss: 0.5382\n",
            "Batch 2783/4975 - Training Loss: 1.1151\n",
            "Batch 2784/4975 - Training Loss: 0.8248\n",
            "Batch 2785/4975 - Training Loss: 0.3876\n",
            "Batch 2786/4975 - Training Loss: 0.8889\n",
            "Batch 2787/4975 - Training Loss: 0.6757\n",
            "Batch 2788/4975 - Training Loss: 0.8167\n",
            "Batch 2789/4975 - Training Loss: 0.5931\n",
            "Batch 2790/4975 - Training Loss: 0.3320\n",
            "Batch 2791/4975 - Training Loss: 0.4636\n",
            "Batch 2792/4975 - Training Loss: 0.5567\n",
            "Batch 2793/4975 - Training Loss: 0.6556\n",
            "Batch 2794/4975 - Training Loss: 0.3360\n",
            "Batch 2795/4975 - Training Loss: 0.6109\n",
            "Batch 2796/4975 - Training Loss: 0.5027\n",
            "Batch 2797/4975 - Training Loss: 0.5131\n",
            "Batch 2798/4975 - Training Loss: 0.6368\n",
            "Batch 2799/4975 - Training Loss: 0.7568\n",
            "Batch 2800/4975 - Training Loss: 0.7779\n",
            "Batch 2801/4975 - Training Loss: 0.5212\n",
            "Batch 2802/4975 - Training Loss: 0.8390\n",
            "Batch 2803/4975 - Training Loss: 0.3304\n",
            "Batch 2804/4975 - Training Loss: 0.6290\n",
            "Batch 2805/4975 - Training Loss: 0.6669\n",
            "Batch 2806/4975 - Training Loss: 0.5815\n",
            "Batch 2807/4975 - Training Loss: 0.7249\n",
            "Batch 2808/4975 - Training Loss: 0.7043\n",
            "Batch 2809/4975 - Training Loss: 0.6611\n",
            "Batch 2810/4975 - Training Loss: 0.4822\n",
            "Batch 2811/4975 - Training Loss: 0.5242\n",
            "Batch 2812/4975 - Training Loss: 0.9589\n",
            "Batch 2813/4975 - Training Loss: 0.7108\n",
            "Batch 2814/4975 - Training Loss: 0.6364\n",
            "Batch 2815/4975 - Training Loss: 0.4258\n",
            "Batch 2816/4975 - Training Loss: 0.4613\n",
            "Batch 2817/4975 - Training Loss: 0.6943\n",
            "Batch 2818/4975 - Training Loss: 0.5698\n",
            "Batch 2819/4975 - Training Loss: 0.7259\n",
            "Batch 2820/4975 - Training Loss: 0.3786\n",
            "Batch 2821/4975 - Training Loss: 0.5542\n",
            "Batch 2822/4975 - Training Loss: 0.7480\n",
            "Batch 2823/4975 - Training Loss: 0.6620\n",
            "Batch 2824/4975 - Training Loss: 0.6867\n",
            "Batch 2825/4975 - Training Loss: 0.4788\n",
            "Batch 2826/4975 - Training Loss: 0.8301\n",
            "Batch 2827/4975 - Training Loss: 0.4155\n",
            "Batch 2828/4975 - Training Loss: 0.7309\n",
            "Batch 2829/4975 - Training Loss: 0.3809\n",
            "Batch 2830/4975 - Training Loss: 0.6277\n",
            "Batch 2831/4975 - Training Loss: 0.6300\n",
            "Batch 2832/4975 - Training Loss: 0.6877\n",
            "Batch 2833/4975 - Training Loss: 0.9205\n",
            "Batch 2834/4975 - Training Loss: 0.8462\n",
            "Batch 2835/4975 - Training Loss: 0.3960\n",
            "Batch 2836/4975 - Training Loss: 0.4882\n",
            "Batch 2837/4975 - Training Loss: 0.4372\n",
            "Batch 2838/4975 - Training Loss: 0.4198\n",
            "Batch 2839/4975 - Training Loss: 0.6058\n",
            "Batch 2840/4975 - Training Loss: 0.6751\n",
            "Batch 2841/4975 - Training Loss: 0.6820\n",
            "Batch 2842/4975 - Training Loss: 0.6116\n",
            "Batch 2843/4975 - Training Loss: 0.5077\n",
            "Batch 2844/4975 - Training Loss: 0.4620\n",
            "Batch 2845/4975 - Training Loss: 0.7106\n",
            "Batch 2846/4975 - Training Loss: 0.7917\n",
            "Batch 2847/4975 - Training Loss: 0.6357\n",
            "Batch 2848/4975 - Training Loss: 0.3671\n",
            "Batch 2849/4975 - Training Loss: 0.7802\n",
            "Batch 2850/4975 - Training Loss: 0.4278\n",
            "Batch 2851/4975 - Training Loss: 0.5743\n",
            "Batch 2852/4975 - Training Loss: 0.6062\n",
            "Batch 2853/4975 - Training Loss: 0.5617\n",
            "Batch 2854/4975 - Training Loss: 0.7124\n",
            "Batch 2855/4975 - Training Loss: 0.5464\n",
            "Batch 2856/4975 - Training Loss: 0.5486\n",
            "Batch 2857/4975 - Training Loss: 0.9132\n",
            "Batch 2858/4975 - Training Loss: 0.9811\n",
            "Batch 2859/4975 - Training Loss: 0.6582\n",
            "Batch 2860/4975 - Training Loss: 0.4892\n",
            "Batch 2861/4975 - Training Loss: 0.6243\n",
            "Batch 2862/4975 - Training Loss: 0.5755\n",
            "Batch 2863/4975 - Training Loss: 0.5123\n",
            "Batch 2864/4975 - Training Loss: 0.7682\n",
            "Batch 2865/4975 - Training Loss: 0.5356\n",
            "Batch 2866/4975 - Training Loss: 0.5175\n",
            "Batch 2867/4975 - Training Loss: 0.7826\n",
            "Batch 2868/4975 - Training Loss: 0.4636\n",
            "Batch 2869/4975 - Training Loss: 0.5194\n",
            "Batch 2870/4975 - Training Loss: 0.6154\n",
            "Batch 2871/4975 - Training Loss: 0.5743\n",
            "Batch 2872/4975 - Training Loss: 0.3463\n",
            "Batch 2873/4975 - Training Loss: 0.5268\n",
            "Batch 2874/4975 - Training Loss: 0.8654\n",
            "Batch 2875/4975 - Training Loss: 0.5864\n",
            "Batch 2876/4975 - Training Loss: 0.5530\n",
            "Batch 2877/4975 - Training Loss: 0.5522\n",
            "Batch 2878/4975 - Training Loss: 0.6117\n",
            "Batch 2879/4975 - Training Loss: 1.2208\n",
            "Batch 2880/4975 - Training Loss: 0.7331\n",
            "Batch 2881/4975 - Training Loss: 0.5028\n",
            "Batch 2882/4975 - Training Loss: 0.3095\n",
            "Batch 2883/4975 - Training Loss: 0.7076\n",
            "Batch 2884/4975 - Training Loss: 0.5940\n",
            "Batch 2885/4975 - Training Loss: 0.4582\n",
            "Batch 2886/4975 - Training Loss: 0.4654\n",
            "Batch 2887/4975 - Training Loss: 0.6613\n",
            "Batch 2888/4975 - Training Loss: 0.8590\n",
            "Batch 2889/4975 - Training Loss: 0.7221\n",
            "Batch 2890/4975 - Training Loss: 0.3820\n",
            "Batch 2891/4975 - Training Loss: 0.5879\n",
            "Batch 2892/4975 - Training Loss: 0.8748\n",
            "Batch 2893/4975 - Training Loss: 0.5603\n",
            "Batch 2894/4975 - Training Loss: 0.4527\n",
            "Batch 2895/4975 - Training Loss: 0.3793\n",
            "Batch 2896/4975 - Training Loss: 0.4678\n",
            "Batch 2897/4975 - Training Loss: 0.5120\n",
            "Batch 2898/4975 - Training Loss: 0.7059\n",
            "Batch 2899/4975 - Training Loss: 0.5294\n",
            "Batch 2900/4975 - Training Loss: 0.5795\n",
            "Batch 2901/4975 - Training Loss: 0.6141\n",
            "Batch 2902/4975 - Training Loss: 0.6432\n",
            "Batch 2903/4975 - Training Loss: 0.8701\n",
            "Batch 2904/4975 - Training Loss: 0.3646\n",
            "Batch 2905/4975 - Training Loss: 0.7085\n",
            "Batch 2906/4975 - Training Loss: 0.6117\n",
            "Batch 2907/4975 - Training Loss: 0.2551\n",
            "Batch 2908/4975 - Training Loss: 0.5996\n",
            "Batch 2909/4975 - Training Loss: 0.6006\n",
            "Batch 2910/4975 - Training Loss: 0.4984\n",
            "Batch 2911/4975 - Training Loss: 0.5973\n",
            "Batch 2912/4975 - Training Loss: 0.4913\n",
            "Batch 2913/4975 - Training Loss: 0.6853\n",
            "Batch 2914/4975 - Training Loss: 0.7035\n",
            "Batch 2915/4975 - Training Loss: 0.7572\n",
            "Batch 2916/4975 - Training Loss: 0.4282\n",
            "Batch 2917/4975 - Training Loss: 0.5226\n",
            "Batch 2918/4975 - Training Loss: 0.7366\n",
            "Batch 2919/4975 - Training Loss: 0.8152\n",
            "Batch 2920/4975 - Training Loss: 0.5256\n",
            "Batch 2921/4975 - Training Loss: 0.6526\n",
            "Batch 2922/4975 - Training Loss: 0.5609\n",
            "Batch 2923/4975 - Training Loss: 0.6217\n",
            "Batch 2924/4975 - Training Loss: 0.5308\n",
            "Batch 2925/4975 - Training Loss: 0.4116\n",
            "Batch 2926/4975 - Training Loss: 0.7736\n",
            "Batch 2927/4975 - Training Loss: 0.4860\n",
            "Batch 2928/4975 - Training Loss: 0.5742\n",
            "Batch 2929/4975 - Training Loss: 0.6762\n",
            "Batch 2930/4975 - Training Loss: 0.6447\n",
            "Batch 2931/4975 - Training Loss: 0.5301\n",
            "Batch 2932/4975 - Training Loss: 0.7026\n",
            "Batch 2933/4975 - Training Loss: 0.7954\n",
            "Batch 2934/4975 - Training Loss: 0.2988\n",
            "Batch 2935/4975 - Training Loss: 0.7405\n",
            "Batch 2936/4975 - Training Loss: 0.4969\n",
            "Batch 2937/4975 - Training Loss: 0.4193\n",
            "Batch 2938/4975 - Training Loss: 0.4438\n",
            "Batch 2939/4975 - Training Loss: 0.8942\n",
            "Batch 2940/4975 - Training Loss: 0.4167\n",
            "Batch 2941/4975 - Training Loss: 0.5643\n",
            "Batch 2942/4975 - Training Loss: 0.7986\n",
            "Batch 2943/4975 - Training Loss: 0.5732\n",
            "Batch 2944/4975 - Training Loss: 0.4753\n",
            "Batch 2945/4975 - Training Loss: 0.5017\n",
            "Batch 2946/4975 - Training Loss: 0.6475\n",
            "Batch 2947/4975 - Training Loss: 0.5485\n",
            "Batch 2948/4975 - Training Loss: 0.8094\n",
            "Batch 2949/4975 - Training Loss: 0.5357\n",
            "Batch 2950/4975 - Training Loss: 0.6706\n",
            "Batch 2951/4975 - Training Loss: 0.5175\n",
            "Batch 2952/4975 - Training Loss: 0.5780\n",
            "Batch 2953/4975 - Training Loss: 0.9143\n",
            "Batch 2954/4975 - Training Loss: 0.7095\n",
            "Batch 2955/4975 - Training Loss: 0.5651\n",
            "Batch 2956/4975 - Training Loss: 0.5814\n",
            "Batch 2957/4975 - Training Loss: 0.7344\n",
            "Batch 2958/4975 - Training Loss: 0.6234\n",
            "Batch 2959/4975 - Training Loss: 0.6205\n",
            "Batch 2960/4975 - Training Loss: 0.4454\n",
            "Batch 2961/4975 - Training Loss: 0.8005\n",
            "Batch 2962/4975 - Training Loss: 0.7420\n",
            "Batch 2963/4975 - Training Loss: 0.6249\n",
            "Batch 2964/4975 - Training Loss: 0.3320\n",
            "Batch 2965/4975 - Training Loss: 0.5083\n",
            "Batch 2966/4975 - Training Loss: 0.7377\n",
            "Batch 2967/4975 - Training Loss: 0.9460\n",
            "Batch 2968/4975 - Training Loss: 0.6134\n",
            "Batch 2969/4975 - Training Loss: 0.6072\n",
            "Batch 2970/4975 - Training Loss: 0.6898\n",
            "Batch 2971/4975 - Training Loss: 0.7372\n",
            "Batch 2972/4975 - Training Loss: 0.5009\n",
            "Batch 2973/4975 - Training Loss: 0.4456\n",
            "Batch 2974/4975 - Training Loss: 0.2725\n",
            "Batch 2975/4975 - Training Loss: 0.5809\n",
            "Batch 2976/4975 - Training Loss: 0.4299\n",
            "Batch 2977/4975 - Training Loss: 0.6834\n",
            "Batch 2978/4975 - Training Loss: 0.5561\n",
            "Batch 2979/4975 - Training Loss: 0.7540\n",
            "Batch 2980/4975 - Training Loss: 0.9119\n",
            "Batch 2981/4975 - Training Loss: 0.5132\n",
            "Batch 2982/4975 - Training Loss: 0.7271\n",
            "Batch 2983/4975 - Training Loss: 0.5800\n",
            "Batch 2984/4975 - Training Loss: 1.1654\n",
            "Batch 2985/4975 - Training Loss: 0.5969\n",
            "Batch 2986/4975 - Training Loss: 0.4929\n",
            "Batch 2987/4975 - Training Loss: 0.7519\n",
            "Batch 2988/4975 - Training Loss: 0.5038\n",
            "Batch 2989/4975 - Training Loss: 0.7538\n",
            "Batch 2990/4975 - Training Loss: 0.3898\n",
            "Batch 2991/4975 - Training Loss: 0.5968\n",
            "Batch 2992/4975 - Training Loss: 0.5261\n",
            "Batch 2993/4975 - Training Loss: 0.6460\n",
            "Batch 2994/4975 - Training Loss: 0.3320\n",
            "Batch 2995/4975 - Training Loss: 0.6813\n",
            "Batch 2996/4975 - Training Loss: 0.5936\n",
            "Batch 2997/4975 - Training Loss: 0.8320\n",
            "Batch 2998/4975 - Training Loss: 0.7802\n",
            "Batch 2999/4975 - Training Loss: 0.7315\n",
            "Batch 3000/4975 - Training Loss: 0.6565\n",
            "Batch 3001/4975 - Training Loss: 0.6029\n",
            "Batch 3002/4975 - Training Loss: 0.8333\n",
            "Batch 3003/4975 - Training Loss: 0.5380\n",
            "Batch 3004/4975 - Training Loss: 0.5761\n",
            "Batch 3005/4975 - Training Loss: 0.4812\n",
            "Batch 3006/4975 - Training Loss: 0.5051\n",
            "Batch 3007/4975 - Training Loss: 0.3056\n",
            "Batch 3008/4975 - Training Loss: 0.7273\n",
            "Batch 3009/4975 - Training Loss: 0.9375\n",
            "Batch 3010/4975 - Training Loss: 0.7425\n",
            "Batch 3011/4975 - Training Loss: 0.5878\n",
            "Batch 3012/4975 - Training Loss: 0.8917\n",
            "Batch 3013/4975 - Training Loss: 0.6308\n",
            "Batch 3014/4975 - Training Loss: 0.8279\n",
            "Batch 3015/4975 - Training Loss: 0.5536\n",
            "Batch 3016/4975 - Training Loss: 0.7287\n",
            "Batch 3017/4975 - Training Loss: 0.4396\n",
            "Batch 3018/4975 - Training Loss: 0.6892\n",
            "Batch 3019/4975 - Training Loss: 0.5070\n",
            "Batch 3020/4975 - Training Loss: 0.5731\n",
            "Batch 3021/4975 - Training Loss: 0.6858\n",
            "Batch 3022/4975 - Training Loss: 0.4962\n",
            "Batch 3023/4975 - Training Loss: 0.5879\n",
            "Batch 3024/4975 - Training Loss: 0.5229\n",
            "Batch 3025/4975 - Training Loss: 0.6013\n",
            "Batch 3026/4975 - Training Loss: 0.4091\n",
            "Batch 3027/4975 - Training Loss: 0.5537\n",
            "Batch 3028/4975 - Training Loss: 0.5939\n",
            "Batch 3029/4975 - Training Loss: 0.5499\n",
            "Batch 3030/4975 - Training Loss: 0.8128\n",
            "Batch 3031/4975 - Training Loss: 0.6734\n",
            "Batch 3032/4975 - Training Loss: 0.7215\n",
            "Batch 3033/4975 - Training Loss: 0.4649\n",
            "Batch 3034/4975 - Training Loss: 0.6738\n",
            "Batch 3035/4975 - Training Loss: 0.6385\n",
            "Batch 3036/4975 - Training Loss: 0.2915\n",
            "Batch 3037/4975 - Training Loss: 0.5931\n",
            "Batch 3038/4975 - Training Loss: 0.4172\n",
            "Batch 3039/4975 - Training Loss: 0.5150\n",
            "Batch 3040/4975 - Training Loss: 0.6030\n",
            "Batch 3041/4975 - Training Loss: 0.6707\n",
            "Batch 3042/4975 - Training Loss: 0.3383\n",
            "Batch 3043/4975 - Training Loss: 0.5738\n",
            "Batch 3044/4975 - Training Loss: 0.6278\n",
            "Batch 3045/4975 - Training Loss: 0.7722\n",
            "Batch 3046/4975 - Training Loss: 0.6811\n",
            "Batch 3047/4975 - Training Loss: 0.8282\n",
            "Batch 3048/4975 - Training Loss: 0.7446\n",
            "Batch 3049/4975 - Training Loss: 0.4853\n",
            "Batch 3050/4975 - Training Loss: 0.4339\n",
            "Batch 3051/4975 - Training Loss: 0.7537\n",
            "Batch 3052/4975 - Training Loss: 0.5344\n",
            "Batch 3053/4975 - Training Loss: 0.8790\n",
            "Batch 3054/4975 - Training Loss: 0.6380\n",
            "Batch 3055/4975 - Training Loss: 0.6619\n",
            "Batch 3056/4975 - Training Loss: 0.7868\n",
            "Batch 3057/4975 - Training Loss: 0.5955\n",
            "Batch 3058/4975 - Training Loss: 0.4963\n",
            "Batch 3059/4975 - Training Loss: 0.8892\n",
            "Batch 3060/4975 - Training Loss: 0.7337\n",
            "Batch 3061/4975 - Training Loss: 0.5053\n",
            "Batch 3062/4975 - Training Loss: 0.7233\n",
            "Batch 3063/4975 - Training Loss: 0.7825\n",
            "Batch 3064/4975 - Training Loss: 0.5089\n",
            "Batch 3065/4975 - Training Loss: 0.5339\n",
            "Batch 3066/4975 - Training Loss: 0.5488\n",
            "Batch 3067/4975 - Training Loss: 0.6116\n",
            "Batch 3068/4975 - Training Loss: 0.4160\n",
            "Batch 3069/4975 - Training Loss: 0.8906\n",
            "Batch 3070/4975 - Training Loss: 0.8332\n",
            "Batch 3071/4975 - Training Loss: 0.5299\n",
            "Batch 3072/4975 - Training Loss: 0.7145\n",
            "Batch 3073/4975 - Training Loss: 0.4322\n",
            "Batch 3074/4975 - Training Loss: 0.5110\n",
            "Batch 3075/4975 - Training Loss: 0.5321\n",
            "Batch 3076/4975 - Training Loss: 0.8084\n",
            "Batch 3077/4975 - Training Loss: 0.5823\n",
            "Batch 3078/4975 - Training Loss: 0.4148\n",
            "Batch 3079/4975 - Training Loss: 0.6043\n",
            "Batch 3080/4975 - Training Loss: 0.4140\n",
            "Batch 3081/4975 - Training Loss: 0.6432\n",
            "Batch 3082/4975 - Training Loss: 0.9035\n",
            "Batch 3083/4975 - Training Loss: 0.8404\n",
            "Batch 3084/4975 - Training Loss: 0.6583\n",
            "Batch 3085/4975 - Training Loss: 0.5149\n",
            "Batch 3086/4975 - Training Loss: 0.6241\n",
            "Batch 3087/4975 - Training Loss: 0.6700\n",
            "Batch 3088/4975 - Training Loss: 0.6785\n",
            "Batch 3089/4975 - Training Loss: 0.6950\n",
            "Batch 3090/4975 - Training Loss: 0.4578\n",
            "Batch 3091/4975 - Training Loss: 0.4897\n",
            "Batch 3092/4975 - Training Loss: 0.5478\n",
            "Batch 3093/4975 - Training Loss: 0.8146\n",
            "Batch 3094/4975 - Training Loss: 0.9140\n",
            "Batch 3095/4975 - Training Loss: 0.7977\n",
            "Batch 3096/4975 - Training Loss: 0.9802\n",
            "Batch 3097/4975 - Training Loss: 0.4341\n",
            "Batch 3098/4975 - Training Loss: 0.4144\n",
            "Batch 3099/4975 - Training Loss: 0.4621\n",
            "Batch 3100/4975 - Training Loss: 0.5957\n",
            "Batch 3101/4975 - Training Loss: 0.3258\n",
            "Batch 3102/4975 - Training Loss: 0.6593\n",
            "Batch 3103/4975 - Training Loss: 1.1581\n",
            "Batch 3104/4975 - Training Loss: 0.6236\n",
            "Batch 3105/4975 - Training Loss: 0.8770\n",
            "Batch 3106/4975 - Training Loss: 0.7699\n",
            "Batch 3107/4975 - Training Loss: 0.7171\n",
            "Batch 3108/4975 - Training Loss: 0.6549\n",
            "Batch 3109/4975 - Training Loss: 0.6406\n",
            "Batch 3110/4975 - Training Loss: 0.5219\n",
            "Batch 3111/4975 - Training Loss: 0.6437\n",
            "Batch 3112/4975 - Training Loss: 0.6495\n",
            "Batch 3113/4975 - Training Loss: 0.4980\n",
            "Batch 3114/4975 - Training Loss: 0.6793\n",
            "Batch 3115/4975 - Training Loss: 0.8592\n",
            "Batch 3116/4975 - Training Loss: 0.6324\n",
            "Batch 3117/4975 - Training Loss: 0.4320\n",
            "Batch 3118/4975 - Training Loss: 0.7811\n",
            "Batch 3119/4975 - Training Loss: 0.9329\n",
            "Batch 3120/4975 - Training Loss: 0.6150\n",
            "Batch 3121/4975 - Training Loss: 0.3157\n",
            "Batch 3122/4975 - Training Loss: 0.6564\n",
            "Batch 3123/4975 - Training Loss: 0.5354\n",
            "Batch 3124/4975 - Training Loss: 1.0027\n",
            "Batch 3125/4975 - Training Loss: 0.8876\n",
            "Batch 3126/4975 - Training Loss: 0.8875\n",
            "Batch 3127/4975 - Training Loss: 0.5687\n",
            "Batch 3128/4975 - Training Loss: 0.7374\n",
            "Batch 3129/4975 - Training Loss: 0.7880\n",
            "Batch 3130/4975 - Training Loss: 0.6330\n",
            "Batch 3131/4975 - Training Loss: 0.3603\n",
            "Batch 3132/4975 - Training Loss: 0.8494\n",
            "Batch 3133/4975 - Training Loss: 0.6858\n",
            "Batch 3134/4975 - Training Loss: 0.7942\n",
            "Batch 3135/4975 - Training Loss: 0.4850\n",
            "Batch 3136/4975 - Training Loss: 0.6501\n",
            "Batch 3137/4975 - Training Loss: 0.4387\n",
            "Batch 3138/4975 - Training Loss: 0.6972\n",
            "Batch 3139/4975 - Training Loss: 0.8118\n",
            "Batch 3140/4975 - Training Loss: 0.6607\n",
            "Batch 3141/4975 - Training Loss: 0.8764\n",
            "Batch 3142/4975 - Training Loss: 1.0633\n",
            "Batch 3143/4975 - Training Loss: 0.6457\n",
            "Batch 3144/4975 - Training Loss: 0.5161\n",
            "Batch 3145/4975 - Training Loss: 0.6400\n",
            "Batch 3146/4975 - Training Loss: 0.4090\n",
            "Batch 3147/4975 - Training Loss: 0.6494\n",
            "Batch 3148/4975 - Training Loss: 0.5077\n",
            "Batch 3149/4975 - Training Loss: 0.5293\n",
            "Batch 3150/4975 - Training Loss: 0.6054\n",
            "Batch 3151/4975 - Training Loss: 0.4409\n",
            "Batch 3152/4975 - Training Loss: 0.7563\n",
            "Batch 3153/4975 - Training Loss: 0.5358\n",
            "Batch 3154/4975 - Training Loss: 0.6168\n",
            "Batch 3155/4975 - Training Loss: 0.8044\n",
            "Batch 3156/4975 - Training Loss: 0.3966\n",
            "Batch 3157/4975 - Training Loss: 0.6800\n",
            "Batch 3158/4975 - Training Loss: 0.6575\n",
            "Batch 3159/4975 - Training Loss: 0.4603\n",
            "Batch 3160/4975 - Training Loss: 0.8203\n",
            "Batch 3161/4975 - Training Loss: 0.5993\n",
            "Batch 3162/4975 - Training Loss: 0.6839\n",
            "Batch 3163/4975 - Training Loss: 0.4991\n",
            "Batch 3164/4975 - Training Loss: 0.6871\n",
            "Batch 3165/4975 - Training Loss: 0.5545\n",
            "Batch 3166/4975 - Training Loss: 0.3979\n",
            "Batch 3167/4975 - Training Loss: 0.3884\n",
            "Batch 3168/4975 - Training Loss: 0.6256\n",
            "Batch 3169/4975 - Training Loss: 0.5596\n",
            "Batch 3170/4975 - Training Loss: 0.7127\n",
            "Batch 3171/4975 - Training Loss: 0.5956\n",
            "Batch 3172/4975 - Training Loss: 0.8970\n",
            "Batch 3173/4975 - Training Loss: 0.5829\n",
            "Batch 3174/4975 - Training Loss: 0.4872\n",
            "Batch 3175/4975 - Training Loss: 0.9757\n",
            "Batch 3176/4975 - Training Loss: 0.7132\n",
            "Batch 3177/4975 - Training Loss: 0.8827\n",
            "Batch 3178/4975 - Training Loss: 0.6104\n",
            "Batch 3179/4975 - Training Loss: 0.6148\n",
            "Batch 3180/4975 - Training Loss: 0.7052\n",
            "Batch 3181/4975 - Training Loss: 0.7353\n",
            "Batch 3182/4975 - Training Loss: 0.5795\n",
            "Batch 3183/4975 - Training Loss: 0.3647\n",
            "Batch 3184/4975 - Training Loss: 0.6061\n",
            "Batch 3185/4975 - Training Loss: 1.0584\n",
            "Batch 3186/4975 - Training Loss: 0.3949\n",
            "Batch 3187/4975 - Training Loss: 0.5237\n",
            "Batch 3188/4975 - Training Loss: 0.5662\n",
            "Batch 3189/4975 - Training Loss: 0.5702\n",
            "Batch 3190/4975 - Training Loss: 0.5343\n",
            "Batch 3191/4975 - Training Loss: 0.6914\n",
            "Batch 3192/4975 - Training Loss: 1.0778\n",
            "Batch 3193/4975 - Training Loss: 0.5535\n",
            "Batch 3194/4975 - Training Loss: 0.8247\n",
            "Batch 3195/4975 - Training Loss: 0.5445\n",
            "Batch 3196/4975 - Training Loss: 0.8727\n",
            "Batch 3197/4975 - Training Loss: 0.8665\n",
            "Batch 3198/4975 - Training Loss: 0.8328\n",
            "Batch 3199/4975 - Training Loss: 1.1367\n",
            "Batch 3200/4975 - Training Loss: 0.7886\n",
            "Batch 3201/4975 - Training Loss: 0.3364\n",
            "Batch 3202/4975 - Training Loss: 0.4452\n",
            "Batch 3203/4975 - Training Loss: 0.6710\n",
            "Batch 3204/4975 - Training Loss: 0.3253\n",
            "Batch 3205/4975 - Training Loss: 0.5885\n",
            "Batch 3206/4975 - Training Loss: 0.5268\n",
            "Batch 3207/4975 - Training Loss: 0.4766\n",
            "Batch 3208/4975 - Training Loss: 0.7974\n",
            "Batch 3209/4975 - Training Loss: 0.5052\n",
            "Batch 3210/4975 - Training Loss: 0.3488\n",
            "Batch 3211/4975 - Training Loss: 0.8956\n",
            "Batch 3212/4975 - Training Loss: 0.7217\n",
            "Batch 3213/4975 - Training Loss: 0.6283\n",
            "Batch 3214/4975 - Training Loss: 0.5302\n",
            "Batch 3215/4975 - Training Loss: 0.7411\n",
            "Batch 3216/4975 - Training Loss: 0.4557\n",
            "Batch 3217/4975 - Training Loss: 0.4581\n",
            "Batch 3218/4975 - Training Loss: 0.7861\n",
            "Batch 3219/4975 - Training Loss: 0.4996\n",
            "Batch 3220/4975 - Training Loss: 0.6060\n",
            "Batch 3221/4975 - Training Loss: 0.3732\n",
            "Batch 3222/4975 - Training Loss: 0.4930\n",
            "Batch 3223/4975 - Training Loss: 0.9705\n",
            "Batch 3224/4975 - Training Loss: 0.7257\n",
            "Batch 3225/4975 - Training Loss: 0.5666\n",
            "Batch 3226/4975 - Training Loss: 0.8058\n",
            "Batch 3227/4975 - Training Loss: 0.3922\n",
            "Batch 3228/4975 - Training Loss: 0.7282\n",
            "Batch 3229/4975 - Training Loss: 0.7344\n",
            "Batch 3230/4975 - Training Loss: 0.2843\n",
            "Batch 3231/4975 - Training Loss: 0.3789\n",
            "Batch 3232/4975 - Training Loss: 0.6209\n",
            "Batch 3233/4975 - Training Loss: 0.4028\n",
            "Batch 3234/4975 - Training Loss: 0.7107\n",
            "Batch 3235/4975 - Training Loss: 0.3747\n",
            "Batch 3236/4975 - Training Loss: 0.3407\n",
            "Batch 3237/4975 - Training Loss: 0.5860\n",
            "Batch 3238/4975 - Training Loss: 0.8751\n",
            "Batch 3239/4975 - Training Loss: 0.6032\n",
            "Batch 3240/4975 - Training Loss: 0.3351\n",
            "Batch 3241/4975 - Training Loss: 0.9061\n",
            "Batch 3242/4975 - Training Loss: 0.7185\n",
            "Batch 3243/4975 - Training Loss: 0.4239\n",
            "Batch 3244/4975 - Training Loss: 0.5607\n",
            "Batch 3245/4975 - Training Loss: 0.7078\n",
            "Batch 3246/4975 - Training Loss: 0.9218\n",
            "Batch 3247/4975 - Training Loss: 0.5297\n",
            "Batch 3248/4975 - Training Loss: 1.1519\n",
            "Batch 3249/4975 - Training Loss: 0.8235\n",
            "Batch 3250/4975 - Training Loss: 0.5985\n",
            "Batch 3251/4975 - Training Loss: 0.6245\n",
            "Batch 3252/4975 - Training Loss: 0.7336\n",
            "Batch 3253/4975 - Training Loss: 0.7345\n",
            "Batch 3254/4975 - Training Loss: 0.7061\n",
            "Batch 3255/4975 - Training Loss: 0.5878\n",
            "Batch 3256/4975 - Training Loss: 0.6996\n",
            "Batch 3257/4975 - Training Loss: 0.4949\n",
            "Batch 3258/4975 - Training Loss: 0.6339\n",
            "Batch 3259/4975 - Training Loss: 0.9230\n",
            "Batch 3260/4975 - Training Loss: 0.6000\n",
            "Batch 3261/4975 - Training Loss: 0.6944\n",
            "Batch 3262/4975 - Training Loss: 0.7775\n",
            "Batch 3263/4975 - Training Loss: 0.5721\n",
            "Batch 3264/4975 - Training Loss: 0.3031\n",
            "Batch 3265/4975 - Training Loss: 0.9433\n",
            "Batch 3266/4975 - Training Loss: 0.8957\n",
            "Batch 3267/4975 - Training Loss: 0.6557\n",
            "Batch 3268/4975 - Training Loss: 0.5864\n",
            "Batch 3269/4975 - Training Loss: 0.6049\n",
            "Batch 3270/4975 - Training Loss: 0.4767\n",
            "Batch 3271/4975 - Training Loss: 0.4225\n",
            "Batch 3272/4975 - Training Loss: 0.3356\n",
            "Batch 3273/4975 - Training Loss: 0.4958\n",
            "Batch 3274/4975 - Training Loss: 0.6176\n",
            "Batch 3275/4975 - Training Loss: 0.8060\n",
            "Batch 3276/4975 - Training Loss: 0.6722\n",
            "Batch 3277/4975 - Training Loss: 0.8160\n",
            "Batch 3278/4975 - Training Loss: 0.3809\n",
            "Batch 3279/4975 - Training Loss: 0.7095\n",
            "Batch 3280/4975 - Training Loss: 0.4866\n",
            "Batch 3281/4975 - Training Loss: 0.5394\n",
            "Batch 3282/4975 - Training Loss: 0.7275\n",
            "Batch 3283/4975 - Training Loss: 1.0780\n",
            "Batch 3284/4975 - Training Loss: 1.0245\n",
            "Batch 3285/4975 - Training Loss: 0.3502\n",
            "Batch 3286/4975 - Training Loss: 0.7123\n",
            "Batch 3287/4975 - Training Loss: 0.6489\n",
            "Batch 3288/4975 - Training Loss: 0.7973\n",
            "Batch 3289/4975 - Training Loss: 0.4606\n",
            "Batch 3290/4975 - Training Loss: 0.5865\n",
            "Batch 3291/4975 - Training Loss: 0.6234\n",
            "Batch 3292/4975 - Training Loss: 0.6348\n",
            "Batch 3293/4975 - Training Loss: 0.6456\n",
            "Batch 3294/4975 - Training Loss: 0.5393\n",
            "Batch 3295/4975 - Training Loss: 0.6902\n",
            "Batch 3296/4975 - Training Loss: 0.7314\n",
            "Batch 3297/4975 - Training Loss: 0.6097\n",
            "Batch 3298/4975 - Training Loss: 0.7118\n",
            "Batch 3299/4975 - Training Loss: 0.9895\n",
            "Batch 3300/4975 - Training Loss: 0.9757\n",
            "Batch 3301/4975 - Training Loss: 0.5177\n",
            "Batch 3302/4975 - Training Loss: 0.5529\n",
            "Batch 3303/4975 - Training Loss: 0.5703\n",
            "Batch 3304/4975 - Training Loss: 0.6538\n",
            "Batch 3305/4975 - Training Loss: 0.6256\n",
            "Batch 3306/4975 - Training Loss: 0.7297\n",
            "Batch 3307/4975 - Training Loss: 0.5571\n",
            "Batch 3308/4975 - Training Loss: 0.6833\n",
            "Batch 3309/4975 - Training Loss: 0.6442\n",
            "Batch 3310/4975 - Training Loss: 0.3680\n",
            "Batch 3311/4975 - Training Loss: 1.1614\n",
            "Batch 3312/4975 - Training Loss: 0.5906\n",
            "Batch 3313/4975 - Training Loss: 0.6468\n",
            "Batch 3314/4975 - Training Loss: 0.5881\n",
            "Batch 3315/4975 - Training Loss: 0.4020\n",
            "Batch 3316/4975 - Training Loss: 0.8754\n",
            "Batch 3317/4975 - Training Loss: 0.4363\n",
            "Batch 3318/4975 - Training Loss: 0.5315\n",
            "Batch 3319/4975 - Training Loss: 0.4160\n",
            "Batch 3320/4975 - Training Loss: 0.3753\n",
            "Batch 3321/4975 - Training Loss: 0.7255\n",
            "Batch 3322/4975 - Training Loss: 1.1134\n",
            "Batch 3323/4975 - Training Loss: 0.6684\n",
            "Batch 3324/4975 - Training Loss: 0.5127\n",
            "Batch 3325/4975 - Training Loss: 0.6942\n",
            "Batch 3326/4975 - Training Loss: 0.6214\n",
            "Batch 3327/4975 - Training Loss: 0.6525\n",
            "Batch 3328/4975 - Training Loss: 0.7987\n",
            "Batch 3329/4975 - Training Loss: 0.5247\n",
            "Batch 3330/4975 - Training Loss: 0.4502\n",
            "Batch 3331/4975 - Training Loss: 0.7555\n",
            "Batch 3332/4975 - Training Loss: 0.6275\n",
            "Batch 3333/4975 - Training Loss: 0.6187\n",
            "Batch 3334/4975 - Training Loss: 0.5933\n",
            "Batch 3335/4975 - Training Loss: 0.8066\n",
            "Batch 3336/4975 - Training Loss: 0.5868\n",
            "Batch 3337/4975 - Training Loss: 0.9499\n",
            "Batch 3338/4975 - Training Loss: 0.4080\n",
            "Batch 3339/4975 - Training Loss: 0.5440\n",
            "Batch 3340/4975 - Training Loss: 0.5257\n",
            "Batch 3341/4975 - Training Loss: 0.6752\n",
            "Batch 3342/4975 - Training Loss: 0.4917\n",
            "Batch 3343/4975 - Training Loss: 0.5081\n",
            "Batch 3344/4975 - Training Loss: 0.7187\n",
            "Batch 3345/4975 - Training Loss: 0.8529\n",
            "Batch 3346/4975 - Training Loss: 0.6045\n",
            "Batch 3347/4975 - Training Loss: 0.5197\n",
            "Batch 3348/4975 - Training Loss: 0.4401\n",
            "Batch 3349/4975 - Training Loss: 0.4080\n",
            "Batch 3350/4975 - Training Loss: 0.5627\n",
            "Batch 3351/4975 - Training Loss: 0.5227\n",
            "Batch 3352/4975 - Training Loss: 0.5657\n",
            "Batch 3353/4975 - Training Loss: 0.9172\n",
            "Batch 3354/4975 - Training Loss: 0.7676\n",
            "Batch 3355/4975 - Training Loss: 0.5987\n",
            "Batch 3356/4975 - Training Loss: 0.5625\n",
            "Batch 3357/4975 - Training Loss: 0.5727\n",
            "Batch 3358/4975 - Training Loss: 0.7677\n",
            "Batch 3359/4975 - Training Loss: 0.5145\n",
            "Batch 3360/4975 - Training Loss: 0.5518\n",
            "Batch 3361/4975 - Training Loss: 0.6422\n",
            "Batch 3362/4975 - Training Loss: 0.7180\n",
            "Batch 3363/4975 - Training Loss: 0.6110\n",
            "Batch 3364/4975 - Training Loss: 0.6515\n",
            "Batch 3365/4975 - Training Loss: 0.6516\n",
            "Batch 3366/4975 - Training Loss: 0.5517\n",
            "Batch 3367/4975 - Training Loss: 0.4979\n",
            "Batch 3368/4975 - Training Loss: 0.7855\n",
            "Batch 3369/4975 - Training Loss: 0.7175\n",
            "Batch 3370/4975 - Training Loss: 0.6153\n",
            "Batch 3371/4975 - Training Loss: 0.4732\n",
            "Batch 3372/4975 - Training Loss: 0.4925\n",
            "Batch 3373/4975 - Training Loss: 0.6156\n",
            "Batch 3374/4975 - Training Loss: 0.8958\n",
            "Batch 3375/4975 - Training Loss: 0.4874\n",
            "Batch 3376/4975 - Training Loss: 0.6707\n",
            "Batch 3377/4975 - Training Loss: 0.4847\n",
            "Batch 3378/4975 - Training Loss: 0.5875\n",
            "Batch 3379/4975 - Training Loss: 0.3138\n",
            "Batch 3380/4975 - Training Loss: 0.6157\n",
            "Batch 3381/4975 - Training Loss: 0.4710\n",
            "Batch 3382/4975 - Training Loss: 1.1327\n",
            "Batch 3383/4975 - Training Loss: 0.6672\n",
            "Batch 3384/4975 - Training Loss: 0.9921\n",
            "Batch 3385/4975 - Training Loss: 0.7799\n",
            "Batch 3386/4975 - Training Loss: 0.4369\n",
            "Batch 3387/4975 - Training Loss: 0.5750\n",
            "Batch 3388/4975 - Training Loss: 0.5032\n",
            "Batch 3389/4975 - Training Loss: 0.4860\n",
            "Batch 3390/4975 - Training Loss: 0.7026\n",
            "Batch 3391/4975 - Training Loss: 0.5969\n",
            "Batch 3392/4975 - Training Loss: 0.5434\n",
            "Batch 3393/4975 - Training Loss: 0.7588\n",
            "Batch 3394/4975 - Training Loss: 0.5824\n",
            "Batch 3395/4975 - Training Loss: 0.4750\n",
            "Batch 3396/4975 - Training Loss: 0.9955\n",
            "Batch 3397/4975 - Training Loss: 0.5500\n",
            "Batch 3398/4975 - Training Loss: 0.9730\n",
            "Batch 3399/4975 - Training Loss: 0.7286\n",
            "Batch 3400/4975 - Training Loss: 0.6193\n",
            "Batch 3401/4975 - Training Loss: 0.6347\n",
            "Batch 3402/4975 - Training Loss: 0.4589\n",
            "Batch 3403/4975 - Training Loss: 0.5446\n",
            "Batch 3404/4975 - Training Loss: 0.6061\n",
            "Batch 3405/4975 - Training Loss: 0.5743\n",
            "Batch 3406/4975 - Training Loss: 0.4994\n",
            "Batch 3407/4975 - Training Loss: 0.8890\n",
            "Batch 3408/4975 - Training Loss: 0.4212\n",
            "Batch 3409/4975 - Training Loss: 0.7600\n",
            "Batch 3410/4975 - Training Loss: 0.3810\n",
            "Batch 3411/4975 - Training Loss: 0.5007\n",
            "Batch 3412/4975 - Training Loss: 0.6740\n",
            "Batch 3413/4975 - Training Loss: 0.7302\n",
            "Batch 3414/4975 - Training Loss: 0.7262\n",
            "Batch 3415/4975 - Training Loss: 0.4719\n",
            "Batch 3416/4975 - Training Loss: 0.6219\n",
            "Batch 3417/4975 - Training Loss: 0.7874\n",
            "Batch 3418/4975 - Training Loss: 0.7181\n",
            "Batch 3419/4975 - Training Loss: 0.6940\n",
            "Batch 3420/4975 - Training Loss: 1.1113\n",
            "Batch 3421/4975 - Training Loss: 0.6044\n",
            "Batch 3422/4975 - Training Loss: 0.9585\n",
            "Batch 3423/4975 - Training Loss: 0.8597\n",
            "Batch 3424/4975 - Training Loss: 0.4538\n",
            "Batch 3425/4975 - Training Loss: 0.4931\n",
            "Batch 3426/4975 - Training Loss: 0.5082\n",
            "Batch 3427/4975 - Training Loss: 0.7216\n",
            "Batch 3428/4975 - Training Loss: 0.6557\n",
            "Batch 3429/4975 - Training Loss: 0.6867\n",
            "Batch 3430/4975 - Training Loss: 0.5287\n",
            "Batch 3431/4975 - Training Loss: 0.8361\n",
            "Batch 3432/4975 - Training Loss: 0.3312\n",
            "Batch 3433/4975 - Training Loss: 0.5853\n",
            "Batch 3434/4975 - Training Loss: 0.6278\n",
            "Batch 3435/4975 - Training Loss: 0.5990\n",
            "Batch 3436/4975 - Training Loss: 0.7523\n",
            "Batch 3437/4975 - Training Loss: 0.7815\n",
            "Batch 3438/4975 - Training Loss: 0.4752\n",
            "Batch 3439/4975 - Training Loss: 0.9626\n",
            "Batch 3440/4975 - Training Loss: 0.6372\n",
            "Batch 3441/4975 - Training Loss: 0.5245\n",
            "Batch 3442/4975 - Training Loss: 0.4902\n",
            "Batch 3443/4975 - Training Loss: 0.7327\n",
            "Batch 3444/4975 - Training Loss: 0.9145\n",
            "Batch 3445/4975 - Training Loss: 0.7423\n",
            "Batch 3446/4975 - Training Loss: 0.5079\n",
            "Batch 3447/4975 - Training Loss: 0.8583\n",
            "Batch 3448/4975 - Training Loss: 0.6341\n",
            "Batch 3449/4975 - Training Loss: 0.8336\n",
            "Batch 3450/4975 - Training Loss: 0.6146\n",
            "Batch 3451/4975 - Training Loss: 0.7531\n",
            "Batch 3452/4975 - Training Loss: 0.5831\n",
            "Batch 3453/4975 - Training Loss: 0.5439\n",
            "Batch 3454/4975 - Training Loss: 0.4131\n",
            "Batch 3455/4975 - Training Loss: 0.6172\n",
            "Batch 3456/4975 - Training Loss: 0.5669\n",
            "Batch 3457/4975 - Training Loss: 0.8340\n",
            "Batch 3458/4975 - Training Loss: 0.4456\n",
            "Batch 3459/4975 - Training Loss: 0.5614\n",
            "Batch 3460/4975 - Training Loss: 0.9652\n",
            "Batch 3461/4975 - Training Loss: 0.6138\n",
            "Batch 3462/4975 - Training Loss: 0.6719\n",
            "Batch 3463/4975 - Training Loss: 0.7581\n",
            "Batch 3464/4975 - Training Loss: 0.4991\n",
            "Batch 3465/4975 - Training Loss: 0.3373\n",
            "Batch 3466/4975 - Training Loss: 0.3104\n",
            "Batch 3467/4975 - Training Loss: 0.5891\n",
            "Batch 3468/4975 - Training Loss: 0.7064\n",
            "Batch 3469/4975 - Training Loss: 0.5298\n",
            "Batch 3470/4975 - Training Loss: 0.4097\n",
            "Batch 3471/4975 - Training Loss: 0.5041\n",
            "Batch 3472/4975 - Training Loss: 0.4319\n",
            "Batch 3473/4975 - Training Loss: 0.5658\n",
            "Batch 3474/4975 - Training Loss: 0.4937\n",
            "Batch 3475/4975 - Training Loss: 0.4872\n",
            "Batch 3476/4975 - Training Loss: 0.6604\n",
            "Batch 3477/4975 - Training Loss: 0.8717\n",
            "Batch 3478/4975 - Training Loss: 0.4655\n",
            "Batch 3479/4975 - Training Loss: 0.7965\n",
            "Batch 3480/4975 - Training Loss: 0.6092\n",
            "Batch 3481/4975 - Training Loss: 0.6202\n",
            "Batch 3482/4975 - Training Loss: 0.6266\n",
            "Batch 3483/4975 - Training Loss: 0.6510\n",
            "Batch 3484/4975 - Training Loss: 0.7468\n",
            "Batch 3485/4975 - Training Loss: 0.8861\n",
            "Batch 3486/4975 - Training Loss: 0.6239\n",
            "Batch 3487/4975 - Training Loss: 0.6647\n",
            "Batch 3488/4975 - Training Loss: 0.4747\n",
            "Batch 3489/4975 - Training Loss: 0.9548\n",
            "Batch 3490/4975 - Training Loss: 0.4596\n",
            "Batch 3491/4975 - Training Loss: 0.6815\n",
            "Batch 3492/4975 - Training Loss: 0.5761\n",
            "Batch 3493/4975 - Training Loss: 0.7582\n",
            "Batch 3494/4975 - Training Loss: 0.4934\n",
            "Batch 3495/4975 - Training Loss: 0.4639\n",
            "Batch 3496/4975 - Training Loss: 0.5464\n",
            "Batch 3497/4975 - Training Loss: 0.7047\n",
            "Batch 3498/4975 - Training Loss: 0.4045\n",
            "Batch 3499/4975 - Training Loss: 0.5612\n",
            "Batch 3500/4975 - Training Loss: 0.4713\n",
            "Batch 3501/4975 - Training Loss: 0.6530\n",
            "Batch 3502/4975 - Training Loss: 0.8242\n",
            "Batch 3503/4975 - Training Loss: 0.5268\n",
            "Batch 3504/4975 - Training Loss: 0.7324\n",
            "Batch 3505/4975 - Training Loss: 0.5054\n",
            "Batch 3506/4975 - Training Loss: 0.8616\n",
            "Batch 3507/4975 - Training Loss: 0.6798\n",
            "Batch 3508/4975 - Training Loss: 0.4640\n",
            "Batch 3509/4975 - Training Loss: 0.3877\n",
            "Batch 3510/4975 - Training Loss: 0.4636\n",
            "Batch 3511/4975 - Training Loss: 0.5103\n",
            "Batch 3512/4975 - Training Loss: 0.4863\n",
            "Batch 3513/4975 - Training Loss: 0.4619\n",
            "Batch 3514/4975 - Training Loss: 0.6858\n",
            "Batch 3515/4975 - Training Loss: 0.6871\n",
            "Batch 3516/4975 - Training Loss: 0.5786\n",
            "Batch 3517/4975 - Training Loss: 0.4666\n",
            "Batch 3518/4975 - Training Loss: 0.9330\n",
            "Batch 3519/4975 - Training Loss: 0.4710\n",
            "Batch 3520/4975 - Training Loss: 0.7401\n",
            "Batch 3521/4975 - Training Loss: 0.5677\n",
            "Batch 3522/4975 - Training Loss: 0.7993\n",
            "Batch 3523/4975 - Training Loss: 0.7815\n",
            "Batch 3524/4975 - Training Loss: 0.6051\n",
            "Batch 3525/4975 - Training Loss: 0.9295\n",
            "Batch 3526/4975 - Training Loss: 0.8280\n",
            "Batch 3527/4975 - Training Loss: 0.4659\n",
            "Batch 3528/4975 - Training Loss: 0.4274\n",
            "Batch 3529/4975 - Training Loss: 0.6006\n",
            "Batch 3530/4975 - Training Loss: 0.7275\n",
            "Batch 3531/4975 - Training Loss: 0.7335\n",
            "Batch 3532/4975 - Training Loss: 0.5618\n",
            "Batch 3533/4975 - Training Loss: 0.6372\n",
            "Batch 3534/4975 - Training Loss: 0.4874\n",
            "Batch 3535/4975 - Training Loss: 0.3208\n",
            "Batch 3536/4975 - Training Loss: 0.6087\n",
            "Batch 3537/4975 - Training Loss: 0.6988\n",
            "Batch 3538/4975 - Training Loss: 0.3758\n",
            "Batch 3539/4975 - Training Loss: 0.6786\n",
            "Batch 3540/4975 - Training Loss: 0.5800\n",
            "Batch 3541/4975 - Training Loss: 0.4574\n",
            "Batch 3542/4975 - Training Loss: 0.5019\n",
            "Batch 3543/4975 - Training Loss: 0.6634\n",
            "Batch 3544/4975 - Training Loss: 0.4885\n",
            "Batch 3545/4975 - Training Loss: 0.6961\n",
            "Batch 3546/4975 - Training Loss: 0.8897\n",
            "Batch 3547/4975 - Training Loss: 0.6455\n",
            "Batch 3548/4975 - Training Loss: 0.3860\n",
            "Batch 3549/4975 - Training Loss: 0.5471\n",
            "Batch 3550/4975 - Training Loss: 0.4939\n",
            "Batch 3551/4975 - Training Loss: 0.6090\n",
            "Batch 3552/4975 - Training Loss: 0.6974\n",
            "Batch 3553/4975 - Training Loss: 0.6566\n",
            "Batch 3554/4975 - Training Loss: 0.9185\n",
            "Batch 3555/4975 - Training Loss: 0.6645\n",
            "Batch 3556/4975 - Training Loss: 0.6797\n",
            "Batch 3557/4975 - Training Loss: 0.5196\n",
            "Batch 3558/4975 - Training Loss: 0.5307\n",
            "Batch 3559/4975 - Training Loss: 0.6318\n",
            "Batch 3560/4975 - Training Loss: 0.8024\n",
            "Batch 3561/4975 - Training Loss: 0.6114\n",
            "Batch 3562/4975 - Training Loss: 0.6557\n",
            "Batch 3563/4975 - Training Loss: 0.6784\n",
            "Batch 3564/4975 - Training Loss: 0.8838\n",
            "Batch 3565/4975 - Training Loss: 0.6909\n",
            "Batch 3566/4975 - Training Loss: 0.8717\n",
            "Batch 3567/4975 - Training Loss: 0.9790\n",
            "Batch 3568/4975 - Training Loss: 0.6469\n",
            "Batch 3569/4975 - Training Loss: 0.7427\n",
            "Batch 3570/4975 - Training Loss: 0.5106\n",
            "Batch 3571/4975 - Training Loss: 0.4734\n",
            "Batch 3572/4975 - Training Loss: 0.8158\n",
            "Batch 3573/4975 - Training Loss: 0.4078\n",
            "Batch 3574/4975 - Training Loss: 0.4870\n",
            "Batch 3575/4975 - Training Loss: 0.6286\n",
            "Batch 3576/4975 - Training Loss: 0.4666\n",
            "Batch 3577/4975 - Training Loss: 0.6551\n",
            "Batch 3578/4975 - Training Loss: 0.6924\n",
            "Batch 3579/4975 - Training Loss: 0.6979\n",
            "Batch 3580/4975 - Training Loss: 0.5569\n",
            "Batch 3581/4975 - Training Loss: 0.4624\n",
            "Batch 3582/4975 - Training Loss: 0.5489\n",
            "Batch 3583/4975 - Training Loss: 0.7987\n",
            "Batch 3584/4975 - Training Loss: 0.5729\n",
            "Batch 3585/4975 - Training Loss: 0.4654\n",
            "Batch 3586/4975 - Training Loss: 0.4368\n",
            "Batch 3587/4975 - Training Loss: 0.6995\n",
            "Batch 3588/4975 - Training Loss: 0.8106\n",
            "Batch 3589/4975 - Training Loss: 0.7570\n",
            "Batch 3590/4975 - Training Loss: 0.7261\n",
            "Batch 3591/4975 - Training Loss: 0.6736\n",
            "Batch 3592/4975 - Training Loss: 0.5056\n",
            "Batch 3593/4975 - Training Loss: 0.7333\n",
            "Batch 3594/4975 - Training Loss: 1.0359\n",
            "Batch 3595/4975 - Training Loss: 1.0049\n",
            "Batch 3596/4975 - Training Loss: 0.7080\n",
            "Batch 3597/4975 - Training Loss: 0.5441\n",
            "Batch 3598/4975 - Training Loss: 1.0785\n",
            "Batch 3599/4975 - Training Loss: 0.6856\n",
            "Batch 3600/4975 - Training Loss: 0.6738\n",
            "Batch 3601/4975 - Training Loss: 0.4382\n",
            "Batch 3602/4975 - Training Loss: 0.6908\n",
            "Batch 3603/4975 - Training Loss: 0.6696\n",
            "Batch 3604/4975 - Training Loss: 0.5674\n",
            "Batch 3605/4975 - Training Loss: 0.6463\n",
            "Batch 3606/4975 - Training Loss: 0.6790\n",
            "Batch 3607/4975 - Training Loss: 0.7472\n",
            "Batch 3608/4975 - Training Loss: 0.7777\n",
            "Batch 3609/4975 - Training Loss: 0.6383\n",
            "Batch 3610/4975 - Training Loss: 0.5250\n",
            "Batch 3611/4975 - Training Loss: 0.6041\n",
            "Batch 3612/4975 - Training Loss: 0.5969\n",
            "Batch 3613/4975 - Training Loss: 0.5171\n",
            "Batch 3614/4975 - Training Loss: 0.5191\n",
            "Batch 3615/4975 - Training Loss: 0.9346\n",
            "Batch 3616/4975 - Training Loss: 0.3806\n",
            "Batch 3617/4975 - Training Loss: 0.5324\n",
            "Batch 3618/4975 - Training Loss: 0.5917\n",
            "Batch 3619/4975 - Training Loss: 0.5032\n",
            "Batch 3620/4975 - Training Loss: 0.4677\n",
            "Batch 3621/4975 - Training Loss: 0.4706\n",
            "Batch 3622/4975 - Training Loss: 0.4397\n",
            "Batch 3623/4975 - Training Loss: 0.3727\n",
            "Batch 3624/4975 - Training Loss: 0.5142\n",
            "Batch 3625/4975 - Training Loss: 0.8451\n",
            "Batch 3626/4975 - Training Loss: 0.8704\n",
            "Batch 3627/4975 - Training Loss: 0.5907\n",
            "Batch 3628/4975 - Training Loss: 0.3636\n",
            "Batch 3629/4975 - Training Loss: 0.4575\n",
            "Batch 3630/4975 - Training Loss: 0.6693\n",
            "Batch 3631/4975 - Training Loss: 0.5480\n",
            "Batch 3632/4975 - Training Loss: 0.9656\n",
            "Batch 3633/4975 - Training Loss: 0.3339\n",
            "Batch 3634/4975 - Training Loss: 0.3860\n",
            "Batch 3635/4975 - Training Loss: 0.4607\n",
            "Batch 3636/4975 - Training Loss: 0.4049\n",
            "Batch 3637/4975 - Training Loss: 0.7974\n",
            "Batch 3638/4975 - Training Loss: 0.6456\n",
            "Batch 3639/4975 - Training Loss: 0.5126\n",
            "Batch 3640/4975 - Training Loss: 0.7238\n",
            "Batch 3641/4975 - Training Loss: 0.6875\n",
            "Batch 3642/4975 - Training Loss: 0.7075\n",
            "Batch 3643/4975 - Training Loss: 0.3670\n",
            "Batch 3644/4975 - Training Loss: 0.5002\n",
            "Batch 3645/4975 - Training Loss: 0.4453\n",
            "Batch 3646/4975 - Training Loss: 0.8211\n",
            "Batch 3647/4975 - Training Loss: 0.5830\n",
            "Batch 3648/4975 - Training Loss: 0.5030\n",
            "Batch 3649/4975 - Training Loss: 0.5293\n",
            "Batch 3650/4975 - Training Loss: 0.8796\n",
            "Batch 3651/4975 - Training Loss: 0.7685\n",
            "Batch 3652/4975 - Training Loss: 0.8933\n",
            "Batch 3653/4975 - Training Loss: 0.6800\n",
            "Batch 3654/4975 - Training Loss: 0.5584\n",
            "Batch 3655/4975 - Training Loss: 0.4628\n",
            "Batch 3656/4975 - Training Loss: 0.5621\n",
            "Batch 3657/4975 - Training Loss: 0.7248\n",
            "Batch 3658/4975 - Training Loss: 0.8071\n",
            "Batch 3659/4975 - Training Loss: 0.4141\n",
            "Batch 3660/4975 - Training Loss: 0.7124\n",
            "Batch 3661/4975 - Training Loss: 0.4972\n",
            "Batch 3662/4975 - Training Loss: 0.4719\n",
            "Batch 3663/4975 - Training Loss: 0.6324\n",
            "Batch 3664/4975 - Training Loss: 0.5620\n",
            "Batch 3665/4975 - Training Loss: 0.3174\n",
            "Batch 3666/4975 - Training Loss: 0.7329\n",
            "Batch 3667/4975 - Training Loss: 0.6742\n",
            "Batch 3668/4975 - Training Loss: 0.4232\n",
            "Batch 3669/4975 - Training Loss: 0.6668\n",
            "Batch 3670/4975 - Training Loss: 0.4076\n",
            "Batch 3671/4975 - Training Loss: 0.6495\n",
            "Batch 3672/4975 - Training Loss: 0.7164\n",
            "Batch 3673/4975 - Training Loss: 0.4086\n",
            "Batch 3674/4975 - Training Loss: 0.7087\n",
            "Batch 3675/4975 - Training Loss: 0.4632\n",
            "Batch 3676/4975 - Training Loss: 0.4865\n",
            "Batch 3677/4975 - Training Loss: 0.4198\n",
            "Batch 3678/4975 - Training Loss: 0.7735\n",
            "Batch 3679/4975 - Training Loss: 0.3977\n",
            "Batch 3680/4975 - Training Loss: 0.7993\n",
            "Batch 3681/4975 - Training Loss: 0.3309\n",
            "Batch 3682/4975 - Training Loss: 0.6890\n",
            "Batch 3683/4975 - Training Loss: 0.7117\n",
            "Batch 3684/4975 - Training Loss: 0.5767\n",
            "Batch 3685/4975 - Training Loss: 0.6637\n",
            "Batch 3686/4975 - Training Loss: 0.5328\n",
            "Batch 3687/4975 - Training Loss: 0.6903\n",
            "Batch 3688/4975 - Training Loss: 0.5607\n",
            "Batch 3689/4975 - Training Loss: 0.5286\n",
            "Batch 3690/4975 - Training Loss: 0.5340\n",
            "Batch 3691/4975 - Training Loss: 0.9055\n",
            "Batch 3692/4975 - Training Loss: 0.7667\n",
            "Batch 3693/4975 - Training Loss: 0.7277\n",
            "Batch 3694/4975 - Training Loss: 0.6438\n",
            "Batch 3695/4975 - Training Loss: 0.7019\n",
            "Batch 3696/4975 - Training Loss: 0.4044\n",
            "Batch 3697/4975 - Training Loss: 0.6000\n",
            "Batch 3698/4975 - Training Loss: 0.4628\n",
            "Batch 3699/4975 - Training Loss: 0.5667\n",
            "Batch 3700/4975 - Training Loss: 1.0720\n",
            "Batch 3701/4975 - Training Loss: 0.5420\n",
            "Batch 3702/4975 - Training Loss: 0.7032\n",
            "Batch 3703/4975 - Training Loss: 0.6015\n",
            "Batch 3704/4975 - Training Loss: 0.5759\n",
            "Batch 3705/4975 - Training Loss: 0.4359\n",
            "Batch 3706/4975 - Training Loss: 0.7672\n",
            "Batch 3707/4975 - Training Loss: 0.7004\n",
            "Batch 3708/4975 - Training Loss: 0.5917\n",
            "Batch 3709/4975 - Training Loss: 0.8194\n",
            "Batch 3710/4975 - Training Loss: 0.9949\n",
            "Batch 3711/4975 - Training Loss: 0.5572\n",
            "Batch 3712/4975 - Training Loss: 0.6167\n",
            "Batch 3713/4975 - Training Loss: 0.7309\n",
            "Batch 3714/4975 - Training Loss: 0.6038\n",
            "Batch 3715/4975 - Training Loss: 0.8047\n",
            "Batch 3716/4975 - Training Loss: 0.6837\n",
            "Batch 3717/4975 - Training Loss: 0.8985\n",
            "Batch 3718/4975 - Training Loss: 0.8952\n",
            "Batch 3719/4975 - Training Loss: 0.6829\n",
            "Batch 3720/4975 - Training Loss: 0.8940\n",
            "Batch 3721/4975 - Training Loss: 0.7788\n",
            "Batch 3722/4975 - Training Loss: 0.4256\n",
            "Batch 3723/4975 - Training Loss: 0.5297\n",
            "Batch 3724/4975 - Training Loss: 0.6490\n",
            "Batch 3725/4975 - Training Loss: 0.6592\n",
            "Batch 3726/4975 - Training Loss: 0.3341\n",
            "Batch 3727/4975 - Training Loss: 0.7936\n",
            "Batch 3728/4975 - Training Loss: 0.8091\n",
            "Batch 3729/4975 - Training Loss: 0.5033\n",
            "Batch 3730/4975 - Training Loss: 0.7533\n",
            "Batch 3731/4975 - Training Loss: 0.6608\n",
            "Batch 3732/4975 - Training Loss: 0.4680\n",
            "Batch 3733/4975 - Training Loss: 0.4872\n",
            "Batch 3734/4975 - Training Loss: 0.4384\n",
            "Batch 3735/4975 - Training Loss: 0.5328\n",
            "Batch 3736/4975 - Training Loss: 0.5839\n",
            "Batch 3737/4975 - Training Loss: 0.5799\n",
            "Batch 3738/4975 - Training Loss: 0.4354\n",
            "Batch 3739/4975 - Training Loss: 0.7794\n",
            "Batch 3740/4975 - Training Loss: 0.7109\n",
            "Batch 3741/4975 - Training Loss: 0.9765\n",
            "Batch 3742/4975 - Training Loss: 0.6407\n",
            "Batch 3743/4975 - Training Loss: 0.3380\n",
            "Batch 3744/4975 - Training Loss: 0.4915\n",
            "Batch 3745/4975 - Training Loss: 0.5598\n",
            "Batch 3746/4975 - Training Loss: 0.4621\n",
            "Batch 3747/4975 - Training Loss: 0.6134\n",
            "Batch 3748/4975 - Training Loss: 0.3524\n",
            "Batch 3749/4975 - Training Loss: 0.4843\n",
            "Batch 3750/4975 - Training Loss: 0.8737\n",
            "Batch 3751/4975 - Training Loss: 0.4392\n",
            "Batch 3752/4975 - Training Loss: 0.6009\n",
            "Batch 3753/4975 - Training Loss: 0.4835\n",
            "Batch 3754/4975 - Training Loss: 0.5810\n",
            "Batch 3755/4975 - Training Loss: 0.6384\n",
            "Batch 3756/4975 - Training Loss: 0.4299\n",
            "Batch 3757/4975 - Training Loss: 0.5045\n",
            "Batch 3758/4975 - Training Loss: 0.5123\n",
            "Batch 3759/4975 - Training Loss: 0.7500\n",
            "Batch 3760/4975 - Training Loss: 0.5963\n",
            "Batch 3761/4975 - Training Loss: 0.5713\n",
            "Batch 3762/4975 - Training Loss: 0.3526\n",
            "Batch 3763/4975 - Training Loss: 0.5215\n",
            "Batch 3764/4975 - Training Loss: 0.6562\n",
            "Batch 3765/4975 - Training Loss: 0.6993\n",
            "Batch 3766/4975 - Training Loss: 0.9115\n",
            "Batch 3767/4975 - Training Loss: 0.6841\n",
            "Batch 3768/4975 - Training Loss: 0.6064\n",
            "Batch 3769/4975 - Training Loss: 0.8698\n",
            "Batch 3770/4975 - Training Loss: 0.5068\n",
            "Batch 3771/4975 - Training Loss: 0.5326\n",
            "Batch 3772/4975 - Training Loss: 0.7430\n",
            "Batch 3773/4975 - Training Loss: 0.5850\n",
            "Batch 3774/4975 - Training Loss: 0.9793\n",
            "Batch 3775/4975 - Training Loss: 0.6723\n",
            "Batch 3776/4975 - Training Loss: 0.6368\n",
            "Batch 3777/4975 - Training Loss: 0.4135\n",
            "Batch 3778/4975 - Training Loss: 0.5873\n",
            "Batch 3779/4975 - Training Loss: 0.6939\n",
            "Batch 3780/4975 - Training Loss: 0.4553\n",
            "Batch 3781/4975 - Training Loss: 0.4259\n",
            "Batch 3782/4975 - Training Loss: 0.6647\n",
            "Batch 3783/4975 - Training Loss: 0.7037\n",
            "Batch 3784/4975 - Training Loss: 0.8264\n",
            "Batch 3785/4975 - Training Loss: 0.4875\n",
            "Batch 3786/4975 - Training Loss: 0.7854\n",
            "Batch 3787/4975 - Training Loss: 0.5377\n",
            "Batch 3788/4975 - Training Loss: 0.5472\n",
            "Batch 3789/4975 - Training Loss: 0.6707\n",
            "Batch 3790/4975 - Training Loss: 0.8577\n",
            "Batch 3791/4975 - Training Loss: 0.3647\n",
            "Batch 3792/4975 - Training Loss: 0.9196\n",
            "Batch 3793/4975 - Training Loss: 0.5605\n",
            "Batch 3794/4975 - Training Loss: 0.7478\n",
            "Batch 3795/4975 - Training Loss: 0.5736\n",
            "Batch 3796/4975 - Training Loss: 0.5864\n",
            "Batch 3797/4975 - Training Loss: 0.9384\n",
            "Batch 3798/4975 - Training Loss: 0.5043\n",
            "Batch 3799/4975 - Training Loss: 0.6480\n",
            "Batch 3800/4975 - Training Loss: 0.7493\n",
            "Batch 3801/4975 - Training Loss: 0.4723\n",
            "Batch 3802/4975 - Training Loss: 0.3997\n",
            "Batch 3803/4975 - Training Loss: 0.5912\n",
            "Batch 3804/4975 - Training Loss: 0.3975\n",
            "Batch 3805/4975 - Training Loss: 0.7669\n",
            "Batch 3806/4975 - Training Loss: 0.5444\n",
            "Batch 3807/4975 - Training Loss: 1.1319\n",
            "Batch 3808/4975 - Training Loss: 0.5961\n",
            "Batch 3809/4975 - Training Loss: 0.7271\n",
            "Batch 3810/4975 - Training Loss: 0.5556\n",
            "Batch 3811/4975 - Training Loss: 0.6502\n",
            "Batch 3812/4975 - Training Loss: 0.5300\n",
            "Batch 3813/4975 - Training Loss: 0.5034\n",
            "Batch 3814/4975 - Training Loss: 0.3944\n",
            "Batch 3815/4975 - Training Loss: 0.6594\n",
            "Batch 3816/4975 - Training Loss: 0.5554\n",
            "Batch 3817/4975 - Training Loss: 0.6487\n",
            "Batch 3818/4975 - Training Loss: 0.5288\n",
            "Batch 3819/4975 - Training Loss: 0.7514\n",
            "Batch 3820/4975 - Training Loss: 0.4180\n",
            "Batch 3821/4975 - Training Loss: 0.5739\n",
            "Batch 3822/4975 - Training Loss: 0.4851\n",
            "Batch 3823/4975 - Training Loss: 0.6311\n",
            "Batch 3824/4975 - Training Loss: 0.6944\n",
            "Batch 3825/4975 - Training Loss: 0.7554\n",
            "Batch 3826/4975 - Training Loss: 0.5238\n",
            "Batch 3827/4975 - Training Loss: 0.8836\n",
            "Batch 3828/4975 - Training Loss: 0.4965\n",
            "Batch 3829/4975 - Training Loss: 0.7854\n",
            "Batch 3830/4975 - Training Loss: 0.7617\n",
            "Batch 3831/4975 - Training Loss: 0.4203\n",
            "Batch 3832/4975 - Training Loss: 1.0367\n",
            "Batch 3833/4975 - Training Loss: 1.2400\n",
            "Batch 3834/4975 - Training Loss: 0.6895\n",
            "Batch 3835/4975 - Training Loss: 0.3077\n",
            "Batch 3836/4975 - Training Loss: 0.4081\n",
            "Batch 3837/4975 - Training Loss: 0.8524\n",
            "Batch 3838/4975 - Training Loss: 0.4636\n",
            "Batch 3839/4975 - Training Loss: 0.4851\n",
            "Batch 3840/4975 - Training Loss: 0.6233\n",
            "Batch 3841/4975 - Training Loss: 0.9834\n",
            "Batch 3842/4975 - Training Loss: 0.8363\n",
            "Batch 3843/4975 - Training Loss: 0.6365\n",
            "Batch 3844/4975 - Training Loss: 0.5407\n",
            "Batch 3845/4975 - Training Loss: 0.7036\n",
            "Batch 3846/4975 - Training Loss: 0.5679\n",
            "Batch 3847/4975 - Training Loss: 0.3107\n",
            "Batch 3848/4975 - Training Loss: 0.3564\n",
            "Batch 3849/4975 - Training Loss: 0.6123\n",
            "Batch 3850/4975 - Training Loss: 0.7138\n",
            "Batch 3851/4975 - Training Loss: 0.5727\n",
            "Batch 3852/4975 - Training Loss: 0.7194\n",
            "Batch 3853/4975 - Training Loss: 0.6598\n",
            "Batch 3854/4975 - Training Loss: 0.7100\n",
            "Batch 3855/4975 - Training Loss: 0.5730\n",
            "Batch 3856/4975 - Training Loss: 0.4670\n",
            "Batch 3857/4975 - Training Loss: 0.5736\n",
            "Batch 3858/4975 - Training Loss: 0.5339\n",
            "Batch 3859/4975 - Training Loss: 0.6548\n",
            "Batch 3860/4975 - Training Loss: 0.8231\n",
            "Batch 3861/4975 - Training Loss: 0.7060\n",
            "Batch 3862/4975 - Training Loss: 0.3745\n",
            "Batch 3863/4975 - Training Loss: 0.4968\n",
            "Batch 3864/4975 - Training Loss: 0.5390\n",
            "Batch 3865/4975 - Training Loss: 0.5707\n",
            "Batch 3866/4975 - Training Loss: 0.4822\n",
            "Batch 3867/4975 - Training Loss: 0.3580\n",
            "Batch 3868/4975 - Training Loss: 0.8178\n",
            "Batch 3869/4975 - Training Loss: 0.7540\n",
            "Batch 3870/4975 - Training Loss: 0.5488\n",
            "Batch 3871/4975 - Training Loss: 0.2707\n",
            "Batch 3872/4975 - Training Loss: 0.5824\n",
            "Batch 3873/4975 - Training Loss: 0.7693\n",
            "Batch 3874/4975 - Training Loss: 0.6161\n",
            "Batch 3875/4975 - Training Loss: 0.5569\n",
            "Batch 3876/4975 - Training Loss: 0.9241\n",
            "Batch 3877/4975 - Training Loss: 0.4407\n",
            "Batch 3878/4975 - Training Loss: 0.4197\n",
            "Batch 3879/4975 - Training Loss: 0.5523\n",
            "Batch 3880/4975 - Training Loss: 0.4362\n",
            "Batch 3881/4975 - Training Loss: 0.5227\n",
            "Batch 3882/4975 - Training Loss: 0.5161\n",
            "Batch 3883/4975 - Training Loss: 0.5164\n",
            "Batch 3884/4975 - Training Loss: 0.5164\n",
            "Batch 3885/4975 - Training Loss: 0.8452\n",
            "Batch 3886/4975 - Training Loss: 0.3689\n",
            "Batch 3887/4975 - Training Loss: 0.5014\n",
            "Batch 3888/4975 - Training Loss: 0.6877\n",
            "Batch 3889/4975 - Training Loss: 0.7908\n",
            "Batch 3890/4975 - Training Loss: 0.4439\n",
            "Batch 3891/4975 - Training Loss: 0.4658\n",
            "Batch 3892/4975 - Training Loss: 0.8097\n",
            "Batch 3893/4975 - Training Loss: 0.7711\n",
            "Batch 3894/4975 - Training Loss: 0.6885\n",
            "Batch 3895/4975 - Training Loss: 0.7772\n",
            "Batch 3896/4975 - Training Loss: 0.6642\n",
            "Batch 3897/4975 - Training Loss: 0.4896\n",
            "Batch 3898/4975 - Training Loss: 0.4635\n",
            "Batch 3899/4975 - Training Loss: 0.5348\n",
            "Batch 3900/4975 - Training Loss: 0.8365\n",
            "Batch 3901/4975 - Training Loss: 0.7329\n",
            "Batch 3902/4975 - Training Loss: 0.4947\n",
            "Batch 3903/4975 - Training Loss: 0.4696\n",
            "Batch 3904/4975 - Training Loss: 0.5120\n",
            "Batch 3905/4975 - Training Loss: 0.3484\n",
            "Batch 3906/4975 - Training Loss: 0.4257\n",
            "Batch 3907/4975 - Training Loss: 0.5719\n",
            "Batch 3908/4975 - Training Loss: 0.6718\n",
            "Batch 3909/4975 - Training Loss: 0.6108\n",
            "Batch 3910/4975 - Training Loss: 0.4459\n",
            "Batch 3911/4975 - Training Loss: 0.4051\n",
            "Batch 3912/4975 - Training Loss: 0.5002\n",
            "Batch 3913/4975 - Training Loss: 0.7639\n",
            "Batch 3914/4975 - Training Loss: 0.7499\n",
            "Batch 3915/4975 - Training Loss: 0.6487\n",
            "Batch 3916/4975 - Training Loss: 0.8683\n",
            "Batch 3917/4975 - Training Loss: 0.4121\n",
            "Batch 3918/4975 - Training Loss: 0.7294\n",
            "Batch 3919/4975 - Training Loss: 0.7752\n",
            "Batch 3920/4975 - Training Loss: 0.6997\n",
            "Batch 3921/4975 - Training Loss: 0.3810\n",
            "Batch 3922/4975 - Training Loss: 0.4803\n",
            "Batch 3923/4975 - Training Loss: 0.4859\n",
            "Batch 3924/4975 - Training Loss: 0.5009\n",
            "Batch 3925/4975 - Training Loss: 0.7741\n",
            "Batch 3926/4975 - Training Loss: 0.6718\n",
            "Batch 3927/4975 - Training Loss: 0.7782\n",
            "Batch 3928/4975 - Training Loss: 0.6774\n",
            "Batch 3929/4975 - Training Loss: 0.3583\n",
            "Batch 3930/4975 - Training Loss: 0.9752\n",
            "Batch 3931/4975 - Training Loss: 0.6511\n",
            "Batch 3932/4975 - Training Loss: 1.0208\n",
            "Batch 3933/4975 - Training Loss: 0.5763\n",
            "Batch 3934/4975 - Training Loss: 0.3034\n",
            "Batch 3935/4975 - Training Loss: 0.5891\n",
            "Batch 3936/4975 - Training Loss: 0.4313\n",
            "Batch 3937/4975 - Training Loss: 0.6027\n",
            "Batch 3938/4975 - Training Loss: 0.6450\n",
            "Batch 3939/4975 - Training Loss: 0.6527\n",
            "Batch 3940/4975 - Training Loss: 0.6318\n",
            "Batch 3941/4975 - Training Loss: 0.5652\n",
            "Batch 3942/4975 - Training Loss: 0.9752\n",
            "Batch 3943/4975 - Training Loss: 0.6552\n",
            "Batch 3944/4975 - Training Loss: 0.6736\n",
            "Batch 3945/4975 - Training Loss: 0.4697\n",
            "Batch 3946/4975 - Training Loss: 0.6560\n",
            "Batch 3947/4975 - Training Loss: 0.7461\n",
            "Batch 3948/4975 - Training Loss: 0.6982\n",
            "Batch 3949/4975 - Training Loss: 0.7339\n",
            "Batch 3950/4975 - Training Loss: 0.7543\n",
            "Batch 3951/4975 - Training Loss: 0.9091\n",
            "Batch 3952/4975 - Training Loss: 0.3858\n",
            "Batch 3953/4975 - Training Loss: 1.0644\n",
            "Batch 3954/4975 - Training Loss: 1.0028\n",
            "Batch 3955/4975 - Training Loss: 0.5254\n",
            "Batch 3956/4975 - Training Loss: 0.4847\n",
            "Batch 3957/4975 - Training Loss: 0.6038\n",
            "Batch 3958/4975 - Training Loss: 0.6785\n",
            "Batch 3959/4975 - Training Loss: 0.8634\n",
            "Batch 3960/4975 - Training Loss: 0.6061\n",
            "Batch 3961/4975 - Training Loss: 0.5451\n",
            "Batch 3962/4975 - Training Loss: 0.5948\n",
            "Batch 3963/4975 - Training Loss: 0.5539\n",
            "Batch 3964/4975 - Training Loss: 0.7017\n",
            "Batch 3965/4975 - Training Loss: 0.6721\n",
            "Batch 3966/4975 - Training Loss: 0.4264\n",
            "Batch 3967/4975 - Training Loss: 0.4550\n",
            "Batch 3968/4975 - Training Loss: 0.5234\n",
            "Batch 3969/4975 - Training Loss: 0.5424\n",
            "Batch 3970/4975 - Training Loss: 0.8306\n",
            "Batch 3971/4975 - Training Loss: 0.4276\n",
            "Batch 3972/4975 - Training Loss: 0.8157\n",
            "Batch 3973/4975 - Training Loss: 0.6900\n",
            "Batch 3974/4975 - Training Loss: 0.7735\n",
            "Batch 3975/4975 - Training Loss: 0.5384\n",
            "Batch 3976/4975 - Training Loss: 0.8163\n",
            "Batch 3977/4975 - Training Loss: 0.5042\n",
            "Batch 3978/4975 - Training Loss: 0.6300\n",
            "Batch 3979/4975 - Training Loss: 0.5599\n",
            "Batch 3980/4975 - Training Loss: 0.4941\n",
            "Batch 3981/4975 - Training Loss: 0.7549\n",
            "Batch 3982/4975 - Training Loss: 0.5566\n",
            "Batch 3983/4975 - Training Loss: 0.6017\n",
            "Batch 3984/4975 - Training Loss: 0.6533\n",
            "Batch 3985/4975 - Training Loss: 0.5199\n",
            "Batch 3986/4975 - Training Loss: 0.7793\n",
            "Batch 3987/4975 - Training Loss: 0.4523\n",
            "Batch 3988/4975 - Training Loss: 1.0858\n",
            "Batch 3989/4975 - Training Loss: 0.4526\n",
            "Batch 3990/4975 - Training Loss: 0.5061\n",
            "Batch 3991/4975 - Training Loss: 0.8482\n",
            "Batch 3992/4975 - Training Loss: 0.5987\n",
            "Batch 3993/4975 - Training Loss: 0.6770\n",
            "Batch 3994/4975 - Training Loss: 0.6103\n",
            "Batch 3995/4975 - Training Loss: 0.6463\n",
            "Batch 3996/4975 - Training Loss: 0.6066\n",
            "Batch 3997/4975 - Training Loss: 0.4851\n",
            "Batch 3998/4975 - Training Loss: 0.8153\n",
            "Batch 3999/4975 - Training Loss: 0.8325\n",
            "Batch 4000/4975 - Training Loss: 0.6113\n",
            "Batch 4001/4975 - Training Loss: 0.6072\n",
            "Batch 4002/4975 - Training Loss: 0.6936\n",
            "Batch 4003/4975 - Training Loss: 0.6098\n",
            "Batch 4004/4975 - Training Loss: 0.8783\n",
            "Batch 4005/4975 - Training Loss: 0.5694\n",
            "Batch 4006/4975 - Training Loss: 0.5206\n",
            "Batch 4007/4975 - Training Loss: 0.3851\n",
            "Batch 4008/4975 - Training Loss: 0.6604\n",
            "Batch 4009/4975 - Training Loss: 0.6558\n",
            "Batch 4010/4975 - Training Loss: 0.7562\n",
            "Batch 4011/4975 - Training Loss: 0.8931\n",
            "Batch 4012/4975 - Training Loss: 0.8371\n",
            "Batch 4013/4975 - Training Loss: 0.5673\n",
            "Batch 4014/4975 - Training Loss: 0.7699\n",
            "Batch 4015/4975 - Training Loss: 0.7611\n",
            "Batch 4016/4975 - Training Loss: 0.5578\n",
            "Batch 4017/4975 - Training Loss: 0.5126\n",
            "Batch 4018/4975 - Training Loss: 1.0130\n",
            "Batch 4019/4975 - Training Loss: 0.5581\n",
            "Batch 4020/4975 - Training Loss: 0.6276\n",
            "Batch 4021/4975 - Training Loss: 0.6718\n",
            "Batch 4022/4975 - Training Loss: 0.6178\n",
            "Batch 4023/4975 - Training Loss: 0.4478\n",
            "Batch 4024/4975 - Training Loss: 0.5411\n",
            "Batch 4025/4975 - Training Loss: 0.5627\n",
            "Batch 4026/4975 - Training Loss: 0.9143\n",
            "Batch 4027/4975 - Training Loss: 0.5597\n",
            "Batch 4028/4975 - Training Loss: 1.0132\n",
            "Batch 4029/4975 - Training Loss: 0.6991\n",
            "Batch 4030/4975 - Training Loss: 0.5131\n",
            "Batch 4031/4975 - Training Loss: 0.5077\n",
            "Batch 4032/4975 - Training Loss: 0.7656\n",
            "Batch 4033/4975 - Training Loss: 0.7680\n",
            "Batch 4034/4975 - Training Loss: 0.5435\n",
            "Batch 4035/4975 - Training Loss: 0.4768\n",
            "Batch 4036/4975 - Training Loss: 0.6057\n",
            "Batch 4037/4975 - Training Loss: 0.6301\n",
            "Batch 4038/4975 - Training Loss: 0.4845\n",
            "Batch 4039/4975 - Training Loss: 0.4751\n",
            "Batch 4040/4975 - Training Loss: 0.7068\n",
            "Batch 4041/4975 - Training Loss: 0.7062\n",
            "Batch 4042/4975 - Training Loss: 0.6819\n",
            "Batch 4043/4975 - Training Loss: 0.6153\n",
            "Batch 4044/4975 - Training Loss: 0.6753\n",
            "Batch 4045/4975 - Training Loss: 0.6078\n",
            "Batch 4046/4975 - Training Loss: 0.5727\n",
            "Batch 4047/4975 - Training Loss: 0.5632\n",
            "Batch 4048/4975 - Training Loss: 0.7364\n",
            "Batch 4049/4975 - Training Loss: 0.7566\n",
            "Batch 4050/4975 - Training Loss: 0.4250\n",
            "Batch 4051/4975 - Training Loss: 0.6520\n",
            "Batch 4052/4975 - Training Loss: 0.9204\n",
            "Batch 4053/4975 - Training Loss: 0.6123\n",
            "Batch 4054/4975 - Training Loss: 0.4272\n",
            "Batch 4055/4975 - Training Loss: 0.5798\n",
            "Batch 4056/4975 - Training Loss: 0.8837\n",
            "Batch 4057/4975 - Training Loss: 0.4747\n",
            "Batch 4058/4975 - Training Loss: 0.7854\n",
            "Batch 4059/4975 - Training Loss: 0.6580\n",
            "Batch 4060/4975 - Training Loss: 0.6371\n",
            "Batch 4061/4975 - Training Loss: 0.6329\n",
            "Batch 4062/4975 - Training Loss: 0.5093\n",
            "Batch 4063/4975 - Training Loss: 0.6676\n",
            "Batch 4064/4975 - Training Loss: 0.6443\n",
            "Batch 4065/4975 - Training Loss: 0.9379\n",
            "Batch 4066/4975 - Training Loss: 0.4811\n",
            "Batch 4067/4975 - Training Loss: 0.5615\n",
            "Batch 4068/4975 - Training Loss: 0.4681\n",
            "Batch 4069/4975 - Training Loss: 0.5083\n",
            "Batch 4070/4975 - Training Loss: 0.4438\n",
            "Batch 4071/4975 - Training Loss: 0.7698\n",
            "Batch 4072/4975 - Training Loss: 0.5610\n",
            "Batch 4073/4975 - Training Loss: 0.4604\n",
            "Batch 4074/4975 - Training Loss: 0.8344\n",
            "Batch 4075/4975 - Training Loss: 1.0758\n",
            "Batch 4076/4975 - Training Loss: 0.6858\n",
            "Batch 4077/4975 - Training Loss: 0.7666\n",
            "Batch 4078/4975 - Training Loss: 0.7249\n",
            "Batch 4079/4975 - Training Loss: 0.3749\n",
            "Batch 4080/4975 - Training Loss: 1.0720\n",
            "Batch 4081/4975 - Training Loss: 0.6751\n",
            "Batch 4082/4975 - Training Loss: 0.7764\n",
            "Batch 4083/4975 - Training Loss: 0.8276\n",
            "Batch 4084/4975 - Training Loss: 1.0275\n",
            "Batch 4085/4975 - Training Loss: 0.6247\n",
            "Batch 4086/4975 - Training Loss: 0.5711\n",
            "Batch 4087/4975 - Training Loss: 0.5789\n",
            "Batch 4088/4975 - Training Loss: 0.6516\n",
            "Batch 4089/4975 - Training Loss: 0.9217\n",
            "Batch 4090/4975 - Training Loss: 0.4899\n",
            "Batch 4091/4975 - Training Loss: 0.6413\n",
            "Batch 4092/4975 - Training Loss: 0.6466\n",
            "Batch 4093/4975 - Training Loss: 0.5865\n",
            "Batch 4094/4975 - Training Loss: 0.7120\n",
            "Batch 4095/4975 - Training Loss: 0.4018\n",
            "Batch 4096/4975 - Training Loss: 0.5203\n",
            "Batch 4097/4975 - Training Loss: 1.0754\n",
            "Batch 4098/4975 - Training Loss: 0.6773\n",
            "Batch 4099/4975 - Training Loss: 0.5898\n",
            "Batch 4100/4975 - Training Loss: 0.5749\n",
            "Batch 4101/4975 - Training Loss: 0.6827\n",
            "Batch 4102/4975 - Training Loss: 0.8455\n",
            "Batch 4103/4975 - Training Loss: 0.7702\n",
            "Batch 4104/4975 - Training Loss: 0.6685\n",
            "Batch 4105/4975 - Training Loss: 0.5986\n",
            "Batch 4106/4975 - Training Loss: 0.5633\n",
            "Batch 4107/4975 - Training Loss: 0.6969\n",
            "Batch 4108/4975 - Training Loss: 0.5123\n",
            "Batch 4109/4975 - Training Loss: 0.5573\n",
            "Batch 4110/4975 - Training Loss: 0.4287\n",
            "Batch 4111/4975 - Training Loss: 0.7090\n",
            "Batch 4112/4975 - Training Loss: 0.5381\n",
            "Batch 4113/4975 - Training Loss: 0.8837\n",
            "Batch 4114/4975 - Training Loss: 0.5045\n",
            "Batch 4115/4975 - Training Loss: 0.5811\n",
            "Batch 4116/4975 - Training Loss: 0.6781\n",
            "Batch 4117/4975 - Training Loss: 0.9578\n",
            "Batch 4118/4975 - Training Loss: 0.5868\n",
            "Batch 4119/4975 - Training Loss: 0.7858\n",
            "Batch 4120/4975 - Training Loss: 0.7992\n",
            "Batch 4121/4975 - Training Loss: 0.4944\n",
            "Batch 4122/4975 - Training Loss: 0.6407\n",
            "Batch 4123/4975 - Training Loss: 0.8207\n",
            "Batch 4124/4975 - Training Loss: 0.5504\n",
            "Batch 4125/4975 - Training Loss: 0.8885\n",
            "Batch 4126/4975 - Training Loss: 0.4218\n",
            "Batch 4127/4975 - Training Loss: 0.2771\n",
            "Batch 4128/4975 - Training Loss: 0.8898\n",
            "Batch 4129/4975 - Training Loss: 0.6415\n",
            "Batch 4130/4975 - Training Loss: 0.6709\n",
            "Batch 4131/4975 - Training Loss: 0.5472\n",
            "Batch 4132/4975 - Training Loss: 0.6054\n",
            "Batch 4133/4975 - Training Loss: 0.4710\n",
            "Batch 4134/4975 - Training Loss: 0.7181\n",
            "Batch 4135/4975 - Training Loss: 0.5854\n",
            "Batch 4136/4975 - Training Loss: 0.7435\n",
            "Batch 4137/4975 - Training Loss: 0.4928\n",
            "Batch 4138/4975 - Training Loss: 0.5949\n",
            "Batch 4139/4975 - Training Loss: 0.6334\n",
            "Batch 4140/4975 - Training Loss: 0.4124\n",
            "Batch 4141/4975 - Training Loss: 1.2182\n",
            "Batch 4142/4975 - Training Loss: 0.5610\n",
            "Batch 4143/4975 - Training Loss: 0.3512\n",
            "Batch 4144/4975 - Training Loss: 0.8297\n",
            "Batch 4145/4975 - Training Loss: 0.5578\n",
            "Batch 4146/4975 - Training Loss: 0.9105\n",
            "Batch 4147/4975 - Training Loss: 0.5779\n",
            "Batch 4148/4975 - Training Loss: 0.5299\n",
            "Batch 4149/4975 - Training Loss: 0.2860\n",
            "Batch 4150/4975 - Training Loss: 0.6765\n",
            "Batch 4151/4975 - Training Loss: 0.3871\n",
            "Batch 4152/4975 - Training Loss: 0.8127\n",
            "Batch 4153/4975 - Training Loss: 0.5869\n",
            "Batch 4154/4975 - Training Loss: 0.5210\n",
            "Batch 4155/4975 - Training Loss: 0.6892\n",
            "Batch 4156/4975 - Training Loss: 0.8295\n",
            "Batch 4157/4975 - Training Loss: 0.7871\n",
            "Batch 4158/4975 - Training Loss: 0.7277\n",
            "Batch 4159/4975 - Training Loss: 0.6320\n",
            "Batch 4160/4975 - Training Loss: 0.4432\n",
            "Batch 4161/4975 - Training Loss: 0.6129\n",
            "Batch 4162/4975 - Training Loss: 0.5144\n",
            "Batch 4163/4975 - Training Loss: 0.4294\n",
            "Batch 4164/4975 - Training Loss: 0.5300\n",
            "Batch 4165/4975 - Training Loss: 0.8832\n",
            "Batch 4166/4975 - Training Loss: 0.5008\n",
            "Batch 4167/4975 - Training Loss: 0.5734\n",
            "Batch 4168/4975 - Training Loss: 0.5266\n",
            "Batch 4169/4975 - Training Loss: 0.7826\n",
            "Batch 4170/4975 - Training Loss: 0.6479\n",
            "Batch 4171/4975 - Training Loss: 0.7778\n",
            "Batch 4172/4975 - Training Loss: 0.7010\n",
            "Batch 4173/4975 - Training Loss: 0.5183\n",
            "Batch 4174/4975 - Training Loss: 0.7855\n",
            "Batch 4175/4975 - Training Loss: 0.6385\n",
            "Batch 4176/4975 - Training Loss: 0.6306\n",
            "Batch 4177/4975 - Training Loss: 0.6093\n",
            "Batch 4178/4975 - Training Loss: 0.4985\n",
            "Batch 4179/4975 - Training Loss: 0.5991\n",
            "Batch 4180/4975 - Training Loss: 0.8995\n",
            "Batch 4181/4975 - Training Loss: 0.5005\n",
            "Batch 4182/4975 - Training Loss: 0.4847\n",
            "Batch 4183/4975 - Training Loss: 0.7842\n",
            "Batch 4184/4975 - Training Loss: 0.6062\n",
            "Batch 4185/4975 - Training Loss: 0.7870\n",
            "Batch 4186/4975 - Training Loss: 0.5456\n",
            "Batch 4187/4975 - Training Loss: 0.5826\n",
            "Batch 4188/4975 - Training Loss: 0.4953\n",
            "Batch 4189/4975 - Training Loss: 0.9374\n",
            "Batch 4190/4975 - Training Loss: 0.3890\n",
            "Batch 4191/4975 - Training Loss: 0.7492\n",
            "Batch 4192/4975 - Training Loss: 0.9224\n",
            "Batch 4193/4975 - Training Loss: 0.7065\n",
            "Batch 4194/4975 - Training Loss: 0.8745\n",
            "Batch 4195/4975 - Training Loss: 0.5744\n",
            "Batch 4196/4975 - Training Loss: 0.3769\n",
            "Batch 4197/4975 - Training Loss: 0.7439\n",
            "Batch 4198/4975 - Training Loss: 0.5662\n",
            "Batch 4199/4975 - Training Loss: 0.8572\n",
            "Batch 4200/4975 - Training Loss: 0.7261\n",
            "Batch 4201/4975 - Training Loss: 0.5718\n",
            "Batch 4202/4975 - Training Loss: 0.3026\n",
            "Batch 4203/4975 - Training Loss: 0.5720\n",
            "Batch 4204/4975 - Training Loss: 0.6782\n",
            "Batch 4205/4975 - Training Loss: 0.7164\n",
            "Batch 4206/4975 - Training Loss: 0.3748\n",
            "Batch 4207/4975 - Training Loss: 1.0520\n",
            "Batch 4208/4975 - Training Loss: 0.5905\n",
            "Batch 4209/4975 - Training Loss: 0.6311\n",
            "Batch 4210/4975 - Training Loss: 0.6510\n",
            "Batch 4211/4975 - Training Loss: 0.4216\n",
            "Batch 4212/4975 - Training Loss: 0.4119\n",
            "Batch 4213/4975 - Training Loss: 0.9101\n",
            "Batch 4214/4975 - Training Loss: 0.4555\n",
            "Batch 4215/4975 - Training Loss: 0.7815\n",
            "Batch 4216/4975 - Training Loss: 0.6192\n",
            "Batch 4217/4975 - Training Loss: 0.9881\n",
            "Batch 4218/4975 - Training Loss: 0.5248\n",
            "Batch 4219/4975 - Training Loss: 0.4176\n",
            "Batch 4220/4975 - Training Loss: 0.9944\n",
            "Batch 4221/4975 - Training Loss: 0.6196\n",
            "Batch 4222/4975 - Training Loss: 0.8766\n",
            "Batch 4223/4975 - Training Loss: 0.7140\n",
            "Batch 4224/4975 - Training Loss: 0.6413\n",
            "Batch 4225/4975 - Training Loss: 0.7084\n",
            "Batch 4226/4975 - Training Loss: 0.4163\n",
            "Batch 4227/4975 - Training Loss: 0.5266\n",
            "Batch 4228/4975 - Training Loss: 0.3876\n",
            "Batch 4229/4975 - Training Loss: 0.6024\n",
            "Batch 4230/4975 - Training Loss: 0.4839\n",
            "Batch 4231/4975 - Training Loss: 0.7867\n",
            "Batch 4232/4975 - Training Loss: 0.3386\n",
            "Batch 4233/4975 - Training Loss: 0.7750\n",
            "Batch 4234/4975 - Training Loss: 0.4483\n",
            "Batch 4235/4975 - Training Loss: 0.4948\n",
            "Batch 4236/4975 - Training Loss: 0.7099\n",
            "Batch 4237/4975 - Training Loss: 0.5407\n",
            "Batch 4238/4975 - Training Loss: 0.5081\n",
            "Batch 4239/4975 - Training Loss: 0.8767\n",
            "Batch 4240/4975 - Training Loss: 0.4346\n",
            "Batch 4241/4975 - Training Loss: 0.4944\n",
            "Batch 4242/4975 - Training Loss: 0.7713\n",
            "Batch 4243/4975 - Training Loss: 0.6155\n",
            "Batch 4244/4975 - Training Loss: 0.7173\n",
            "Batch 4245/4975 - Training Loss: 0.6294\n",
            "Batch 4246/4975 - Training Loss: 0.5113\n",
            "Batch 4247/4975 - Training Loss: 0.6480\n",
            "Batch 4248/4975 - Training Loss: 0.8155\n",
            "Batch 4249/4975 - Training Loss: 0.6869\n",
            "Batch 4250/4975 - Training Loss: 0.8638\n",
            "Batch 4251/4975 - Training Loss: 0.4683\n",
            "Batch 4252/4975 - Training Loss: 0.3576\n",
            "Batch 4253/4975 - Training Loss: 0.5422\n",
            "Batch 4254/4975 - Training Loss: 0.8049\n",
            "Batch 4255/4975 - Training Loss: 0.6983\n",
            "Batch 4256/4975 - Training Loss: 0.8583\n",
            "Batch 4257/4975 - Training Loss: 0.5033\n",
            "Batch 4258/4975 - Training Loss: 0.3612\n",
            "Batch 4259/4975 - Training Loss: 0.9516\n",
            "Batch 4260/4975 - Training Loss: 0.5372\n",
            "Batch 4261/4975 - Training Loss: 0.6261\n",
            "Batch 4262/4975 - Training Loss: 0.6357\n",
            "Batch 4263/4975 - Training Loss: 0.3897\n",
            "Batch 4264/4975 - Training Loss: 0.5164\n",
            "Batch 4265/4975 - Training Loss: 0.5993\n",
            "Batch 4266/4975 - Training Loss: 0.5015\n",
            "Batch 4267/4975 - Training Loss: 0.5786\n",
            "Batch 4268/4975 - Training Loss: 0.3691\n",
            "Batch 4269/4975 - Training Loss: 0.7179\n",
            "Batch 4270/4975 - Training Loss: 0.4341\n",
            "Batch 4271/4975 - Training Loss: 0.7759\n",
            "Batch 4272/4975 - Training Loss: 0.5985\n",
            "Batch 4273/4975 - Training Loss: 0.4019\n",
            "Batch 4274/4975 - Training Loss: 0.6940\n",
            "Batch 4275/4975 - Training Loss: 0.6632\n",
            "Batch 4276/4975 - Training Loss: 0.7690\n",
            "Batch 4277/4975 - Training Loss: 0.7576\n",
            "Batch 4278/4975 - Training Loss: 0.8667\n",
            "Batch 4279/4975 - Training Loss: 1.0943\n",
            "Batch 4280/4975 - Training Loss: 0.6580\n",
            "Batch 4281/4975 - Training Loss: 0.7315\n",
            "Batch 4282/4975 - Training Loss: 0.5278\n",
            "Batch 4283/4975 - Training Loss: 0.7971\n",
            "Batch 4284/4975 - Training Loss: 0.9448\n",
            "Batch 4285/4975 - Training Loss: 0.5888\n",
            "Batch 4286/4975 - Training Loss: 0.7786\n",
            "Batch 4287/4975 - Training Loss: 0.6140\n",
            "Batch 4288/4975 - Training Loss: 0.6558\n",
            "Batch 4289/4975 - Training Loss: 0.4387\n",
            "Batch 4290/4975 - Training Loss: 0.5484\n",
            "Batch 4291/4975 - Training Loss: 0.4434\n",
            "Batch 4292/4975 - Training Loss: 0.6369\n",
            "Batch 4293/4975 - Training Loss: 0.8474\n",
            "Batch 4294/4975 - Training Loss: 0.7122\n",
            "Batch 4295/4975 - Training Loss: 0.5768\n",
            "Batch 4296/4975 - Training Loss: 0.7874\n",
            "Batch 4297/4975 - Training Loss: 0.7133\n",
            "Batch 4298/4975 - Training Loss: 0.4183\n",
            "Batch 4299/4975 - Training Loss: 0.5600\n",
            "Batch 4300/4975 - Training Loss: 0.3697\n",
            "Batch 4301/4975 - Training Loss: 0.5053\n",
            "Batch 4302/4975 - Training Loss: 0.6655\n",
            "Batch 4303/4975 - Training Loss: 0.7873\n",
            "Batch 4304/4975 - Training Loss: 0.3422\n",
            "Batch 4305/4975 - Training Loss: 0.5255\n",
            "Batch 4306/4975 - Training Loss: 0.8103\n",
            "Batch 4307/4975 - Training Loss: 0.5665\n",
            "Batch 4308/4975 - Training Loss: 0.4966\n",
            "Batch 4309/4975 - Training Loss: 0.4636\n",
            "Batch 4310/4975 - Training Loss: 0.6443\n",
            "Batch 4311/4975 - Training Loss: 0.3466\n",
            "Batch 4312/4975 - Training Loss: 0.4772\n",
            "Batch 4313/4975 - Training Loss: 0.6777\n",
            "Batch 4314/4975 - Training Loss: 0.5424\n",
            "Batch 4315/4975 - Training Loss: 0.6988\n",
            "Batch 4316/4975 - Training Loss: 0.6550\n",
            "Batch 4317/4975 - Training Loss: 0.8584\n",
            "Batch 4318/4975 - Training Loss: 0.4681\n",
            "Batch 4319/4975 - Training Loss: 0.6695\n",
            "Batch 4320/4975 - Training Loss: 0.8179\n",
            "Batch 4321/4975 - Training Loss: 0.7888\n",
            "Batch 4322/4975 - Training Loss: 0.6450\n",
            "Batch 4323/4975 - Training Loss: 0.3941\n",
            "Batch 4324/4975 - Training Loss: 0.5507\n",
            "Batch 4325/4975 - Training Loss: 0.7068\n",
            "Batch 4326/4975 - Training Loss: 0.3634\n",
            "Batch 4327/4975 - Training Loss: 0.4752\n",
            "Batch 4328/4975 - Training Loss: 0.5755\n",
            "Batch 4329/4975 - Training Loss: 0.3704\n",
            "Batch 4330/4975 - Training Loss: 0.5948\n",
            "Batch 4331/4975 - Training Loss: 0.4483\n",
            "Batch 4332/4975 - Training Loss: 0.5064\n",
            "Batch 4333/4975 - Training Loss: 0.7318\n",
            "Batch 4334/4975 - Training Loss: 0.7179\n",
            "Batch 4335/4975 - Training Loss: 0.7333\n",
            "Batch 4336/4975 - Training Loss: 0.5678\n",
            "Batch 4337/4975 - Training Loss: 0.4064\n",
            "Batch 4338/4975 - Training Loss: 0.6508\n",
            "Batch 4339/4975 - Training Loss: 0.5272\n",
            "Batch 4340/4975 - Training Loss: 0.5949\n",
            "Batch 4341/4975 - Training Loss: 0.8833\n",
            "Batch 4342/4975 - Training Loss: 0.5676\n",
            "Batch 4343/4975 - Training Loss: 0.8116\n",
            "Batch 4344/4975 - Training Loss: 1.0234\n",
            "Batch 4345/4975 - Training Loss: 0.4759\n",
            "Batch 4346/4975 - Training Loss: 0.7164\n",
            "Batch 4347/4975 - Training Loss: 0.3190\n",
            "Batch 4348/4975 - Training Loss: 0.8087\n",
            "Batch 4349/4975 - Training Loss: 0.6382\n",
            "Batch 4350/4975 - Training Loss: 0.6491\n",
            "Batch 4351/4975 - Training Loss: 0.8168\n",
            "Batch 4352/4975 - Training Loss: 0.3229\n",
            "Batch 4353/4975 - Training Loss: 0.4691\n",
            "Batch 4354/4975 - Training Loss: 0.5330\n",
            "Batch 4355/4975 - Training Loss: 0.6426\n",
            "Batch 4356/4975 - Training Loss: 0.4364\n",
            "Batch 4357/4975 - Training Loss: 0.9130\n",
            "Batch 4358/4975 - Training Loss: 0.3059\n",
            "Batch 4359/4975 - Training Loss: 0.6446\n",
            "Batch 4360/4975 - Training Loss: 0.6356\n",
            "Batch 4361/4975 - Training Loss: 0.9716\n",
            "Batch 4362/4975 - Training Loss: 0.3817\n",
            "Batch 4363/4975 - Training Loss: 0.5394\n",
            "Batch 4364/4975 - Training Loss: 0.5903\n",
            "Batch 4365/4975 - Training Loss: 0.4786\n",
            "Batch 4366/4975 - Training Loss: 0.6733\n",
            "Batch 4367/4975 - Training Loss: 0.4582\n",
            "Batch 4368/4975 - Training Loss: 0.7839\n",
            "Batch 4369/4975 - Training Loss: 0.8597\n",
            "Batch 4370/4975 - Training Loss: 0.8262\n",
            "Batch 4371/4975 - Training Loss: 0.7842\n",
            "Batch 4372/4975 - Training Loss: 0.4929\n",
            "Batch 4373/4975 - Training Loss: 0.4357\n",
            "Batch 4374/4975 - Training Loss: 0.8915\n",
            "Batch 4375/4975 - Training Loss: 0.6249\n",
            "Batch 4376/4975 - Training Loss: 0.5890\n",
            "Batch 4377/4975 - Training Loss: 0.5591\n",
            "Batch 4378/4975 - Training Loss: 0.8215\n",
            "Batch 4379/4975 - Training Loss: 0.6838\n",
            "Batch 4380/4975 - Training Loss: 0.8968\n",
            "Batch 4381/4975 - Training Loss: 0.9646\n",
            "Batch 4382/4975 - Training Loss: 0.6596\n",
            "Batch 4383/4975 - Training Loss: 0.5509\n",
            "Batch 4384/4975 - Training Loss: 0.4246\n",
            "Batch 4385/4975 - Training Loss: 0.8994\n",
            "Batch 4386/4975 - Training Loss: 0.8002\n",
            "Batch 4387/4975 - Training Loss: 0.5793\n",
            "Batch 4388/4975 - Training Loss: 0.6293\n",
            "Batch 4389/4975 - Training Loss: 0.9055\n",
            "Batch 4390/4975 - Training Loss: 0.7071\n",
            "Batch 4391/4975 - Training Loss: 0.9273\n",
            "Batch 4392/4975 - Training Loss: 0.3973\n",
            "Batch 4393/4975 - Training Loss: 0.8180\n",
            "Batch 4394/4975 - Training Loss: 0.8877\n",
            "Batch 4395/4975 - Training Loss: 0.4464\n",
            "Batch 4396/4975 - Training Loss: 0.7138\n",
            "Batch 4397/4975 - Training Loss: 0.4591\n",
            "Batch 4398/4975 - Training Loss: 1.4081\n",
            "Batch 4399/4975 - Training Loss: 0.5918\n",
            "Batch 4400/4975 - Training Loss: 0.6326\n",
            "Batch 4401/4975 - Training Loss: 0.5397\n",
            "Batch 4402/4975 - Training Loss: 0.6898\n",
            "Batch 4403/4975 - Training Loss: 0.3716\n",
            "Batch 4404/4975 - Training Loss: 0.3792\n",
            "Batch 4405/4975 - Training Loss: 0.5643\n",
            "Batch 4406/4975 - Training Loss: 0.9220\n",
            "Batch 4407/4975 - Training Loss: 0.6819\n",
            "Batch 4408/4975 - Training Loss: 0.8084\n",
            "Batch 4409/4975 - Training Loss: 0.5566\n",
            "Batch 4410/4975 - Training Loss: 0.7317\n",
            "Batch 4411/4975 - Training Loss: 0.5457\n",
            "Batch 4412/4975 - Training Loss: 0.6878\n",
            "Batch 4413/4975 - Training Loss: 0.6602\n",
            "Batch 4414/4975 - Training Loss: 0.5306\n",
            "Batch 4415/4975 - Training Loss: 0.5160\n",
            "Batch 4416/4975 - Training Loss: 0.6203\n",
            "Batch 4417/4975 - Training Loss: 0.8065\n",
            "Batch 4418/4975 - Training Loss: 0.3497\n",
            "Batch 4419/4975 - Training Loss: 0.3636\n",
            "Batch 4420/4975 - Training Loss: 0.7117\n",
            "Batch 4421/4975 - Training Loss: 0.8775\n",
            "Batch 4422/4975 - Training Loss: 0.5295\n",
            "Batch 4423/4975 - Training Loss: 0.7469\n",
            "Batch 4424/4975 - Training Loss: 0.6713\n",
            "Batch 4425/4975 - Training Loss: 0.8048\n",
            "Batch 4426/4975 - Training Loss: 0.6393\n",
            "Batch 4427/4975 - Training Loss: 0.5037\n",
            "Batch 4428/4975 - Training Loss: 0.3905\n",
            "Batch 4429/4975 - Training Loss: 1.0173\n",
            "Batch 4430/4975 - Training Loss: 0.7436\n",
            "Batch 4431/4975 - Training Loss: 0.3824\n",
            "Batch 4432/4975 - Training Loss: 0.4221\n",
            "Batch 4433/4975 - Training Loss: 0.5097\n",
            "Batch 4434/4975 - Training Loss: 0.5969\n",
            "Batch 4435/4975 - Training Loss: 0.6707\n",
            "Batch 4436/4975 - Training Loss: 1.1653\n",
            "Batch 4437/4975 - Training Loss: 0.5311\n",
            "Batch 4438/4975 - Training Loss: 0.3857\n",
            "Batch 4439/4975 - Training Loss: 0.4010\n",
            "Batch 4440/4975 - Training Loss: 0.6159\n",
            "Batch 4441/4975 - Training Loss: 0.5806\n",
            "Batch 4442/4975 - Training Loss: 0.5227\n",
            "Batch 4443/4975 - Training Loss: 0.3886\n",
            "Batch 4444/4975 - Training Loss: 0.5895\n",
            "Batch 4445/4975 - Training Loss: 1.0100\n",
            "Batch 4446/4975 - Training Loss: 0.5476\n",
            "Batch 4447/4975 - Training Loss: 0.4529\n",
            "Batch 4448/4975 - Training Loss: 0.6825\n",
            "Batch 4449/4975 - Training Loss: 0.5344\n",
            "Batch 4450/4975 - Training Loss: 0.5925\n",
            "Batch 4451/4975 - Training Loss: 0.5499\n",
            "Batch 4452/4975 - Training Loss: 0.4389\n",
            "Batch 4453/4975 - Training Loss: 0.8303\n",
            "Batch 4454/4975 - Training Loss: 0.5394\n",
            "Batch 4455/4975 - Training Loss: 0.4720\n",
            "Batch 4456/4975 - Training Loss: 0.5571\n",
            "Batch 4457/4975 - Training Loss: 0.8133\n",
            "Batch 4458/4975 - Training Loss: 0.7101\n",
            "Batch 4459/4975 - Training Loss: 0.5705\n",
            "Batch 4460/4975 - Training Loss: 0.5338\n",
            "Batch 4461/4975 - Training Loss: 0.6758\n",
            "Batch 4462/4975 - Training Loss: 0.3453\n",
            "Batch 4463/4975 - Training Loss: 0.7654\n",
            "Batch 4464/4975 - Training Loss: 0.5866\n",
            "Batch 4465/4975 - Training Loss: 0.6486\n",
            "Batch 4466/4975 - Training Loss: 0.8463\n",
            "Batch 4467/4975 - Training Loss: 0.6459\n",
            "Batch 4468/4975 - Training Loss: 0.6845\n",
            "Batch 4469/4975 - Training Loss: 0.6206\n",
            "Batch 4470/4975 - Training Loss: 0.7223\n",
            "Batch 4471/4975 - Training Loss: 0.6610\n",
            "Batch 4472/4975 - Training Loss: 0.8385\n",
            "Batch 4473/4975 - Training Loss: 0.7595\n",
            "Batch 4474/4975 - Training Loss: 0.9335\n",
            "Batch 4475/4975 - Training Loss: 0.6011\n",
            "Batch 4476/4975 - Training Loss: 0.7741\n",
            "Batch 4477/4975 - Training Loss: 0.8925\n",
            "Batch 4478/4975 - Training Loss: 0.4839\n",
            "Batch 4479/4975 - Training Loss: 0.8553\n",
            "Batch 4480/4975 - Training Loss: 0.6533\n",
            "Batch 4481/4975 - Training Loss: 0.3706\n",
            "Batch 4482/4975 - Training Loss: 0.4975\n",
            "Batch 4483/4975 - Training Loss: 0.6025\n",
            "Batch 4484/4975 - Training Loss: 0.8014\n",
            "Batch 4485/4975 - Training Loss: 0.4695\n",
            "Batch 4486/4975 - Training Loss: 0.5189\n",
            "Batch 4487/4975 - Training Loss: 0.5844\n",
            "Batch 4488/4975 - Training Loss: 0.8067\n",
            "Batch 4489/4975 - Training Loss: 0.7824\n",
            "Batch 4490/4975 - Training Loss: 0.7378\n",
            "Batch 4491/4975 - Training Loss: 0.5508\n",
            "Batch 4492/4975 - Training Loss: 0.4189\n",
            "Batch 4493/4975 - Training Loss: 0.4271\n",
            "Batch 4494/4975 - Training Loss: 0.8660\n",
            "Batch 4495/4975 - Training Loss: 0.8650\n",
            "Batch 4496/4975 - Training Loss: 0.8248\n",
            "Batch 4497/4975 - Training Loss: 0.3846\n",
            "Batch 4498/4975 - Training Loss: 0.7021\n",
            "Batch 4499/4975 - Training Loss: 0.4584\n",
            "Batch 4500/4975 - Training Loss: 0.6415\n",
            "Batch 4501/4975 - Training Loss: 0.5221\n",
            "Batch 4502/4975 - Training Loss: 0.6018\n",
            "Batch 4503/4975 - Training Loss: 0.4731\n",
            "Batch 4504/4975 - Training Loss: 0.7533\n",
            "Batch 4505/4975 - Training Loss: 0.5366\n",
            "Batch 4506/4975 - Training Loss: 0.7183\n",
            "Batch 4507/4975 - Training Loss: 0.5794\n",
            "Batch 4508/4975 - Training Loss: 0.5917\n",
            "Batch 4509/4975 - Training Loss: 0.6702\n",
            "Batch 4510/4975 - Training Loss: 0.3851\n",
            "Batch 4511/4975 - Training Loss: 0.4206\n",
            "Batch 4512/4975 - Training Loss: 0.5303\n",
            "Batch 4513/4975 - Training Loss: 0.6133\n",
            "Batch 4514/4975 - Training Loss: 0.5885\n",
            "Batch 4515/4975 - Training Loss: 0.3060\n",
            "Batch 4516/4975 - Training Loss: 0.4851\n",
            "Batch 4517/4975 - Training Loss: 0.5544\n",
            "Batch 4518/4975 - Training Loss: 0.7342\n",
            "Batch 4519/4975 - Training Loss: 0.3713\n",
            "Batch 4520/4975 - Training Loss: 0.5721\n",
            "Batch 4521/4975 - Training Loss: 0.7241\n",
            "Batch 4522/4975 - Training Loss: 0.2451\n",
            "Batch 4523/4975 - Training Loss: 0.5323\n",
            "Batch 4524/4975 - Training Loss: 0.6578\n",
            "Batch 4525/4975 - Training Loss: 0.5486\n",
            "Batch 4526/4975 - Training Loss: 0.8328\n",
            "Batch 4527/4975 - Training Loss: 0.7203\n",
            "Batch 4528/4975 - Training Loss: 0.5993\n",
            "Batch 4529/4975 - Training Loss: 0.4675\n",
            "Batch 4530/4975 - Training Loss: 0.8388\n",
            "Batch 4531/4975 - Training Loss: 0.3444\n",
            "Batch 4532/4975 - Training Loss: 0.5917\n",
            "Batch 4533/4975 - Training Loss: 0.5584\n",
            "Batch 4534/4975 - Training Loss: 0.7988\n",
            "Batch 4535/4975 - Training Loss: 0.6689\n",
            "Batch 4536/4975 - Training Loss: 0.9568\n",
            "Batch 4537/4975 - Training Loss: 0.6174\n",
            "Batch 4538/4975 - Training Loss: 0.5204\n",
            "Batch 4539/4975 - Training Loss: 0.5003\n",
            "Batch 4540/4975 - Training Loss: 0.3928\n",
            "Batch 4541/4975 - Training Loss: 0.7482\n",
            "Batch 4542/4975 - Training Loss: 0.5233\n",
            "Batch 4543/4975 - Training Loss: 0.7661\n",
            "Batch 4544/4975 - Training Loss: 0.9676\n",
            "Batch 4545/4975 - Training Loss: 0.9656\n",
            "Batch 4546/4975 - Training Loss: 0.3060\n",
            "Batch 4547/4975 - Training Loss: 0.7662\n",
            "Batch 4548/4975 - Training Loss: 0.7238\n",
            "Batch 4549/4975 - Training Loss: 0.6574\n",
            "Batch 4550/4975 - Training Loss: 0.4771\n",
            "Batch 4551/4975 - Training Loss: 0.5883\n",
            "Batch 4552/4975 - Training Loss: 0.8397\n",
            "Batch 4553/4975 - Training Loss: 0.8060\n",
            "Batch 4554/4975 - Training Loss: 0.8163\n",
            "Batch 4555/4975 - Training Loss: 0.6494\n",
            "Batch 4556/4975 - Training Loss: 0.5131\n",
            "Batch 4557/4975 - Training Loss: 0.7358\n",
            "Batch 4558/4975 - Training Loss: 0.6313\n",
            "Batch 4559/4975 - Training Loss: 0.3296\n",
            "Batch 4560/4975 - Training Loss: 0.8913\n",
            "Batch 4561/4975 - Training Loss: 0.8283\n",
            "Batch 4562/4975 - Training Loss: 0.3043\n",
            "Batch 4563/4975 - Training Loss: 0.9111\n",
            "Batch 4564/4975 - Training Loss: 0.5751\n",
            "Batch 4565/4975 - Training Loss: 0.4982\n",
            "Batch 4566/4975 - Training Loss: 0.5132\n",
            "Batch 4567/4975 - Training Loss: 0.4752\n",
            "Batch 4568/4975 - Training Loss: 0.6257\n",
            "Batch 4569/4975 - Training Loss: 0.5812\n",
            "Batch 4570/4975 - Training Loss: 0.4249\n",
            "Batch 4571/4975 - Training Loss: 1.0275\n",
            "Batch 4572/4975 - Training Loss: 0.7246\n",
            "Batch 4573/4975 - Training Loss: 0.6477\n",
            "Batch 4574/4975 - Training Loss: 0.8354\n",
            "Batch 4575/4975 - Training Loss: 0.6341\n",
            "Batch 4576/4975 - Training Loss: 0.3919\n",
            "Batch 4577/4975 - Training Loss: 0.4714\n",
            "Batch 4578/4975 - Training Loss: 0.5002\n",
            "Batch 4579/4975 - Training Loss: 0.5793\n",
            "Batch 4580/4975 - Training Loss: 0.5853\n",
            "Batch 4581/4975 - Training Loss: 0.4672\n",
            "Batch 4582/4975 - Training Loss: 0.4567\n",
            "Batch 4583/4975 - Training Loss: 0.7177\n",
            "Batch 4584/4975 - Training Loss: 0.4112\n",
            "Batch 4585/4975 - Training Loss: 0.6638\n",
            "Batch 4586/4975 - Training Loss: 0.9299\n",
            "Batch 4587/4975 - Training Loss: 0.3541\n",
            "Batch 4588/4975 - Training Loss: 0.4795\n",
            "Batch 4589/4975 - Training Loss: 1.0358\n",
            "Batch 4590/4975 - Training Loss: 1.0071\n",
            "Batch 4591/4975 - Training Loss: 0.5799\n",
            "Batch 4592/4975 - Training Loss: 0.5249\n",
            "Batch 4593/4975 - Training Loss: 0.6126\n",
            "Batch 4594/4975 - Training Loss: 0.6380\n",
            "Batch 4595/4975 - Training Loss: 0.6419\n",
            "Batch 4596/4975 - Training Loss: 0.8008\n",
            "Batch 4597/4975 - Training Loss: 0.4558\n",
            "Batch 4598/4975 - Training Loss: 0.7524\n",
            "Batch 4599/4975 - Training Loss: 0.5117\n",
            "Batch 4600/4975 - Training Loss: 0.5495\n",
            "Batch 4601/4975 - Training Loss: 0.7476\n",
            "Batch 4602/4975 - Training Loss: 0.4152\n",
            "Batch 4603/4975 - Training Loss: 0.5190\n",
            "Batch 4604/4975 - Training Loss: 0.5126\n",
            "Batch 4605/4975 - Training Loss: 0.4903\n",
            "Batch 4606/4975 - Training Loss: 0.3318\n",
            "Batch 4607/4975 - Training Loss: 0.5532\n",
            "Batch 4608/4975 - Training Loss: 0.6665\n",
            "Batch 4609/4975 - Training Loss: 0.9946\n",
            "Batch 4610/4975 - Training Loss: 0.6238\n",
            "Batch 4611/4975 - Training Loss: 0.6408\n",
            "Batch 4612/4975 - Training Loss: 0.5543\n",
            "Batch 4613/4975 - Training Loss: 0.6639\n",
            "Batch 4614/4975 - Training Loss: 0.7849\n",
            "Batch 4615/4975 - Training Loss: 0.5332\n",
            "Batch 4616/4975 - Training Loss: 0.5057\n",
            "Batch 4617/4975 - Training Loss: 0.5896\n",
            "Batch 4618/4975 - Training Loss: 0.8358\n",
            "Batch 4619/4975 - Training Loss: 0.9059\n",
            "Batch 4620/4975 - Training Loss: 0.7951\n",
            "Batch 4621/4975 - Training Loss: 0.4369\n",
            "Batch 4622/4975 - Training Loss: 0.6136\n",
            "Batch 4623/4975 - Training Loss: 0.5455\n",
            "Batch 4624/4975 - Training Loss: 0.6299\n",
            "Batch 4625/4975 - Training Loss: 0.5038\n",
            "Batch 4626/4975 - Training Loss: 0.5003\n",
            "Batch 4627/4975 - Training Loss: 0.7675\n",
            "Batch 4628/4975 - Training Loss: 0.8134\n",
            "Batch 4629/4975 - Training Loss: 0.5130\n",
            "Batch 4630/4975 - Training Loss: 0.4488\n",
            "Batch 4631/4975 - Training Loss: 0.4018\n",
            "Batch 4632/4975 - Training Loss: 0.7637\n",
            "Batch 4633/4975 - Training Loss: 1.1538\n",
            "Batch 4634/4975 - Training Loss: 0.7903\n",
            "Batch 4635/4975 - Training Loss: 0.7118\n",
            "Batch 4636/4975 - Training Loss: 0.4601\n",
            "Batch 4637/4975 - Training Loss: 0.5714\n",
            "Batch 4638/4975 - Training Loss: 0.6819\n",
            "Batch 4639/4975 - Training Loss: 0.6320\n",
            "Batch 4640/4975 - Training Loss: 0.8028\n",
            "Batch 4641/4975 - Training Loss: 0.6301\n",
            "Batch 4642/4975 - Training Loss: 0.5908\n",
            "Batch 4643/4975 - Training Loss: 0.3192\n",
            "Batch 4644/4975 - Training Loss: 0.9489\n",
            "Batch 4645/4975 - Training Loss: 0.9040\n",
            "Batch 4646/4975 - Training Loss: 0.5454\n",
            "Batch 4647/4975 - Training Loss: 0.3846\n",
            "Batch 4648/4975 - Training Loss: 0.5567\n",
            "Batch 4649/4975 - Training Loss: 0.7728\n",
            "Batch 4650/4975 - Training Loss: 0.3820\n",
            "Batch 4651/4975 - Training Loss: 0.9213\n",
            "Batch 4652/4975 - Training Loss: 0.8344\n",
            "Batch 4653/4975 - Training Loss: 0.2365\n",
            "Batch 4654/4975 - Training Loss: 0.5481\n",
            "Batch 4655/4975 - Training Loss: 0.4820\n",
            "Batch 4656/4975 - Training Loss: 0.5689\n",
            "Batch 4657/4975 - Training Loss: 0.5282\n",
            "Batch 4658/4975 - Training Loss: 0.7710\n",
            "Batch 4659/4975 - Training Loss: 0.5040\n",
            "Batch 4660/4975 - Training Loss: 0.9011\n",
            "Batch 4661/4975 - Training Loss: 0.6298\n",
            "Batch 4662/4975 - Training Loss: 0.6578\n",
            "Batch 4663/4975 - Training Loss: 0.4568\n",
            "Batch 4664/4975 - Training Loss: 0.5521\n",
            "Batch 4665/4975 - Training Loss: 0.5691\n",
            "Batch 4666/4975 - Training Loss: 0.5689\n",
            "Batch 4667/4975 - Training Loss: 0.3857\n",
            "Batch 4668/4975 - Training Loss: 0.8396\n",
            "Batch 4669/4975 - Training Loss: 0.5742\n",
            "Batch 4670/4975 - Training Loss: 0.6252\n",
            "Batch 4671/4975 - Training Loss: 0.4819\n",
            "Batch 4672/4975 - Training Loss: 0.6955\n",
            "Batch 4673/4975 - Training Loss: 0.8975\n",
            "Batch 4674/4975 - Training Loss: 0.5041\n",
            "Batch 4675/4975 - Training Loss: 0.4764\n",
            "Batch 4676/4975 - Training Loss: 0.5596\n",
            "Batch 4677/4975 - Training Loss: 0.8033\n",
            "Batch 4678/4975 - Training Loss: 0.7150\n",
            "Batch 4679/4975 - Training Loss: 0.6980\n",
            "Batch 4680/4975 - Training Loss: 0.6643\n",
            "Batch 4681/4975 - Training Loss: 0.3473\n",
            "Batch 4682/4975 - Training Loss: 0.6828\n",
            "Batch 4683/4975 - Training Loss: 0.7310\n",
            "Batch 4684/4975 - Training Loss: 0.5072\n",
            "Batch 4685/4975 - Training Loss: 0.8245\n",
            "Batch 4686/4975 - Training Loss: 0.6976\n",
            "Batch 4687/4975 - Training Loss: 0.6350\n",
            "Batch 4688/4975 - Training Loss: 0.4324\n",
            "Batch 4689/4975 - Training Loss: 0.5432\n",
            "Batch 4690/4975 - Training Loss: 0.7581\n",
            "Batch 4691/4975 - Training Loss: 0.7502\n",
            "Batch 4692/4975 - Training Loss: 0.7202\n",
            "Batch 4693/4975 - Training Loss: 0.4975\n",
            "Batch 4694/4975 - Training Loss: 0.6199\n",
            "Batch 4695/4975 - Training Loss: 0.7627\n",
            "Batch 4696/4975 - Training Loss: 0.6481\n",
            "Batch 4697/4975 - Training Loss: 0.6740\n",
            "Batch 4698/4975 - Training Loss: 0.7417\n",
            "Batch 4699/4975 - Training Loss: 0.5318\n",
            "Batch 4700/4975 - Training Loss: 0.2467\n",
            "Batch 4701/4975 - Training Loss: 0.4963\n",
            "Batch 4702/4975 - Training Loss: 1.0897\n",
            "Batch 4703/4975 - Training Loss: 0.7177\n",
            "Batch 4704/4975 - Training Loss: 0.9097\n",
            "Batch 4705/4975 - Training Loss: 0.8879\n",
            "Batch 4706/4975 - Training Loss: 0.8699\n",
            "Batch 4707/4975 - Training Loss: 1.1670\n",
            "Batch 4708/4975 - Training Loss: 0.9149\n",
            "Batch 4709/4975 - Training Loss: 0.7451\n",
            "Batch 4710/4975 - Training Loss: 0.6176\n",
            "Batch 4711/4975 - Training Loss: 0.4399\n",
            "Batch 4712/4975 - Training Loss: 0.5624\n",
            "Batch 4713/4975 - Training Loss: 0.5304\n",
            "Batch 4714/4975 - Training Loss: 0.7533\n",
            "Batch 4715/4975 - Training Loss: 0.5552\n",
            "Batch 4716/4975 - Training Loss: 0.7055\n",
            "Batch 4717/4975 - Training Loss: 0.4575\n",
            "Batch 4718/4975 - Training Loss: 1.1576\n",
            "Batch 4719/4975 - Training Loss: 0.6351\n",
            "Batch 4720/4975 - Training Loss: 0.7052\n",
            "Batch 4721/4975 - Training Loss: 0.4975\n",
            "Batch 4722/4975 - Training Loss: 0.3388\n",
            "Batch 4723/4975 - Training Loss: 0.6456\n",
            "Batch 4724/4975 - Training Loss: 0.5027\n",
            "Batch 4725/4975 - Training Loss: 0.4787\n",
            "Batch 4726/4975 - Training Loss: 0.5500\n",
            "Batch 4727/4975 - Training Loss: 0.5037\n",
            "Batch 4728/4975 - Training Loss: 0.6950\n",
            "Batch 4729/4975 - Training Loss: 0.8436\n",
            "Batch 4730/4975 - Training Loss: 0.6365\n",
            "Batch 4731/4975 - Training Loss: 0.5596\n",
            "Batch 4732/4975 - Training Loss: 0.6759\n",
            "Batch 4733/4975 - Training Loss: 0.7685\n",
            "Batch 4734/4975 - Training Loss: 0.6882\n",
            "Batch 4735/4975 - Training Loss: 0.5239\n",
            "Batch 4736/4975 - Training Loss: 0.5649\n",
            "Batch 4737/4975 - Training Loss: 0.7519\n",
            "Batch 4738/4975 - Training Loss: 0.5201\n",
            "Batch 4739/4975 - Training Loss: 0.7888\n",
            "Batch 4740/4975 - Training Loss: 0.5554\n",
            "Batch 4741/4975 - Training Loss: 0.5161\n",
            "Batch 4742/4975 - Training Loss: 0.5135\n",
            "Batch 4743/4975 - Training Loss: 0.4965\n",
            "Batch 4744/4975 - Training Loss: 0.5980\n",
            "Batch 4745/4975 - Training Loss: 0.4013\n",
            "Batch 4746/4975 - Training Loss: 0.5545\n",
            "Batch 4747/4975 - Training Loss: 0.4156\n",
            "Batch 4748/4975 - Training Loss: 0.4451\n",
            "Batch 4749/4975 - Training Loss: 0.5146\n",
            "Batch 4750/4975 - Training Loss: 0.7623\n",
            "Batch 4751/4975 - Training Loss: 0.4771\n",
            "Batch 4752/4975 - Training Loss: 0.5261\n",
            "Batch 4753/4975 - Training Loss: 0.7066\n",
            "Batch 4754/4975 - Training Loss: 0.5770\n",
            "Batch 4755/4975 - Training Loss: 0.6213\n",
            "Batch 4756/4975 - Training Loss: 0.6045\n",
            "Batch 4757/4975 - Training Loss: 0.8036\n",
            "Batch 4758/4975 - Training Loss: 0.5169\n",
            "Batch 4759/4975 - Training Loss: 0.6413\n",
            "Batch 4760/4975 - Training Loss: 0.5032\n",
            "Batch 4761/4975 - Training Loss: 0.7289\n",
            "Batch 4762/4975 - Training Loss: 0.9806\n",
            "Batch 4763/4975 - Training Loss: 0.3491\n",
            "Batch 4764/4975 - Training Loss: 0.7299\n",
            "Batch 4765/4975 - Training Loss: 0.7330\n",
            "Batch 4766/4975 - Training Loss: 0.6682\n",
            "Batch 4767/4975 - Training Loss: 0.8677\n",
            "Batch 4768/4975 - Training Loss: 0.5686\n",
            "Batch 4769/4975 - Training Loss: 0.5960\n",
            "Batch 4770/4975 - Training Loss: 0.5214\n",
            "Batch 4771/4975 - Training Loss: 0.7148\n",
            "Batch 4772/4975 - Training Loss: 0.5986\n",
            "Batch 4773/4975 - Training Loss: 0.5685\n",
            "Batch 4774/4975 - Training Loss: 0.7679\n",
            "Batch 4775/4975 - Training Loss: 0.7651\n",
            "Batch 4776/4975 - Training Loss: 0.8060\n",
            "Batch 4777/4975 - Training Loss: 0.5481\n",
            "Batch 4778/4975 - Training Loss: 0.7020\n",
            "Batch 4779/4975 - Training Loss: 0.5207\n",
            "Batch 4780/4975 - Training Loss: 0.4515\n",
            "Batch 4781/4975 - Training Loss: 0.4845\n",
            "Batch 4782/4975 - Training Loss: 0.6727\n",
            "Batch 4783/4975 - Training Loss: 0.6983\n",
            "Batch 4784/4975 - Training Loss: 0.5673\n",
            "Batch 4785/4975 - Training Loss: 0.6193\n",
            "Batch 4786/4975 - Training Loss: 0.6047\n",
            "Batch 4787/4975 - Training Loss: 0.4471\n",
            "Batch 4788/4975 - Training Loss: 0.7952\n",
            "Batch 4789/4975 - Training Loss: 0.5725\n",
            "Batch 4790/4975 - Training Loss: 0.5015\n",
            "Batch 4791/4975 - Training Loss: 1.0437\n",
            "Batch 4792/4975 - Training Loss: 0.5777\n",
            "Batch 4793/4975 - Training Loss: 0.6191\n",
            "Batch 4794/4975 - Training Loss: 0.6435\n",
            "Batch 4795/4975 - Training Loss: 0.4994\n",
            "Batch 4796/4975 - Training Loss: 0.5354\n",
            "Batch 4797/4975 - Training Loss: 0.3562\n",
            "Batch 4798/4975 - Training Loss: 0.5184\n",
            "Batch 4799/4975 - Training Loss: 0.6036\n",
            "Batch 4800/4975 - Training Loss: 0.7273\n",
            "Batch 4801/4975 - Training Loss: 0.3451\n",
            "Batch 4802/4975 - Training Loss: 0.6689\n",
            "Batch 4803/4975 - Training Loss: 0.4484\n",
            "Batch 4804/4975 - Training Loss: 0.6918\n",
            "Batch 4805/4975 - Training Loss: 0.8866\n",
            "Batch 4806/4975 - Training Loss: 0.6100\n",
            "Batch 4807/4975 - Training Loss: 0.5505\n",
            "Batch 4808/4975 - Training Loss: 0.5914\n",
            "Batch 4809/4975 - Training Loss: 0.6282\n",
            "Batch 4810/4975 - Training Loss: 0.6137\n",
            "Batch 4811/4975 - Training Loss: 0.6494\n",
            "Batch 4812/4975 - Training Loss: 0.8291\n",
            "Batch 4813/4975 - Training Loss: 0.7080\n",
            "Batch 4814/4975 - Training Loss: 0.5703\n",
            "Batch 4815/4975 - Training Loss: 0.5317\n",
            "Batch 4816/4975 - Training Loss: 0.4378\n",
            "Batch 4817/4975 - Training Loss: 1.2728\n",
            "Batch 4818/4975 - Training Loss: 0.4982\n",
            "Batch 4819/4975 - Training Loss: 0.3835\n",
            "Batch 4820/4975 - Training Loss: 0.6399\n",
            "Batch 4821/4975 - Training Loss: 0.7068\n",
            "Batch 4822/4975 - Training Loss: 0.7247\n",
            "Batch 4823/4975 - Training Loss: 0.5180\n",
            "Batch 4824/4975 - Training Loss: 0.3061\n",
            "Batch 4825/4975 - Training Loss: 0.7466\n",
            "Batch 4826/4975 - Training Loss: 0.3169\n",
            "Batch 4827/4975 - Training Loss: 0.6475\n",
            "Batch 4828/4975 - Training Loss: 0.5729\n",
            "Batch 4829/4975 - Training Loss: 0.6002\n",
            "Batch 4830/4975 - Training Loss: 0.5770\n",
            "Batch 4831/4975 - Training Loss: 0.4886\n",
            "Batch 4832/4975 - Training Loss: 0.5799\n",
            "Batch 4833/4975 - Training Loss: 0.7727\n",
            "Batch 4834/4975 - Training Loss: 0.7496\n",
            "Batch 4835/4975 - Training Loss: 0.5408\n",
            "Batch 4836/4975 - Training Loss: 0.8626\n",
            "Batch 4837/4975 - Training Loss: 0.8845\n",
            "Batch 4838/4975 - Training Loss: 0.5738\n",
            "Batch 4839/4975 - Training Loss: 0.6167\n",
            "Batch 4840/4975 - Training Loss: 0.8147\n",
            "Batch 4841/4975 - Training Loss: 0.9591\n",
            "Batch 4842/4975 - Training Loss: 0.5921\n",
            "Batch 4843/4975 - Training Loss: 0.7753\n",
            "Batch 4844/4975 - Training Loss: 0.7158\n",
            "Batch 4845/4975 - Training Loss: 0.5676\n",
            "Batch 4846/4975 - Training Loss: 0.8847\n",
            "Batch 4847/4975 - Training Loss: 0.7553\n",
            "Batch 4848/4975 - Training Loss: 0.5122\n",
            "Batch 4849/4975 - Training Loss: 0.7066\n",
            "Batch 4850/4975 - Training Loss: 0.5635\n",
            "Batch 4851/4975 - Training Loss: 0.5198\n",
            "Batch 4852/4975 - Training Loss: 0.9281\n",
            "Batch 4853/4975 - Training Loss: 0.5905\n",
            "Batch 4854/4975 - Training Loss: 0.5438\n",
            "Batch 4855/4975 - Training Loss: 0.4223\n",
            "Batch 4856/4975 - Training Loss: 0.4372\n",
            "Batch 4857/4975 - Training Loss: 0.6514\n",
            "Batch 4858/4975 - Training Loss: 0.6953\n",
            "Batch 4859/4975 - Training Loss: 0.8538\n",
            "Batch 4860/4975 - Training Loss: 0.6903\n",
            "Batch 4861/4975 - Training Loss: 0.5631\n",
            "Batch 4862/4975 - Training Loss: 0.5704\n",
            "Batch 4863/4975 - Training Loss: 0.8486\n",
            "Batch 4864/4975 - Training Loss: 0.3308\n",
            "Batch 4865/4975 - Training Loss: 0.6075\n",
            "Batch 4866/4975 - Training Loss: 0.6789\n",
            "Batch 4867/4975 - Training Loss: 0.6397\n",
            "Batch 4868/4975 - Training Loss: 0.7845\n",
            "Batch 4869/4975 - Training Loss: 0.9397\n",
            "Batch 4870/4975 - Training Loss: 0.6776\n",
            "Batch 4871/4975 - Training Loss: 0.7554\n",
            "Batch 4872/4975 - Training Loss: 0.8268\n",
            "Batch 4873/4975 - Training Loss: 0.6281\n",
            "Batch 4874/4975 - Training Loss: 0.4310\n",
            "Batch 4875/4975 - Training Loss: 0.8111\n",
            "Batch 4876/4975 - Training Loss: 0.6369\n",
            "Batch 4877/4975 - Training Loss: 0.5649\n",
            "Batch 4878/4975 - Training Loss: 0.3919\n",
            "Batch 4879/4975 - Training Loss: 0.5038\n",
            "Batch 4880/4975 - Training Loss: 0.6675\n",
            "Batch 4881/4975 - Training Loss: 0.6139\n",
            "Batch 4882/4975 - Training Loss: 0.4994\n",
            "Batch 4883/4975 - Training Loss: 0.9500\n",
            "Batch 4884/4975 - Training Loss: 0.6792\n",
            "Batch 4885/4975 - Training Loss: 0.7045\n",
            "Batch 4886/4975 - Training Loss: 0.7318\n",
            "Batch 4887/4975 - Training Loss: 0.5600\n",
            "Batch 4888/4975 - Training Loss: 0.6816\n",
            "Batch 4889/4975 - Training Loss: 0.4577\n",
            "Batch 4890/4975 - Training Loss: 0.8209\n",
            "Batch 4891/4975 - Training Loss: 0.5224\n",
            "Batch 4892/4975 - Training Loss: 0.5723\n",
            "Batch 4893/4975 - Training Loss: 0.7517\n",
            "Batch 4894/4975 - Training Loss: 0.6021\n",
            "Batch 4895/4975 - Training Loss: 0.4789\n",
            "Batch 4896/4975 - Training Loss: 0.5203\n",
            "Batch 4897/4975 - Training Loss: 0.5360\n",
            "Batch 4898/4975 - Training Loss: 0.8711\n",
            "Batch 4899/4975 - Training Loss: 0.5518\n",
            "Batch 4900/4975 - Training Loss: 0.7419\n",
            "Batch 4901/4975 - Training Loss: 0.5852\n",
            "Batch 4902/4975 - Training Loss: 0.7937\n",
            "Batch 4903/4975 - Training Loss: 0.5458\n",
            "Batch 4904/4975 - Training Loss: 0.3908\n",
            "Batch 4905/4975 - Training Loss: 0.3948\n",
            "Batch 4906/4975 - Training Loss: 0.6150\n",
            "Batch 4907/4975 - Training Loss: 0.5551\n",
            "Batch 4908/4975 - Training Loss: 0.5367\n",
            "Batch 4909/4975 - Training Loss: 0.9258\n",
            "Batch 4910/4975 - Training Loss: 0.5162\n",
            "Batch 4911/4975 - Training Loss: 0.6871\n",
            "Batch 4912/4975 - Training Loss: 0.7668\n",
            "Batch 4913/4975 - Training Loss: 0.5194\n",
            "Batch 4914/4975 - Training Loss: 0.5470\n",
            "Batch 4915/4975 - Training Loss: 0.6170\n",
            "Batch 4916/4975 - Training Loss: 0.4653\n",
            "Batch 4917/4975 - Training Loss: 0.6069\n",
            "Batch 4918/4975 - Training Loss: 0.9157\n",
            "Batch 4919/4975 - Training Loss: 0.4023\n",
            "Batch 4920/4975 - Training Loss: 0.4157\n",
            "Batch 4921/4975 - Training Loss: 0.5821\n",
            "Batch 4922/4975 - Training Loss: 0.7462\n",
            "Batch 4923/4975 - Training Loss: 0.6125\n",
            "Batch 4924/4975 - Training Loss: 0.7253\n",
            "Batch 4925/4975 - Training Loss: 0.6431\n",
            "Batch 4926/4975 - Training Loss: 0.6370\n",
            "Batch 4927/4975 - Training Loss: 0.4457\n",
            "Batch 4928/4975 - Training Loss: 0.5600\n",
            "Batch 4929/4975 - Training Loss: 0.7247\n",
            "Batch 4930/4975 - Training Loss: 0.3465\n",
            "Batch 4931/4975 - Training Loss: 0.9285\n",
            "Batch 4932/4975 - Training Loss: 0.6250\n",
            "Batch 4933/4975 - Training Loss: 0.6459\n",
            "Batch 4934/4975 - Training Loss: 0.7248\n",
            "Batch 4935/4975 - Training Loss: 0.5649\n",
            "Batch 4936/4975 - Training Loss: 0.5756\n",
            "Batch 4937/4975 - Training Loss: 0.6252\n",
            "Batch 4938/4975 - Training Loss: 0.6133\n",
            "Batch 4939/4975 - Training Loss: 0.8872\n",
            "Batch 4940/4975 - Training Loss: 0.6043\n",
            "Batch 4941/4975 - Training Loss: 0.6622\n",
            "Batch 4942/4975 - Training Loss: 0.5788\n",
            "Batch 4943/4975 - Training Loss: 0.5180\n",
            "Batch 4944/4975 - Training Loss: 0.4551\n",
            "Batch 4945/4975 - Training Loss: 0.4936\n",
            "Batch 4946/4975 - Training Loss: 0.5568\n",
            "Batch 4947/4975 - Training Loss: 0.7420\n",
            "Batch 4948/4975 - Training Loss: 0.5379\n",
            "Batch 4949/4975 - Training Loss: 0.9521\n",
            "Batch 4950/4975 - Training Loss: 0.4323\n",
            "Batch 4951/4975 - Training Loss: 0.5883\n",
            "Batch 4952/4975 - Training Loss: 0.6876\n",
            "Batch 4953/4975 - Training Loss: 0.7415\n",
            "Batch 4954/4975 - Training Loss: 1.0035\n",
            "Batch 4955/4975 - Training Loss: 0.4370\n",
            "Batch 4956/4975 - Training Loss: 0.7611\n",
            "Batch 4957/4975 - Training Loss: 0.7414\n",
            "Batch 4958/4975 - Training Loss: 0.7707\n",
            "Batch 4959/4975 - Training Loss: 0.5031\n",
            "Batch 4960/4975 - Training Loss: 0.4836\n",
            "Batch 4961/4975 - Training Loss: 0.7513\n",
            "Batch 4962/4975 - Training Loss: 0.4491\n",
            "Batch 4963/4975 - Training Loss: 0.5685\n",
            "Batch 4964/4975 - Training Loss: 0.7614\n",
            "Batch 4965/4975 - Training Loss: 0.6645\n",
            "Batch 4966/4975 - Training Loss: 0.5589\n",
            "Batch 4967/4975 - Training Loss: 0.5389\n",
            "Batch 4968/4975 - Training Loss: 0.4962\n",
            "Batch 4969/4975 - Training Loss: 0.7961\n",
            "Batch 4970/4975 - Training Loss: 0.3914\n",
            "Batch 4971/4975 - Training Loss: 0.6523\n",
            "Batch 4972/4975 - Training Loss: 0.3379\n",
            "Batch 4973/4975 - Training Loss: 0.5563\n",
            "Batch 4974/4975 - Training Loss: 0.7713\n",
            "Batch 4975/4975 - Training Loss: 0.8700\n",
            "Average Training Loss for Epoch 4: 0.6214\n",
            "Validation Loss for Epoch 4: 0.8179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After the training loop (at the end of your training script)\n",
        "model_save_path = \"/content/drive/MyDrive/BLIP Fashion Captioning\"  # Specify your save path here\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "# Optionally, save the processor if you want to reuse it later\n",
        "processor.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and processor saved to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usAJ-5PWpHm0",
        "outputId": "182da2ea-7ae3-4032-9ccf-5eabb5b6f754"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and processor saved to /content/drive/MyDrive/BLIP Fashion Captioning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and processor\n",
        "model_path = \"/content/drive/MyDrive/BLIP Fashion Captioning\"  # Replace with the path where your fine-tuned model is saved\n",
        "processor = BlipProcessor.from_pretrained(model_path)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "def generate_caption(image_path):\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB format\n",
        "\n",
        "    # Process the image\n",
        "    encoding = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        outputs = model.generate(**encoding)\n",
        "\n",
        "    # Decode the generated tokens to text\n",
        "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "    return caption\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/0211143036.jpg\"  # Replace with your image path\n",
        "caption = generate_caption(image_path)\n",
        "print(f\"Generated Caption: {caption}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxPV1K8jpZF0",
        "outputId": "ddfa5189-1071-4566-92b3-29dfd02e67fd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Caption: solid white shirt in a cotton weave with notch lapels and a tie at the waist long sleeves\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf1tBLO9qzJX",
        "outputId": "7a983d56-0352-47a0-b3da-5cac4feadd62"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}